id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/pull/3220:582,integrability,version,version,582,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:350,modifiability,maintain,maintained,350,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:582,modifiability,version,version,582,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:373,reliability,doe,doesn,373,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:256,safety,review,review,256,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:350,safety,maintain,maintained,350,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:692,safety,Test,Tests,692,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:790,safety,test,tests,790,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:546,security,modif,modified,546,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:256,testability,review,review,256,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:692,testability,Test,Tests,692,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:790,testability,test,tests,790,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:107,usability,guid,guidelines,107,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:138,usability,guid,guide,138,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:234,usability,workflow,workflow,234,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/pull/3220:672,usability,Close,Closes,672,"fa2 library changed to fa2_modified; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR contains commits changing fa2 to fa2_modified as the original fa2 is not maintained anymore and doesn't work with Python3.9+ . [fa2_modified](https://github.com/AminAlam/fa2_modified) has the exact same functionality as the original fa2, but its Cython codes have been modified to make it work with newer version of Python. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2067. - [x] Tests included or not required because:. No additional rests are required as the already existing tests cover these changes. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3220
https://github.com/scverse/scanpy/issues/3221:2005,availability,mask,mask,2005,". I am new to bioinformatics, hence might be missing something. Please let me know whether it is an actual bug or me using the package wrong. ### Minimal code sample. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:2103,availability,mask,mask,2103,"n actual bug or me using the package wrong. ### Minimal code sample. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio N",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:2156,availability,mask,mask,2156,"al code sample. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:2207,availability,mask,mask,2207,"rt pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:2480,availability,mask,mask,2480,"[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:2578,availability,mask,mask,2578,"to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. go",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:2631,availability,mask,mask,2631,"om.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:2682,availability,mask,mask,2682,"3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. j",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:2721,availability,Error,Error,2721,"ups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:18,deployability,fail,fails,18,"rank_genes_groups fails to account for multiple comparsions when multiple groups are provided. ; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When running sc.tl.rank_genes_groups, one can provide multiple groups for comparison. However, correction for multiple comparisons are done only within a single pair of group and reference. In other words, . ```. for group2 in perts:. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = [group2], . reference = group1, . method = 'wilcoxon'). ```. Gives exactly the same p-values as the following:. ```. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = perts, . reference = group1, . method = 'wilcoxon'). ```. To give a bit more context, I am processing Perturb-Seq data from Replogle Cell 2022 paper. I am new to bioinformatics, hence might be missing something. Please let me know whether it is an actual bug or me using the package wrong. ### Minimal code sample. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:265,deployability,version,version,265,"rank_genes_groups fails to account for multiple comparsions when multiple groups are provided. ; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When running sc.tl.rank_genes_groups, one can provide multiple groups for comparison. However, correction for multiple comparisons are done only within a single pair of group and reference. In other words, . ```. for group2 in perts:. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = [group2], . reference = group1, . method = 'wilcoxon'). ```. Gives exactly the same p-values as the following:. ```. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = perts, . reference = group1, . method = 'wilcoxon'). ```. To give a bit more context, I am processing Perturb-Seq data from Replogle Cell 2022 paper. I am new to bioinformatics, hence might be missing something. Please let me know whether it is an actual bug or me using the package wrong. ### Minimal code sample. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:3015,deployability,Version,Versions,3015,"'pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:5346,deployability,updat,updated,5346,"24.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. nbformat 5.10.4. netifaces 0.11.0. numba 0.59.1. numexpr 2.10.1. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. pkg_resources NA. platformdirs 4.2.1. plotly 5.22.0. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 16.0.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 2.0.0. pygments 2.17.2. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2022.1. referencing NA. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. sympy 1.12. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. torch 2.1.2+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. tzdata 2024.1. uri_template NA. urllib3 2.2.1. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. xxhash NA. yaml 5.4.1. zipp NA. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.3. notebook 7.2.0. -----. Python 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]. Linux-6.5.0-1027-oem-x86_64-with-glibc2.35. -----. Session information updated at 2024-09-02 14:19. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:3287,energy efficiency,cloud,cloudpickle,3287," # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. nbformat 5.10.4. netifaces 0.11.0. numba 0.59.1. numexpr 2.10.1. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. pkg_resources NA. platformdirs 4.2.1. plotly 5.22.0. prometheus_client NA. prompt_toolkit ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:265,integrability,version,version,265,"rank_genes_groups fails to account for multiple comparsions when multiple groups are provided. ; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When running sc.tl.rank_genes_groups, one can provide multiple groups for comparison. However, correction for multiple comparisons are done only within a single pair of group and reference. In other words, . ```. for group2 in perts:. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = [group2], . reference = group1, . method = 'wilcoxon'). ```. Gives exactly the same p-values as the following:. ```. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = perts, . reference = group1, . method = 'wilcoxon'). ```. To give a bit more context, I am processing Perturb-Seq data from Replogle Cell 2022 paper. I am new to bioinformatics, hence might be missing something. Please let me know whether it is an actual bug or me using the package wrong. ### Minimal code sample. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:3015,integrability,Version,Versions,3015,"'pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:4221,interoperability,platform,platformdirs,4221,7.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. nbformat 5.10.4. netifaces 0.11.0. numba 0.59.1. numexpr 2.10.1. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. pkg_resources NA. platformdirs 4.2.1. plotly 5.22.0. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 16.0.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 2.0.0. pygments 2.17.2. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2022.1. referencing NA. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. sympy 1.12. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. torch 2.1.2+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. tzdata 2024.1. uri_template NA. urllib3 2.2.1. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. xxhash NA. yaml 5.4.1. zipp NA. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.3. notebook 7.2.0. -----. Python 3.10,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:265,modifiability,version,version,265,"rank_genes_groups fails to account for multiple comparsions when multiple groups are provided. ; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When running sc.tl.rank_genes_groups, one can provide multiple groups for comparison. However, correction for multiple comparisons are done only within a single pair of group and reference. In other words, . ```. for group2 in perts:. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = [group2], . reference = group1, . method = 'wilcoxon'). ```. Gives exactly the same p-values as the following:. ```. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = perts, . reference = group1, . method = 'wilcoxon'). ```. To give a bit more context, I am processing Perturb-Seq data from Replogle Cell 2022 paper. I am new to bioinformatics, hence might be missing something. Please let me know whether it is an actual bug or me using the package wrong. ### Minimal code sample. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:1134,modifiability,pac,package,1134," met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When running sc.tl.rank_genes_groups, one can provide multiple groups for comparison. However, correction for multiple comparisons are done only within a single pair of group and reference. In other words, . ```. for group2 in perts:. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = [group2], . reference = group1, . method = 'wilcoxon'). ```. Gives exactly the same p-values as the following:. ```. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = perts, . reference = group1, . method = 'wilcoxon'). ```. To give a bit more context, I am processing Perturb-Seq data from Replogle Cell 2022 paper. I am new to bioinformatics, hence might be missing something. Please let me know whether it is an actual bug or me using the package wrong. ### Minimal code sample. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pva",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:3015,modifiability,Version,Versions,3015,"'pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:3436,modifiability,deco,decorator,3436,"genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. nbformat 5.10.4. netifaces 0.11.0. numba 0.59.1. numexpr 2.10.1. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. pkg_resources NA. platformdirs 4.2.1. plotly 5.22.0. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 16.0.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pyd",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:4160,modifiability,pac,packaging,4160,.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. nbformat 5.10.4. netifaces 0.11.0. numba 0.59.1. numexpr 2.10.1. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. pkg_resources NA. platformdirs 4.2.1. plotly 5.22.0. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 16.0.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 2.0.0. pygments 2.17.2. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2022.1. referencing NA. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. sympy 1.12. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. torch 2.1.2+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. tzdata 2024.1. uri_template NA. urllib3 2.2.1. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. xxhash NA. yaml 5.4.1. zipp NA. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_co,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:2721,performance,Error,Error,2721,"ups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:18,reliability,fail,fails,18,"rank_genes_groups fails to account for multiple comparsions when multiple groups are provided. ; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When running sc.tl.rank_genes_groups, one can provide multiple groups for comparison. However, correction for multiple comparisons are done only within a single pair of group and reference. In other words, . ```. for group2 in perts:. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = [group2], . reference = group1, . method = 'wilcoxon'). ```. Gives exactly the same p-values as the following:. ```. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = perts, . reference = group1, . method = 'wilcoxon'). ```. To give a bit more context, I am processing Perturb-Seq data from Replogle Cell 2022 paper. I am new to bioinformatics, hence might be missing something. Please let me know whether it is an actual bug or me using the package wrong. ### Minimal code sample. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:2866,reliability,doe,does,2866,"genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:2955,reliability,doe,does,2955,"on'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:2979,reliability,doe,does,2979,"[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:2721,safety,Error,Error,2721,"ups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:3483,safety,except,exceptiongroup,3483,"ult['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. nbformat 5.10.4. netifaces 0.11.0. numba 0.59.1. numexpr 2.10.1. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. pkg_resources NA. platformdirs 4.2.1. plotly 5.22.0. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 16.0.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 2.0.0. pygments 2.17.2. pyp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:5346,safety,updat,updated,5346,"24.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. nbformat 5.10.4. netifaces 0.11.0. numba 0.59.1. numexpr 2.10.1. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. pkg_resources NA. platformdirs 4.2.1. plotly 5.22.0. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 16.0.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 2.0.0. pygments 2.17.2. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2022.1. referencing NA. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. sympy 1.12. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. torch 2.1.2+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. tzdata 2024.1. uri_template NA. urllib3 2.2.1. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. xxhash NA. yaml 5.4.1. zipp NA. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.3. notebook 7.2.0. -----. Python 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]. Linux-6.5.0-1027-oem-x86_64-with-glibc2.35. -----. Session information updated at 2024-09-02 14:19. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:3213,security,certif,certifi,3213,"nt(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. nbformat 5.10.4. netifaces 0.11.0. numba 0.59.1. numexpr 2.10.1. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. pkg_resources ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:3654,security,iso,isoduration,3654,"lt['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. nbformat 5.10.4. netifaces 0.11.0. numba 0.59.1. numexpr 2.10.1. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. pkg_resources NA. platformdirs 4.2.1. plotly 5.22.0. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 16.0.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 2.0.0. pygments 2.17.2. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2022.1. referencing NA. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.0. send2trash NA. s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:4739,security,soc,socks,4739,"24.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. nbformat 5.10.4. netifaces 0.11.0. numba 0.59.1. numexpr 2.10.1. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. pkg_resources NA. platformdirs 4.2.1. plotly 5.22.0. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 16.0.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 2.0.0. pygments 2.17.2. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2022.1. referencing NA. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. sympy 1.12. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. torch 2.1.2+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. tzdata 2024.1. uri_template NA. urllib3 2.2.1. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. xxhash NA. yaml 5.4.1. zipp NA. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.3. notebook 7.2.0. -----. Python 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]. Linux-6.5.0-1027-oem-x86_64-with-glibc2.35. -----. Session information updated at 2024-09-02 14:19. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:5326,security,Session,Session,5326,"24.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. nbformat 5.10.4. netifaces 0.11.0. numba 0.59.1. numexpr 2.10.1. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. pkg_resources NA. platformdirs 4.2.1. plotly 5.22.0. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 16.0.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 2.0.0. pygments 2.17.2. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2022.1. referencing NA. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. sympy 1.12. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. torch 2.1.2+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. tzdata 2024.1. uri_template NA. urllib3 2.2.1. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. xxhash NA. yaml 5.4.1. zipp NA. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.3. notebook 7.2.0. -----. Python 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]. Linux-6.5.0-1027-oem-x86_64-with-glibc2.35. -----. Session information updated at 2024-09-02 14:19. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:5346,security,updat,updated,5346,"24.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. nbformat 5.10.4. netifaces 0.11.0. numba 0.59.1. numexpr 2.10.1. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. pkg_resources NA. platformdirs 4.2.1. plotly 5.22.0. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 16.0.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 2.0.0. pygments 2.17.2. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2022.1. referencing NA. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. sympy 1.12. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. torch 2.1.2+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. tzdata 2024.1. uri_template NA. urllib3 2.2.1. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. xxhash NA. yaml 5.4.1. zipp NA. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.3. notebook 7.2.0. -----. Python 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]. Linux-6.5.0-1027-oem-x86_64-with-glibc2.35. -----. Session information updated at 2024-09-02 14:19. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:936,testability,context,context,936,"rank_genes_groups fails to account for multiple comparsions when multiple groups are provided. ; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When running sc.tl.rank_genes_groups, one can provide multiple groups for comparison. However, correction for multiple comparisons are done only within a single pair of group and reference. In other words, . ```. for group2 in perts:. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = [group2], . reference = group1, . method = 'wilcoxon'). ```. Gives exactly the same p-values as the following:. ```. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = perts, . reference = group1, . method = 'wilcoxon'). ```. To give a bit more context, I am processing Perturb-Seq data from Replogle Cell 2022 paper. I am new to bioinformatics, hence might be missing something. Please let me know whether it is an actual bug or me using the package wrong. ### Minimal code sample. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:225,usability,confirm,confirmed,225,"rank_genes_groups fails to account for multiple comparsions when multiple groups are provided. ; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When running sc.tl.rank_genes_groups, one can provide multiple groups for comparison. However, correction for multiple comparisons are done only within a single pair of group and reference. In other words, . ```. for group2 in perts:. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = [group2], . reference = group1, . method = 'wilcoxon'). ```. Gives exactly the same p-values as the following:. ```. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = perts, . reference = group1, . method = 'wilcoxon'). ```. To give a bit more context, I am processing Perturb-Seq data from Replogle Cell 2022 paper. I am new to bioinformatics, hence might be missing something. Please let me know whether it is an actual bug or me using the package wrong. ### Minimal code sample. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:308,usability,confirm,confirmed,308,"rank_genes_groups fails to account for multiple comparsions when multiple groups are provided. ; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When running sc.tl.rank_genes_groups, one can provide multiple groups for comparison. However, correction for multiple comparisons are done only within a single pair of group and reference. In other words, . ```. for group2 in perts:. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = [group2], . reference = group1, . method = 'wilcoxon'). ```. Gives exactly the same p-values as the following:. ```. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = perts, . reference = group1, . method = 'wilcoxon'). ```. To give a bit more context, I am processing Perturb-Seq data from Replogle Cell 2022 paper. I am new to bioinformatics, hence might be missing something. Please let me know whether it is an actual bug or me using the package wrong. ### Minimal code sample. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:1153,usability,Minim,Minimal,1153,"checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When running sc.tl.rank_genes_groups, one can provide multiple groups for comparison. However, correction for multiple comparisons are done only within a single pair of group and reference. In other words, . ```. for group2 in perts:. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = [group2], . reference = group1, . method = 'wilcoxon'). ```. Gives exactly the same p-values as the following:. ```. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = perts, . reference = group1, . method = 'wilcoxon'). ```. To give a bit more context, I am processing Perturb-Seq data from Replogle Cell 2022 paper. I am new to bioinformatics, hence might be missing something. Please let me know whether it is an actual bug or me using the package wrong. ### Minimal code sample. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[m",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:1313,usability,minim,minimalistic,1313,"his bug exists on the main branch of scanpy. ### What happened? When running sc.tl.rank_genes_groups, one can provide multiple groups for comparison. However, correction for multiple comparisons are done only within a single pair of group and reference. In other words, . ```. for group2 in perts:. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = [group2], . reference = group1, . method = 'wilcoxon'). ```. Gives exactly the same p-values as the following:. ```. sc.tl.rank_genes_groups(adata, . groupby = ""gene"", . groups = perts, . reference = group1, . method = 'wilcoxon'). ```. To give a bit more context, I am processing Perturb-Seq data from Replogle Cell 2022 paper. I am new to bioinformatics, hence might be missing something. Please let me know whether it is an actual bug or me using the package wrong. ### Minimal code sample. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from anndata import AnnData. p_value_threshold = 0.05. # Create a minimalistic AnnData object. data = np.random.rand(1000, 5) # 1000 cells, 5 genes. obs = pd.DataFrame(index=[f'cell{i}' for i in range(1000)]). var = pd.DataFrame(index=[f'gene{i}' for i in range(5)]). adata = AnnData(X=data, obs=obs, var=var). # Add a 'gene' column to obs to use as groupby. adata.obs['gene'] = np.random.choice(['sample0', 'sample1', 'sample2', 'sample3', 'sample4'], size=1000). # Define groups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:2721,usability,Error,Error,2721,"ups. group1 = 'sample0'. perts = ['sample1', 'sample2', 'sample3', 'sample4']. # Run the loop to get p-values. for group2 in perts:. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=[group2],. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). print('______________________________________________'). # Run all at once. sc.tl.rank_genes_groups(adata,. groupby='gene',. groups=perts,. reference=group1,. method='wilcoxon'). result = adata.uns[""rank_genes_groups""]. for group2 in perts:. #mask = result['pvals_adj'][group2] < p_value_threshold. filtered_genes = result['names'][group2]#[mask]. filtered_pvals = result['pvals_adj'][group2]#[mask]. filtered_scores = result['scores'][group2]#[mask]. print(filtered_pvals). ```. ### Error output. ```pytb. I would expect to see different adjusted p-values for the first and the second case. When looping (first case) the method does not see other comparisons coming from the loop, while in the second case the method does see them but still does not correct for them. ```. ### Versions. <details>. ```. -----. anndata 0.10.7. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. apport_python_hook NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.14.0. cairo 1.20.1. certifi 2024.07.04. cffi 1.16.0. chardet 4.0.0. charset_normalizer 3.3.2. cloudpickle 3.0.0. colorama 0.4.4. comm 0.2.2. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2024.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/issues/3221:4845,usability,tool,toolz,4845,"24.8.0. dateutil 2.9.0.post0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. dill 0.3.8. exceptiongroup 1.2.1. executing 2.0.1. fastjsonschema NA. fqdn NA. gi 3.42.1. gio NA. glib NA. gobject NA. gtk NA. h5py 3.11.0. idna 3.3. igraph 0.11.5. ipykernel 6.29.4. isoduration NA. jedi 0.19.1. jinja2 3.1.3. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.42.0. louvain 0.8.2. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.8.4. more_itertools 8.10.0. mpl_toolkits NA. mpmath 1.3.0. natsort 8.4.0. nbformat 5.10.4. netifaces 0.11.0. numba 0.59.1. numexpr 2.10.1. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. pkg_resources NA. platformdirs 4.2.1. plotly 5.22.0. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 16.0.0. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pydot 2.0.0. pygments 2.17.2. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2022.1. referencing NA. requests 2.31.0. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. sitecustomize NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. sympy 1.12. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. torch 2.1.2+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. tzdata 2024.1. uri_template NA. urllib3 2.2.1. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. xxhash NA. yaml 5.4.1. zipp NA. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.3. notebook 7.2.0. -----. Python 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]. Linux-6.5.0-1027-oem-x86_64-with-glibc2.35. -----. Session information updated at 2024-09-02 14:19. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3221
https://github.com/scverse/scanpy/pull/3222:36,deployability,Updat,Update,36,Backport PR #3216 on branch 1.10.x (Update notebooks); Backport PR #3216: Update notebooks,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3222
https://github.com/scverse/scanpy/pull/3222:74,deployability,Updat,Update,74,Backport PR #3216 on branch 1.10.x (Update notebooks); Backport PR #3216: Update notebooks,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3222
https://github.com/scverse/scanpy/pull/3222:36,safety,Updat,Update,36,Backport PR #3216 on branch 1.10.x (Update notebooks); Backport PR #3216: Update notebooks,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3222
https://github.com/scverse/scanpy/pull/3222:74,safety,Updat,Update,74,Backport PR #3216 on branch 1.10.x (Update notebooks); Backport PR #3216: Update notebooks,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3222
https://github.com/scverse/scanpy/pull/3222:36,security,Updat,Update,36,Backport PR #3216 on branch 1.10.x (Update notebooks); Backport PR #3216: Update notebooks,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3222
https://github.com/scverse/scanpy/pull/3222:74,security,Updat,Update,74,Backport PR #3216 on branch 1.10.x (Update notebooks); Backport PR #3216: Update notebooks,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3222
https://github.com/scverse/scanpy/issues/3224:403,deployability,contain,contain,403,"pbmc68k_reduced raw counts; Hi! . I was playing with the [pbmc68k_reduced](https://scanpy.readthedocs.io/en/stable/generated/scanpy.datasets.pbmc68k_reduced.html) dataset and I saw that it has decimal values even in the ""raw"" matrix:. ```. >>> adata.raw.X.data. array([2.177, 2.177, 2.544, ..., 1.142, 2.255, 1.142], dtype=float32). ```. which transformation has been applied? shouldn't the ""raw"" group contain unprocessed counts? is there a standard for this? Ideally, I would like to know where to find the raw, unprocessed, counts everytime I see an anndata or h5ad file.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3224
https://github.com/scverse/scanpy/issues/3224:344,integrability,transform,transformation,344,"pbmc68k_reduced raw counts; Hi! . I was playing with the [pbmc68k_reduced](https://scanpy.readthedocs.io/en/stable/generated/scanpy.datasets.pbmc68k_reduced.html) dataset and I saw that it has decimal values even in the ""raw"" matrix:. ```. >>> adata.raw.X.data. array([2.177, 2.177, 2.544, ..., 1.142, 2.255, 1.142], dtype=float32). ```. which transformation has been applied? shouldn't the ""raw"" group contain unprocessed counts? is there a standard for this? Ideally, I would like to know where to find the raw, unprocessed, counts everytime I see an anndata or h5ad file.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3224
https://github.com/scverse/scanpy/issues/3224:344,interoperability,transform,transformation,344,"pbmc68k_reduced raw counts; Hi! . I was playing with the [pbmc68k_reduced](https://scanpy.readthedocs.io/en/stable/generated/scanpy.datasets.pbmc68k_reduced.html) dataset and I saw that it has decimal values even in the ""raw"" matrix:. ```. >>> adata.raw.X.data. array([2.177, 2.177, 2.544, ..., 1.142, 2.255, 1.142], dtype=float32). ```. which transformation has been applied? shouldn't the ""raw"" group contain unprocessed counts? is there a standard for this? Ideally, I would like to know where to find the raw, unprocessed, counts everytime I see an anndata or h5ad file.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3224
https://github.com/scverse/scanpy/issues/3224:442,interoperability,standard,standard,442,"pbmc68k_reduced raw counts; Hi! . I was playing with the [pbmc68k_reduced](https://scanpy.readthedocs.io/en/stable/generated/scanpy.datasets.pbmc68k_reduced.html) dataset and I saw that it has decimal values even in the ""raw"" matrix:. ```. >>> adata.raw.X.data. array([2.177, 2.177, 2.544, ..., 1.142, 2.255, 1.142], dtype=float32). ```. which transformation has been applied? shouldn't the ""raw"" group contain unprocessed counts? is there a standard for this? Ideally, I would like to know where to find the raw, unprocessed, counts everytime I see an anndata or h5ad file.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3224
https://github.com/scverse/scanpy/issues/3226:227,energy efficiency,GPU,GPU,227,Dask Sparse PCA; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation. 2. Use already existing mean-var. 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226
https://github.com/scverse/scanpy/issues/3226:418,energy efficiency,GPU,GPU,418,Dask Sparse PCA; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation. 2. Use already existing mean-var. 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226
https://github.com/scverse/scanpy/issues/3226:583,energy efficiency,GPU,GPU,583,Dask Sparse PCA; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation. 2. Use already existing mean-var. 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226
https://github.com/scverse/scanpy/issues/3226:631,energy efficiency,GPU,GPU,631,Dask Sparse PCA; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation. 2. Use already existing mean-var. 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226
https://github.com/scverse/scanpy/issues/3226:89,modifiability,paramet,parameters,89,Dask Sparse PCA; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation. 2. Use already existing mean-var. 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226
https://github.com/scverse/scanpy/issues/3226:558,modifiability,deco,decomp,558,Dask Sparse PCA; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation. 2. Use already existing mean-var. 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226
https://github.com/scverse/scanpy/issues/3226:227,performance,GPU,GPU,227,Dask Sparse PCA; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation. 2. Use already existing mean-var. 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226
https://github.com/scverse/scanpy/issues/3226:418,performance,GPU,GPU,418,Dask Sparse PCA; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation. 2. Use already existing mean-var. 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226
https://github.com/scverse/scanpy/issues/3226:583,performance,GPU,GPU,583,Dask Sparse PCA; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation. 2. Use already existing mean-var. 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226
https://github.com/scverse/scanpy/issues/3226:616,performance,perform,performance,616,Dask Sparse PCA; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation. 2. Use already existing mean-var. 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226
https://github.com/scverse/scanpy/issues/3226:631,performance,GPU,GPU,631,Dask Sparse PCA; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation. 2. Use already existing mean-var. 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226
https://github.com/scverse/scanpy/issues/3226:616,usability,perform,performance,616,Dask Sparse PCA; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation. 2. Use already existing mean-var. 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226
https://github.com/scverse/scanpy/issues/3226:751,usability,help,helpful,751,Dask Sparse PCA; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. @Intron7 and I set up a nice implementation using GPU here: https://github.com/scverse/rapids_singlecell/pull/179/files#diff-483d6f872ddf4abd63e32af66a8566e0dcb40ba853a8672771dcbffb0235b7f9. It should be straightforward (and easier) without GPU. Basic outline of computational-heavy steps:. 1. Chunked covariance matrix calculation. 2. Use already existing mean-var. 3. Eigenvalue decomp. It works well on GPU so hopefully we can get nice performance on GPU. I suspect using numba-in-dask for the covariance matrix calculation (similar to @Intron7 's kernel) would be super helpful.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3226
https://github.com/scverse/scanpy/pull/3227:224,availability,error,error,224,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:358,availability,cluster,clustering,358,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:450,availability,down,downsample,450,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:481,availability,down,downsample,481,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:509,availability,down,downsample,509,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:571,availability,down,downsampling,571,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:685,availability,down,downsample,685,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:743,availability,down,downsample,743,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:757,availability,down,downsample,757,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:358,deployability,cluster,clustering,358,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:407,deployability,fail,failed,407,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:526,deployability,log,logger,526,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:701,deployability,log,logger,701,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:912,deployability,fail,failed,912,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2875,deployability,releas,release,2875,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2900,deployability,Releas,Release,2900,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:1387,energy efficiency,estimat,estimator,1387,"n_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To f",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:1421,energy efficiency,estimat,estimator,1421,"guments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:1617,energy efficiency,estimat,estimator,1617,"im, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:43,integrability,batch,batch,43,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:1379,integrability,wrap,wrapper,1379,"_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2164,integrability,batch,batch,2164,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2355,integrability,batch,batch,2355,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:1379,interoperability,wrapper,wrapper,1379,"_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:343,modifiability,pac,packages,343,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:859,modifiability,pac,packages,859,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:1304,modifiability,pac,packages,1304,"conda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError:",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:1360,modifiability,deco,decorator,1360,"ing.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch numb",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:1733,modifiability,pac,packages,1733," with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:1750,modifiability,deco,decomposition,1750,"downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:43,performance,batch,batch,43,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:224,performance,error,error,224,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2164,performance,batch,batch,2164,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2355,performance,batch,batch,2355,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:407,reliability,fail,failed,407,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:912,reliability,fail,failed,912,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:224,safety,error,error,224,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:526,safety,log,logger,526,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:701,safety,log,logger,701,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2426,safety,except,except,2426,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2677,safety,review,review,2677,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2777,safety,Test,Tests,2777,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:526,security,log,logger,526,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:701,security,log,logger,701,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:526,testability,log,logger,526,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:701,testability,log,logger,701,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2677,testability,review,review,2677,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2777,testability,Test,Tests,2777,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:224,usability,error,error,224,"fix bug for partial_fit when the number of batch samples is less than n_comp; For incremental PCA: `sc.tl.pca(adata, n_comps=ndim, chunked=True)`. sometimes, the number of samples for the last chunk is smaller than ndim, an error would be throw:. ```pytb. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/pym3c/clustering.py:377, in run_dimension_reduction(***failed resolving arguments***). 375 if not downsample or obs_chunk_size > downsample or adata.n_obs < downsample:. 376 logger.info(f""Running IncrementalPCA without downsampling""). --> 377 sc.tl.pca(adata, n_comps=ndim, chunked=True,. 378 chunk_size=obs_chunk_size). 379 else: # downsample. 380 logger.info(f""Running IncrementalPCA with downsample = {downsample}""). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/scanpy/preprocessing/_pca.py:255, in pca(***failed resolving arguments***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2528,usability,guid,guidelines,2528,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2559,usability,guid,guide,2559,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2655,usability,workflow,workflow,2655,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/pull/3227:2761,usability,Close,Closes,2761,"s***). 253 for chunk, _, _ in adata_comp.chunked_X(chunk_size):. 254 chunk = chunk.toarray() if issparse(chunk) else chunk. --> 255 pca_.partial_fit(chunk). 257 for chunk, start, end in adata_comp.chunked_X(chunk_size):. 258 chunk = chunk.toarray() if issparse(chunk) else chunk. File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs). 1466 estimator._validate_params(). 1468 with config_context(. 1469 skip_parameter_validation=(. 1470 prefer_skip_nested_validation or global_skip_validation. 1471 ). 1472 ):. -> 1473 return fit_method(estimator, *args, **kwargs). File /anvil/projects/x-mcb130189/Wubin/Software/miniconda3/envs/m3c/lib/python3.9/site-packages/sklearn/decomposition/_incremental_pca.py:304, in IncrementalPCA.partial_fit(self, X, y, check_input). 298 raise ValueError(. 299 ""n_components=%r invalid for n_features=%d, need "". 300 ""more rows than columns for IncrementalPCA "". 301 ""processing"" % (self.n_components, n_features). 302 ). 303 elif not self.n_components <= n_samples:. --> 304 raise ValueError(. 305 ""n_components=%r must be less or equal to "". 306 ""the batch number of samples "". 307 ""%d."" % (self.n_components, n_samples). 308 ). 309 else:. 310 self.n_components_ = self.n_components. ValueError: n_components=100 must be less or equal to the batch number of samples 77. ```. To fix this bug, I added a `try` and `except` to line 255 of _pca.py. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3227
https://github.com/scverse/scanpy/issues/3228:7,availability,cluster,clustering,7,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:398,availability,cluster,clustering,398,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2512,availability,cluster,clusters,2512,"ata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2937,availability,Error,Error,2937,"t_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'Va",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:7,deployability,cluster,clustering,7,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:214,deployability,version,version,214,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:398,deployability,cluster,clustering,398,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:1640,deployability,contain,contains,1640,"aries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2212,deployability,Log,Logarithmize,2212,"e_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2425,deployability,log,log,2425,"for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.Rand",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2512,deployability,cluster,clusters,2512,"ata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:8208,deployability,Version,Versions,8208,"nd.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. .... repeated. ```. ### Versions. ```. -----. anndata 0.10.9. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15.0. chardet 5.2.0. charset_normalizer 2.0.9. colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:10402,deployability,updat,updated,10402,"colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. pooch v1.7.0. prometheus_client NA. prompt_toolkit 3.0.22. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.10.0. pyparsing 3.1.4. pythoncom NA. pythonjsonlogger NA. pytz 2024.1. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. referencing NA. requests 2.32.3. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.1. send2trash NA. session_info 1.0.0. setuptools_scm NA. simplejson 3.19.2. six 1.16.0. sklearn 1.5.1. sniffio 1.3.1. storemagic NA. threadpoolctl 3.5.0. torch 1.10.1+cpu. tornado 6.4.1. tqdm 4.66.5. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 1.26.7. wcwidth 0.2.5. webcolors 24.8.0. websocket 1.8.0. win32api NA. win32com NA. win32con NA. win32security NA. win32trace NA. winerror NA. yaml 6.0. zipp NA. zmq 26.2.0. zoneinfo NA. -----. IPython 7.29.0. jupyter_client 7.4.9. jupyter_core 5.7.2. jupyterlab 4.2.5. notebook 7.2.2. -----. Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. -----. Session information updated at 2024-09-12 19:03. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:627,energy efficiency,Core,Core,627,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:707,energy efficiency,Core,Core,707,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:9868,energy efficiency,cpu,cpu,9868,"colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. pooch v1.7.0. prometheus_client NA. prompt_toolkit 3.0.22. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.10.0. pyparsing 3.1.4. pythoncom NA. pythonjsonlogger NA. pytz 2024.1. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. referencing NA. requests 2.32.3. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.1. send2trash NA. session_info 1.0.0. setuptools_scm NA. simplejson 3.19.2. six 1.16.0. sklearn 1.5.1. sniffio 1.3.1. storemagic NA. threadpoolctl 3.5.0. torch 1.10.1+cpu. tornado 6.4.1. tqdm 4.66.5. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 1.26.7. wcwidth 0.2.5. webcolors 24.8.0. websocket 1.8.0. win32api NA. win32com NA. win32con NA. win32security NA. win32trace NA. winerror NA. yaml 6.0. zipp NA. zmq 26.2.0. zoneinfo NA. -----. IPython 7.29.0. jupyter_client 7.4.9. jupyter_core 5.7.2. jupyterlab 4.2.5. notebook 7.2.2. -----. Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. -----. Session information updated at 2024-09-12 19:03. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:214,integrability,version,version,214,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:8208,integrability,Version,Versions,8208,"nd.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. .... repeated. ```. ### Versions. ```. -----. anndata 0.10.9. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15.0. chardet 5.2.0. charset_normalizer 2.0.9. colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:9238,interoperability,platform,platformdirs,9238,. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15.0. chardet 5.2.0. charset_normalizer 2.0.9. colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. pooch v1.7.0. prometheus_client NA. prompt_toolkit 3.0.22. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.10.0. pyparsing 3.1.4. pythoncom NA. pythonjsonlogger NA. pytz 2024.1. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. referencing NA. requests 2.32.3. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.1. send2trash NA. session_info 1.0.0. setuptools_scm NA. simplejson 3.19.2. six 1.16.0. sklearn 1.5.1. sniffio 1.3.1. storemagic NA. threadpoolctl 3.5.0. torch 1.10.1+cpu. tornado 6.4.1. tqdm 4.66.5. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 1.26.7. wcwidth 0.2.5. webcolors 24.8.0. websocket 1.8.0. win32api NA. win32com NA. win32con NA. win32security NA. win32trace NA. winerror NA. yaml 6.0. zipp NA. zmq 26.2.0. zoneinfo NA. -----. IPython 7.29.0. jupyter_client 7.4.9. jupyter_core 5.7.2. jupyterlab 4.2.5. notebook,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:214,modifiability,version,version,214,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:550,modifiability,exten,extended,550,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2107,modifiability,layer,layers,2107,"ems():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.Ran",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:8208,modifiability,Version,Versions,8208,"nd.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. .... repeated. ```. ### Versions. ```. -----. anndata 0.10.9. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15.0. chardet 5.2.0. charset_normalizer 2.0.9. colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:8521,modifiability,deco,decorator,8521,"rand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. .... repeated. ```. ### Versions. ```. -----. anndata 0.10.9. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15.0. chardet 5.2.0. charset_normalizer 2.0.9. colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. pooch v1.7.0. prometheus_client NA. prompt_toolkit 3.0.22. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.10.0. pyparsing 3.1.4. pythoncom NA. pythonjsonlogger NA.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:9158,modifiability,pac,packaging,9158,RandomState.randint. .... repeated. ```. ### Versions. ```. -----. anndata 0.10.9. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15.0. chardet 5.2.0. charset_normalizer 2.0.9. colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. pooch v1.7.0. prometheus_client NA. prompt_toolkit 3.0.22. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.10.0. pyparsing 3.1.4. pythoncom NA. pythonjsonlogger NA. pytz 2024.1. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. referencing NA. requests 2.32.3. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.1. send2trash NA. session_info 1.0.0. setuptools_scm NA. simplejson 3.19.2. six 1.16.0. sklearn 1.5.1. sniffio 1.3.1. storemagic NA. threadpoolctl 3.5.0. torch 1.10.1+cpu. tornado 6.4.1. tqdm 4.66.5. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 1.26.7. wcwidth 0.2.5. webcolors 24.8.0. websocket 1.8.0. win32api NA. win32com NA. win32con NA. win32security NA. win32trace NA. winerror NA. yaml 6.0. zipp NA. zmq 26.2.0. zoneinfo NA. -----. IPy,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2937,performance,Error,Error,2937,"t_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'Va",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:9868,performance,cpu,cpu,9868,"colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. pooch v1.7.0. prometheus_client NA. prompt_toolkit 3.0.22. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.10.0. pyparsing 3.1.4. pythoncom NA. pythonjsonlogger NA. pytz 2024.1. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. referencing NA. requests 2.32.3. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.1. send2trash NA. session_info 1.0.0. setuptools_scm NA. simplejson 3.19.2. six 1.16.0. sklearn 1.5.1. sniffio 1.3.1. storemagic NA. threadpoolctl 3.5.0. torch 1.10.1+cpu. tornado 6.4.1. tqdm 4.66.5. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 1.26.7. wcwidth 0.2.5. webcolors 24.8.0. websocket 1.8.0. win32api NA. win32com NA. win32con NA. win32security NA. win32trace NA. winerror NA. yaml 6.0. zipp NA. zmq 26.2.0. zoneinfo NA. -----. IPython 7.29.0. jupyter_client 7.4.9. jupyter_core 5.7.2. jupyterlab 4.2.5. notebook 7.2.2. -----. Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. -----. Session information updated at 2024-09-12 19:03. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:18,reliability,doe,does,18,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:447,reliability,doe,doesn,447,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2212,safety,Log,Logarithmize,2212,"e_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2425,safety,log,log,2425,"for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.Rand",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2826,safety,prevent,prevent,2826,"_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.rando",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2937,safety,Error,Error,2937,"t_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'Va",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2960,safety,Except,Exception,2960,"er_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:3276,safety,Except,Exception,3276,"s(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:3592,safety,Except,Exception,3592,"lor=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:3908,safety,Except,Exception,3908,"iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:4224,safety,Except,Exception,4224,". ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:4540,safety,Except,Exception,4540,". ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:4856,safety,Except,Exception,4856,". ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:5172,safety,Except,Exception,5172,". ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:5488,safety,Except,Exception,5488,". ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:5804,safety,Except,Exception,5804,". ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:6120,safety,Except,Exception,6120,". ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:6436,safety,Except,Exception,6436,". ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:6752,safety,Except,Exception,6752,". ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:7068,safety,Except,Exception,7068,". ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (mo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:7384,safety,Except,Exception,7384,". ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. .... repeated. ```. ### Versions. ```. -----. anndata 0.10.9. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:7700,safety,Except,Exception,7700,". ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. .... repeated. ```. ### Versions. ```. -----. anndata 0.10.9. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15.0. chardet 5.2.0. charset_normalizer 2.0.9. colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:8016,safety,Except,Exception,8016,". ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. .... repeated. ```. ### Versions. ```. -----. anndata 0.10.9. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15.0. chardet 5.2.0. charset_normalizer 2.0.9. colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:8573,safety,except,exceptiongroup,8573,"d_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. .... repeated. ```. ### Versions. ```. -----. anndata 0.10.9. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15.0. chardet 5.2.0. charset_normalizer 2.0.9. colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. pooch v1.7.0. prometheus_client NA. prompt_toolkit 3.0.22. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.10.0. pyparsing 3.1.4. pythoncom NA. pythonjsonlogger NA. pytz 2024.1. pywin32_bootstrap NA. pywin32_system32 N",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:10402,safety,updat,updated,10402,"colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. pooch v1.7.0. prometheus_client NA. prompt_toolkit 3.0.22. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.10.0. pyparsing 3.1.4. pythoncom NA. pythonjsonlogger NA. pytz 2024.1. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. referencing NA. requests 2.32.3. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.1. send2trash NA. session_info 1.0.0. setuptools_scm NA. simplejson 3.19.2. six 1.16.0. sklearn 1.5.1. sniffio 1.3.1. storemagic NA. threadpoolctl 3.5.0. torch 1.10.1+cpu. tornado 6.4.1. tqdm 4.66.5. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 1.26.7. wcwidth 0.2.5. webcolors 24.8.0. websocket 1.8.0. win32api NA. win32com NA. win32con NA. win32security NA. win32trace NA. winerror NA. yaml 6.0. zipp NA. zmq 26.2.0. zoneinfo NA. -----. IPython 7.29.0. jupyter_client 7.4.9. jupyter_core 5.7.2. jupyterlab 4.2.5. notebook 7.2.2. -----. Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. -----. Session information updated at 2024-09-12 19:03. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2212,security,Log,Logarithmize,2212,"e_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2425,security,log,log,2425,"for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.Rand",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2826,security,preven,prevent,2826,"_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.rando",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:8360,security,certif,certifi,8360,"f bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. .... repeated. ```. ### Versions. ```. -----. anndata 0.10.9. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15.0. chardet 5.2.0. charset_normalizer 2.0.9. colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. pooch v1.7.0. prometheus_client NA. prompt_toolkit 3.0.22. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:8740,security,iso,isoduration,8740,"aceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. .... repeated. ```. ### Versions. ```. -----. anndata 0.10.9. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15.0. chardet 5.2.0. charset_normalizer 2.0.9. colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. pooch v1.7.0. prometheus_client NA. prompt_toolkit 3.0.22. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.10.0. pyparsing 3.1.4. pythoncom NA. pythonjsonlogger NA. pytz 2024.1. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. referencing NA. requests 2.32.3. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.1. send2trash NA. session_info 1.0.0. setupto",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:10382,security,Session,Session,10382,"colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. pooch v1.7.0. prometheus_client NA. prompt_toolkit 3.0.22. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.10.0. pyparsing 3.1.4. pythoncom NA. pythonjsonlogger NA. pytz 2024.1. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. referencing NA. requests 2.32.3. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.1. send2trash NA. session_info 1.0.0. setuptools_scm NA. simplejson 3.19.2. six 1.16.0. sklearn 1.5.1. sniffio 1.3.1. storemagic NA. threadpoolctl 3.5.0. torch 1.10.1+cpu. tornado 6.4.1. tqdm 4.66.5. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 1.26.7. wcwidth 0.2.5. webcolors 24.8.0. websocket 1.8.0. win32api NA. win32com NA. win32con NA. win32security NA. win32trace NA. winerror NA. yaml 6.0. zipp NA. zmq 26.2.0. zoneinfo NA. -----. IPython 7.29.0. jupyter_client 7.4.9. jupyter_core 5.7.2. jupyterlab 4.2.5. notebook 7.2.2. -----. Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. -----. Session information updated at 2024-09-12 19:03. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:10402,security,updat,updated,10402,"colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. pooch v1.7.0. prometheus_client NA. prompt_toolkit 3.0.22. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.10.0. pyparsing 3.1.4. pythoncom NA. pythonjsonlogger NA. pytz 2024.1. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. referencing NA. requests 2.32.3. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.1. send2trash NA. session_info 1.0.0. setuptools_scm NA. simplejson 3.19.2. six 1.16.0. sklearn 1.5.1. sniffio 1.3.1. storemagic NA. threadpoolctl 3.5.0. torch 1.10.1+cpu. tornado 6.4.1. tqdm 4.66.5. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 1.26.7. wcwidth 0.2.5. webcolors 24.8.0. websocket 1.8.0. win32api NA. win32com NA. win32con NA. win32security NA. win32trace NA. winerror NA. yaml 6.0. zipp NA. zmq 26.2.0. zoneinfo NA. -----. IPython 7.29.0. jupyter_client 7.4.9. jupyter_core 5.7.2. jupyterlab 4.2.5. notebook 7.2.2. -----. Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. -----. Session information updated at 2024-09-12 19:03. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2212,testability,Log,Logarithmize,2212,"e_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2425,testability,log,log,2425,"for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.Rand",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:3004,testability,Trace,Traceback,3004,"_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:3320,testability,Trace,Traceback,3320,"""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:3636,testability,Trace,Traceback,3636,"ct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:3952,testability,Trace,Traceback,3952,"b. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:4268,testability,Trace,Traceback,4268,"2. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:4584,testability,Trace,Traceback,4584,"2. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:4900,testability,Trace,Traceback,4900,"2. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:5216,testability,Trace,Traceback,5216,"2. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:5532,testability,Trace,Traceback,5532,"2. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:5848,testability,Trace,Traceback,5848,"2. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:6164,testability,Trace,Traceback,6164,"2. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:6480,testability,Trace,Traceback,6480,"2. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:6796,testability,Trace,Traceback,6796,"2. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:7112,testability,Trace,Traceback,7112,"2. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:7428,testability,Trace,Traceback,7428,"2. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. .... repeated. ```. ### Versions. ```. -----. anndata 0.10.9. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15.0. chardet 5.2.0. charset_normalizer 2.0.9.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:7744,testability,Trace,Traceback,7744,"2. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. .... repeated. ```. ### Versions. ```. -----. anndata 0.10.9. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15.0. chardet 5.2.0. charset_normalizer 2.0.9. colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isodurati",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:8060,testability,Trace,Traceback,8060,"2. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. .... repeated. ```. ### Versions. ```. -----. anndata 0.10.9. scanpy 1.10.2. -----. PIL 10.4.0. anyio NA. arrow 1.3.0. attr 24.2.0. attrs 24.2.0. babel 2.16.0. backcall 0.2.0. certifi 2021.10.08. cffi 1.15.0. chardet 5.2.0. charset_normalizer 2.0.9. colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsor",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:9758,testability,simpl,simplejson,9758,"colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. pooch v1.7.0. prometheus_client NA. prompt_toolkit 3.0.22. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.10.0. pyparsing 3.1.4. pythoncom NA. pythonjsonlogger NA. pytz 2024.1. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. referencing NA. requests 2.32.3. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.1. send2trash NA. session_info 1.0.0. setuptools_scm NA. simplejson 3.19.2. six 1.16.0. sklearn 1.5.1. sniffio 1.3.1. storemagic NA. threadpoolctl 3.5.0. torch 1.10.1+cpu. tornado 6.4.1. tqdm 4.66.5. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 1.26.7. wcwidth 0.2.5. webcolors 24.8.0. websocket 1.8.0. win32api NA. win32com NA. win32con NA. win32security NA. win32trace NA. winerror NA. yaml 6.0. zipp NA. zmq 26.2.0. zoneinfo NA. -----. IPython 7.29.0. jupyter_client 7.4.9. jupyter_core 5.7.2. jupyterlab 4.2.5. notebook 7.2.2. -----. Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. -----. Session information updated at 2024-09-12 19:03. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:174,usability,confirm,confirmed,174,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:257,usability,confirm,confirmed,257,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:572,usability,Minim,Minimal,572,"Leiden clustering does not seem to terminate; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I am following the tutorial for Scanpy, but when I run the Leiden clustering section it runs for multiple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:1431,usability,mous,mouse,1431,"ple days and doesn't seem to terminate. Is this normal? If so could a note be added to the tutorial in terms of the extended runtime? ### Minimal code sample. ```python. import scanpy as sc. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. # Core scverse libraries. import anndata as ad. # Data retrieval. import pooch. EXAMPLE_DATA = pooch.create(. path=pooch.os_cache(""scverse_tutorials""),. base_url=""doi:10.6084/m9.figshare.22716739.v1/"",. ). EXAMPLE_DATA.load_registry_from_doi(). samples = {. ""s1d1"": ""s1d1_filtered_feature_bc_matrix.h5"",. ""s1d3"": ""s1d3_filtered_feature_bc_matrix.h5"",. }. adatas = {}. for sample_id, filename in samples.items():. path = EXAMPLE_DATA.fetch(filename). sample_adata = sc.read_10x_h5(path). sample_adata.var_names_make_unique(). adatas[sample_id] = sample_adata. adata = ad.concat(adatas, label=""sample""). adata.obs_names_make_unique(). print(adata.obs[""sample""].value_counts()). # mitochondrial genes, ""MT-"" for human, ""Mt-"" for mouse. adata.var[""mt""] = adata.var_names.str.startswith(""MT-""). # ribosomal genes. adata.var[""ribo""] = adata.var_names.str.startswith((""RPS"", ""RPL"")). # hemoglobin genes. adata.var[""hb""] = adata.var_names.str.contains(""^HB[^(P)]""). sc.pp.calculate_qc_metrics(. adata, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). sc.pl.violin(. adata,. [""n_genes_by_counts"", ""total_counts"", ""pct_counts_mt""],. jitter=0.4,. multi_panel=True,. ). sc.pl.scatter(adata, ""total_counts"", ""n_genes_by_counts"", color=""pct_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:2937,usability,Error,Error,2937,"t_counts_mt""). sc.pp.filter_cells(adata, min_genes=100). sc.pp.filter_genes(adata, min_cells=3). sc.pp.scrublet(adata, batch_key=""sample""). # Saving count data. adata.layers[""counts""] = adata.X.copy(). # Normalizing to median total counts. sc.pp.normalize_total(adata). # Logarithmize the data. sc.pp.log1p(adata). sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=""sample""). sc.pl.highly_variable_genes(adata). sc.tl.pca(adata). sc.pl.pca_variance_ratio(adata, n_pcs=50, log=True). sc.pp.neighbors(adata). sc.tl.umap(adata). sc.tl.leiden(. adata, key_added=""clusters"", flavor=""igraph"", directed=False, n_iterations=2. ). sc.pl.pca(. adata,. color=[""sample"", ""sample"", ""pct_counts_mt"", ""pct_counts_mt""],. dimensions=[(0, 1), (2, 3), (0, 1), (2, 3)],. ncols=2,. size=2,. ). sc.pp.neighbors(adata). sc.pl.umap(. adata,. color=""sample"",. # Setting a smaller point size to get prevent overlap. size=2,. ). ### runs forever:. sc.tl.leiden(adata, flavor=""igraph"", n_iterations=2). ```. ### Error output. ```pytb. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'ValueError'>. Traceback (most recent call last):. File ""numpy\\random\\mtrand.pyx"", line 780, in numpy.random.mtrand.RandomState.randint. File ""numpy\\random\\_bounded_integers.pyx"", line 2881, in numpy.random._bounded_integers._rand_int32. ValueError: high is out of bounds for int32. Exception ignored in: <class 'Va",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/issues/3228:9758,usability,simpl,simplejson,9758,"colorama 0.4.6. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0.post0. debugpy 1.5.1. decorator 5.1.0. defusedxml 0.7.1. entrypoints 0.3. exceptiongroup 1.2.2. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.3. importlib_resources NA. ipykernel 6.5.0. ipython_genutils 0.2.0. ipywidgets 7.7.0. isoduration NA. jedi 0.18.0. jinja2 3.0.3. joblib 1.4.2. json5 0.9.25. jsonpointer 3.0.0. jsonschema 4.23.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.2. jupyterlab_server 2.27.3. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. markupsafe 2.0.1. matplotlib 3.9.2. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. nt NA. ntsecuritycon NA. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.1. pandas 2.2.2. parso 0.8.2. pickleshare 0.7.5. pkg_resources NA. platformdirs 3.5.1. pooch v1.7.0. prometheus_client NA. prompt_toolkit 3.0.22. pydev_ipython NA. pydevconsole NA. pydevd 2.6.0. pydevd_concurrency_analyser NA. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.10.0. pyparsing 3.1.4. pythoncom NA. pythonjsonlogger NA. pytz 2024.1. pywin32_bootstrap NA. pywin32_system32 NA. pywintypes NA. referencing NA. requests 2.32.3. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rpds NA. scipy 1.13.1. send2trash NA. session_info 1.0.0. setuptools_scm NA. simplejson 3.19.2. six 1.16.0. sklearn 1.5.1. sniffio 1.3.1. storemagic NA. threadpoolctl 3.5.0. torch 1.10.1+cpu. tornado 6.4.1. tqdm 4.66.5. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 1.26.7. wcwidth 0.2.5. webcolors 24.8.0. websocket 1.8.0. win32api NA. win32com NA. win32con NA. win32security NA. win32trace NA. winerror NA. yaml 6.0. zipp NA. zmq 26.2.0. zoneinfo NA. -----. IPython 7.29.0. jupyter_client 7.4.9. jupyter_core 5.7.2. jupyterlab 4.2.5. notebook 7.2.2. -----. Python 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.22621-SP0. -----. Session information updated at 2024-09-12 19:03. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3228
https://github.com/scverse/scanpy/pull/3235:25,deployability,releas,release,25,(chore): generate 1.10.3 release notes; @meeseeksmachine backport to main,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3235
https://github.com/scverse/scanpy/pull/3238:59,deployability,releas,release,59,Backport PR #3235 on branch main ((chore): generate 1.10.3 release notes); Backport PR #3235: (chore): generate 1.10.3 release notes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3238
https://github.com/scverse/scanpy/pull/3238:119,deployability,releas,release,119,Backport PR #3235 on branch main ((chore): generate 1.10.3 release notes); Backport PR #3235: (chore): generate 1.10.3 release notes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3238
https://github.com/scverse/scanpy/pull/3239:4,deployability,releas,release,4,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3239:17,deployability,build,building,17,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3239:142,deployability,version,versioning,142,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3239:171,deployability,releas,release,171,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3239:240,deployability,releas,release,240,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3239:284,deployability,releas,release,284,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3239:353,deployability,version,version,353,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3239:441,deployability,releas,release,441,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3239:142,integrability,version,versioning,142,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3239:353,integrability,version,version,353,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3239:133,interoperability,Semant,Semantic,133,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3239:89,modifiability,exten,extending,89,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3239:142,modifiability,version,versioning,142,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3239:353,modifiability,version,version,353,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3239:701,usability,user,user-attachments,701,"Fix release note building and check; See https://setuptools-scm.readthedocs.io/en/latest/extending/#setuptools_scmversion_scheme:. > Semantic versioning for projects with release branches. The same as `guess-next-dev` (incrementing the pre-release or micro segment) however when on a release branch: a branch whose name (ignoring namespace) parses as a version that matches the most recent tag up to the minor segment. Otherwise if on a non-release branch, increments the minor segment and sets the micro segment to zero, then appends `.devN`. Apparently the “ignoring namespace” makes it work with our `<major>.<minor>.x` branch names. I checked if the new check works:. ![grafik](https://github.com/user-attachments/assets/a2620352-f688-47d3-9481-f621783f4ecf).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3239
https://github.com/scverse/scanpy/pull/3240:40,deployability,releas,release,40,Backport PR #3239 on branch 1.10.x (Fix release note building and check); Backport PR #3239: Fix release note building and check,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3240
https://github.com/scverse/scanpy/pull/3240:53,deployability,build,building,53,Backport PR #3239 on branch 1.10.x (Fix release note building and check); Backport PR #3239: Fix release note building and check,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3240
https://github.com/scverse/scanpy/pull/3240:97,deployability,releas,release,97,Backport PR #3239 on branch 1.10.x (Fix release note building and check); Backport PR #3239: Fix release note building and check,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3240
https://github.com/scverse/scanpy/pull/3240:110,deployability,build,building,110,Backport PR #3239 on branch 1.10.x (Fix release note building and check); Backport PR #3239: Fix release note building and check,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3240
https://github.com/scverse/scanpy/issues/3241:580,availability,cluster,clustering,580,"Why making feature names unique instead of aggregation?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi scanpy team! I have a rather conceptual question. Since the beginning of the single-cell analysis era, one of the standard steps in preprocessing is making the feature names unique (e.g. with `adata.var_names_make_unique()`) by adding suffixes to their names. It is recommended in the [scanpy tutorial](https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html) and in the [best practices book](https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html). It is clear how identical feature names make the following data processing challenging, but why are we handling it this way? Wouldn't it make more sense to aggregate features with identical names, summing the counts? From the biological point of view, the same gene name means the same feature, so why split it into several features and corrupt their names?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241
https://github.com/scverse/scanpy/issues/3241:580,deployability,cluster,clustering,580,"Why making feature names unique instead of aggregation?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi scanpy team! I have a rather conceptual question. Since the beginning of the single-cell analysis era, one of the standard steps in preprocessing is making the feature names unique (e.g. with `adata.var_names_make_unique()`) by adding suffixes to their names. It is recommended in the [scanpy tutorial](https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html) and in the [best practices book](https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html). It is clear how identical feature names make the following data processing challenging, but why are we handling it this way? Wouldn't it make more sense to aggregate features with identical names, summing the counts? From the biological point of view, the same gene name means the same feature, so why split it into several features and corrupt their names?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241
https://github.com/scverse/scanpy/issues/3241:334,interoperability,standard,standard,334,"Why making feature names unique instead of aggregation?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi scanpy team! I have a rather conceptual question. Since the beginning of the single-cell analysis era, one of the standard steps in preprocessing is making the feature names unique (e.g. with `adata.var_names_make_unique()`) by adding suffixes to their names. It is recommended in the [scanpy tutorial](https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html) and in the [best practices book](https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html). It is clear how identical feature names make the following data processing challenging, but why are we handling it this way? Wouldn't it make more sense to aggregate features with identical names, summing the counts? From the biological point of view, the same gene name means the same feature, so why split it into several features and corrupt their names?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241
https://github.com/scverse/scanpy/issues/3241:129,modifiability,paramet,parameters,129,"Why making feature names unique instead of aggregation?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi scanpy team! I have a rather conceptual question. Since the beginning of the single-cell analysis era, one of the standard steps in preprocessing is making the feature names unique (e.g. with `adata.var_names_make_unique()`) by adding suffixes to their names. It is recommended in the [scanpy tutorial](https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html) and in the [best practices book](https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html). It is clear how identical feature names make the following data processing challenging, but why are we handling it this way? Wouldn't it make more sense to aggregate features with identical names, summing the counts? From the biological point of view, the same gene name means the same feature, so why split it into several features and corrupt their names?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241
https://github.com/scverse/scanpy/issues/3241:614,reliability,pra,practices,614,"Why making feature names unique instead of aggregation?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi scanpy team! I have a rather conceptual question. Since the beginning of the single-cell analysis era, one of the standard steps in preprocessing is making the feature names unique (e.g. with `adata.var_names_make_unique()`) by adding suffixes to their names. It is recommended in the [scanpy tutorial](https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html) and in the [best practices book](https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html). It is clear how identical feature names make the following data processing challenging, but why are we handling it this way? Wouldn't it make more sense to aggregate features with identical names, summing the counts? From the biological point of view, the same gene name means the same feature, so why split it into several features and corrupt their names?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241
https://github.com/scverse/scanpy/issues/3241:650,reliability,pra,practices,650,"Why making feature names unique instead of aggregation?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi scanpy team! I have a rather conceptual question. Since the beginning of the single-cell analysis era, one of the standard steps in preprocessing is making the feature names unique (e.g. with `adata.var_names_make_unique()`) by adding suffixes to their names. It is recommended in the [scanpy tutorial](https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html) and in the [best practices book](https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html). It is clear how identical feature names make the following data processing challenging, but why are we handling it this way? Wouldn't it make more sense to aggregate features with identical names, summing the counts? From the biological point of view, the same gene name means the same feature, so why split it into several features and corrupt their names?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241
https://github.com/scverse/scanpy/issues/3241:227,security,team,team,227,"Why making feature names unique instead of aggregation?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi scanpy team! I have a rather conceptual question. Since the beginning of the single-cell analysis era, one of the standard steps in preprocessing is making the feature names unique (e.g. with `adata.var_names_make_unique()`) by adding suffixes to their names. It is recommended in the [scanpy tutorial](https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html) and in the [best practices book](https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html). It is clear how identical feature names make the following data processing challenging, but why are we handling it this way? Wouldn't it make more sense to aggregate features with identical names, summing the counts? From the biological point of view, the same gene name means the same feature, so why split it into several features and corrupt their names?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241
https://github.com/scverse/scanpy/issues/3241:731,security,ident,identical,731,"Why making feature names unique instead of aggregation?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi scanpy team! I have a rather conceptual question. Since the beginning of the single-cell analysis era, one of the standard steps in preprocessing is making the feature names unique (e.g. with `adata.var_names_make_unique()`) by adding suffixes to their names. It is recommended in the [scanpy tutorial](https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html) and in the [best practices book](https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html). It is clear how identical feature names make the following data processing challenging, but why are we handling it this way? Wouldn't it make more sense to aggregate features with identical names, summing the counts? From the biological point of view, the same gene name means the same feature, so why split it into several features and corrupt their names?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241
https://github.com/scverse/scanpy/issues/3241:895,security,ident,identical,895,"Why making feature names unique instead of aggregation?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi scanpy team! I have a rather conceptual question. Since the beginning of the single-cell analysis era, one of the standard steps in preprocessing is making the feature names unique (e.g. with `adata.var_names_make_unique()`) by adding suffixes to their names. It is recommended in the [scanpy tutorial](https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html) and in the [best practices book](https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html). It is clear how identical feature names make the following data processing challenging, but why are we handling it this way? Wouldn't it make more sense to aggregate features with identical names, summing the counts? From the biological point of view, the same gene name means the same feature, so why split it into several features and corrupt their names?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241
https://github.com/scverse/scanpy/issues/3241:721,usability,clear,clear,721,"Why making feature names unique instead of aggregation?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi scanpy team! I have a rather conceptual question. Since the beginning of the single-cell analysis era, one of the standard steps in preprocessing is making the feature names unique (e.g. with `adata.var_names_make_unique()`) by adding suffixes to their names. It is recommended in the [scanpy tutorial](https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering.html) and in the [best practices book](https://www.sc-best-practices.org/preprocessing_visualization/quality_control.html). It is clear how identical feature names make the following data processing challenging, but why are we handling it this way? Wouldn't it make more sense to aggregate features with identical names, summing the counts? From the biological point of view, the same gene name means the same feature, so why split it into several features and corrupt their names?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3241
https://github.com/scverse/scanpy/pull/3242:50,availability,failur,failure,50,Backport PR #3069: Upload scrublet scores on test failure; Backport PR #3069: Upload scrublet scores on test failure,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3242
https://github.com/scverse/scanpy/pull/3242:109,availability,failur,failure,109,Backport PR #3069: Upload scrublet scores on test failure; Backport PR #3069: Upload scrublet scores on test failure,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3242
https://github.com/scverse/scanpy/pull/3242:50,deployability,fail,failure,50,Backport PR #3069: Upload scrublet scores on test failure; Backport PR #3069: Upload scrublet scores on test failure,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3242
https://github.com/scverse/scanpy/pull/3242:109,deployability,fail,failure,109,Backport PR #3069: Upload scrublet scores on test failure; Backport PR #3069: Upload scrublet scores on test failure,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3242
https://github.com/scverse/scanpy/pull/3242:50,performance,failur,failure,50,Backport PR #3069: Upload scrublet scores on test failure; Backport PR #3069: Upload scrublet scores on test failure,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3242
https://github.com/scverse/scanpy/pull/3242:109,performance,failur,failure,109,Backport PR #3069: Upload scrublet scores on test failure; Backport PR #3069: Upload scrublet scores on test failure,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3242
https://github.com/scverse/scanpy/pull/3242:50,reliability,fail,failure,50,Backport PR #3069: Upload scrublet scores on test failure; Backport PR #3069: Upload scrublet scores on test failure,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3242
https://github.com/scverse/scanpy/pull/3242:109,reliability,fail,failure,109,Backport PR #3069: Upload scrublet scores on test failure; Backport PR #3069: Upload scrublet scores on test failure,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3242
https://github.com/scverse/scanpy/pull/3242:45,safety,test,test,45,Backport PR #3069: Upload scrublet scores on test failure; Backport PR #3069: Upload scrublet scores on test failure,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3242
https://github.com/scverse/scanpy/pull/3242:104,safety,test,test,104,Backport PR #3069: Upload scrublet scores on test failure; Backport PR #3069: Upload scrublet scores on test failure,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3242
https://github.com/scverse/scanpy/pull/3242:45,testability,test,test,45,Backport PR #3069: Upload scrublet scores on test failure; Backport PR #3069: Upload scrublet scores on test failure,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3242
https://github.com/scverse/scanpy/pull/3242:104,testability,test,test,104,Backport PR #3069: Upload scrublet scores on test failure; Backport PR #3069: Upload scrublet scores on test failure,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3242
https://github.com/scverse/scanpy/pull/3243:332,availability,toler,tolerance,332,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:580,availability,toler,tolerance,580,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:179,deployability,observ,observations,179,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:113,energy efficiency,heat,heatmap,113,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:38,modifiability,paramet,parameter,38,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:166,modifiability,variab,variables,166,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:202,modifiability,variab,variables,202,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:254,performance,time,time,254,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:332,reliability,toleran,tolerance,332,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:580,reliability,toleran,tolerance,580,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:53,safety,test,tests,53,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:234,safety,test,test,234,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:387,safety,test,tests,387,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:423,safety,test,tests,423,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:496,safety,test,test,496,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:800,safety,test,tests,800,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:955,safety,test,tests,955,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:53,testability,test,tests,53,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:179,testability,observ,observations,179,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:234,testability,test,test,234,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:387,testability,test,tests,387,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:423,testability,test,tests,423,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:496,testability,test,test,496,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:800,testability,test,tests,800,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:955,testability,test,tests,955,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3243:519,usability,behavi,behavior,519,"Fix stacked_violin’s `standard_scale` parameter; The tests use `group` but the code handles `obs`. TODO: should `heatmap` be changed too? Its docs actually speak of “variables or observations” and not “variables or groups”. About the test changes:. Some time ago, matplotlib made a change to font rendering. Since we have such high tolerance when comparing plots, that didn’t affect our tests. But that also means that our tests are almost useless, since the actual qualitative difference in the test that _did_ change behavior due to my PR wasn’t caught. Therefore I lowered the tolerance, which meant I had to regenerate everything with the new font rendering. | Before | After |. |--------|--------|. | ![](https://raw.githubusercontent.com/scverse/scanpy/bd758395a669c31a6c9eaa9239750fde368d3ca7/tests/_images/stacked_violin_std_scale_group/expected.png) | ![](https://raw.githubusercontent.com/scverse/scanpy/c06bbc83218ee426fa54e681ab39c8006e1668c0/tests/_images/stacked_violin_std_scale_group/expected.png) |",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3243
https://github.com/scverse/scanpy/pull/3244:8,deployability,scale,scale,8,Finish `scale`→`density_norm` deprecation; `sc.pl.violin` and `sc.pl.stacked_violin` were missed in #2844,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3244
https://github.com/scverse/scanpy/pull/3244:8,energy efficiency,scale,scale,8,Finish `scale`→`density_norm` deprecation; `sc.pl.violin` and `sc.pl.stacked_violin` were missed in #2844,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3244
https://github.com/scverse/scanpy/pull/3244:8,modifiability,scal,scale,8,Finish `scale`→`density_norm` deprecation; `sc.pl.violin` and `sc.pl.stacked_violin` were missed in #2844,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3244
https://github.com/scverse/scanpy/pull/3244:8,performance,scale,scale,8,Finish `scale`→`density_norm` deprecation; `sc.pl.violin` and `sc.pl.stacked_violin` were missed in #2844,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3244
https://github.com/scverse/scanpy/issues/3245:5,integrability,sub,submodule,5,`io` submodule from `anndata`; ### What kind of feature would you like to request? Other? ### Please describe your wishes. We need to handle https://github.com/scverse/anndata/pull/1682 here so that warnings are not thrown on import. - [ ] remove all references to `anndata._io` from inside the codebase if there are any. - [x] move all `anndata.read_xxxx` usages to `anndata.io.read_xxxx` https://github.com/scverse/scanpy/pull/3289. - [ ] Remove deprecated `anndata.read` function,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3245
https://github.com/scverse/scanpy/pull/3246:74,modifiability,paramet,parameter,74,Backport PR #3243 on branch 1.10.x (Fix stacked_violin’s `standard_scale` parameter); Backport PR #3243: Fix stacked_violin’s `standard_scale` parameter,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3246
https://github.com/scverse/scanpy/pull/3246:143,modifiability,paramet,parameter,143,Backport PR #3243 on branch 1.10.x (Fix stacked_violin’s `standard_scale` parameter); Backport PR #3243: Fix stacked_violin’s `standard_scale` parameter,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3246
https://github.com/scverse/scanpy/pull/3247:44,deployability,scale,scale,44,Backport PR #3244 on branch 1.10.x (Finish `scale`→`density_norm` deprecation); Backport PR #3244: Finish `scale`→`density_norm` deprecation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3247
https://github.com/scverse/scanpy/pull/3247:107,deployability,scale,scale,107,Backport PR #3244 on branch 1.10.x (Finish `scale`→`density_norm` deprecation); Backport PR #3244: Finish `scale`→`density_norm` deprecation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3247
https://github.com/scverse/scanpy/pull/3247:44,energy efficiency,scale,scale,44,Backport PR #3244 on branch 1.10.x (Finish `scale`→`density_norm` deprecation); Backport PR #3244: Finish `scale`→`density_norm` deprecation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3247
https://github.com/scverse/scanpy/pull/3247:107,energy efficiency,scale,scale,107,Backport PR #3244 on branch 1.10.x (Finish `scale`→`density_norm` deprecation); Backport PR #3244: Finish `scale`→`density_norm` deprecation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3247
https://github.com/scverse/scanpy/pull/3247:44,modifiability,scal,scale,44,Backport PR #3244 on branch 1.10.x (Finish `scale`→`density_norm` deprecation); Backport PR #3244: Finish `scale`→`density_norm` deprecation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3247
https://github.com/scverse/scanpy/pull/3247:107,modifiability,scal,scale,107,Backport PR #3244 on branch 1.10.x (Finish `scale`→`density_norm` deprecation); Backport PR #3244: Finish `scale`→`density_norm` deprecation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3247
https://github.com/scverse/scanpy/pull/3247:44,performance,scale,scale,44,Backport PR #3244 on branch 1.10.x (Finish `scale`→`density_norm` deprecation); Backport PR #3244: Finish `scale`→`density_norm` deprecation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3247
https://github.com/scverse/scanpy/pull/3247:107,performance,scale,scale,107,Backport PR #3244 on branch 1.10.x (Finish `scale`→`density_norm` deprecation); Backport PR #3244: Finish `scale`→`density_norm` deprecation,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3247
https://github.com/scverse/scanpy/pull/3252:26,modifiability,paramet,parameter,26,"Deprecate defunct `order` parameter in `stacked_violin`; That one never worked. This PR also adds typing for `categories_order`, which has been supported.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3252
https://github.com/scverse/scanpy/pull/3252:144,usability,support,supported,144,"Deprecate defunct `order` parameter in `stacked_violin`; That one never worked. This PR also adds typing for `categories_order`, which has been supported.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3252
https://github.com/scverse/scanpy/pull/3253:62,modifiability,paramet,parameter,62,Backport PR #3252 on branch 1.10.x (Deprecate defunct `order` parameter in `stacked_violin`); Backport PR #3252: Deprecate defunct `order` parameter in `stacked_violin`,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3253
https://github.com/scverse/scanpy/pull/3253:139,modifiability,paramet,parameter,139,Backport PR #3252 on branch 1.10.x (Deprecate defunct `order` parameter in `stacked_violin`); Backport PR #3252: Deprecate defunct `order` parameter in `stacked_violin`,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3253
https://github.com/scverse/scanpy/issues/3255:439,deployability,log,logarithm,439,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3255:521,energy efficiency,current,currently,521,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3255:339,reliability,pra,practices,339,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3255:370,reliability,pra,practices,370,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3255:439,safety,log,logarithm,439,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3255:439,security,log,logarithm,439,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3255:109,testability,simpl,simple,109,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3255:439,testability,log,logarithm,439,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3255:463,testability,simpl,simple,463,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3255:101,usability,tool,tool,101,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3255:109,usability,simpl,simple,109,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3255:125,usability,tool,tool,125,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3255:173,usability,tool,tools,173,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3255:463,usability,simpl,simple,463,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3255:479,usability,workflow,workflows,479,"Add scran nomalisation into scanpy; ### What kind of feature would you like to request? New analysis tool: A simple analysis tool you have been using and are missing in `sc.tools`? ### Please describe your wishes. As previously attempted (#823), it would be amazing to have scran normalisation implemented in scanpy. It's one of the [best practices](https://www.sc-best-practices.org/preprocessing_visualization/normalization.html#shifted-logarithm) and for many simple analysis workflows, it might even be the only step currently requiring to switch to R.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3255
https://github.com/scverse/scanpy/issues/3260:429,availability,error,error,429,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:678,availability,error,error,678,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1100,availability,error,errors,1100,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1382,availability,Error,Error,1382,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:243,deployability,version,version,243,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1628,deployability,Version,Versions,1628,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:610,energy efficiency,model,model-glm,610,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:638,energy efficiency,current,currently,638,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1510,energy efficiency,predict,prediction,1510,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:243,integrability,version,version,243,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1628,integrability,Version,Versions,1628,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:243,modifiability,version,version,243,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1531,modifiability,paramet,parameter,1531,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1628,modifiability,Version,Versions,1628,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:429,performance,error,error,429,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:678,performance,error,error,678,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1100,performance,error,errors,1100,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1382,performance,Error,Error,1382,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:14,reliability,doe,does,14,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:429,safety,error,error,429,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:678,safety,error,error,678,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1100,safety,error,errors,1100,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1382,safety,Error,Error,1382,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1510,safety,predict,prediction,1510,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1521,safety,detect,detected,1521,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:610,security,model,model-glm,610,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1521,security,detect,detected,1521,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1552,security,ident,identified,1552,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:203,usability,confirm,confirmed,203,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:286,usability,confirm,confirmed,286,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:429,usability,error,error,429,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:678,usability,error,error,678,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:716,usability,user,users,716,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1100,usability,error,errors,1100,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1146,usability,Minim,Minimal,1146,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3260:1382,usability,Error,Error,1382,"`regress_out` does not capture PerfectSeparation with statsmodels >= 0.14; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi,. since statsmodels 0.14, perfect separation no longer raises an error but a warning (see [function doc here](https://www.statsmodels.org/stable/generated/statsmodels.genmod.generalized_linear_model.GLM.html#statsmodels-genmod-generalized-linear-model-glm)). Because scanpy currently only catches the now-outdated error (instead of catch the warning), users may see many warnings from `regress_out` when no perfect separation exists (see usage in scanpy [here](https://github.com/scverse/scanpy/blob/main/src/scanpy/preprocessing/_simple.py#L759-L761)). It seems to follow on the heels of [this issue](https://github.com/statsmodels/statsmodels/issues/2680) in statsmodels. I propose to implement that the warning is caught just as the errors were being caught. Cheers,. Jesko. ### Minimal code sample. ```python. import anndata as ad. import scanpy as sc. import numpy as np. import pandas as pd. adata = ad.AnnData(np.array([[0,0,1,1]]).T, obs=pd.DataFrame({""a"":[0,0,1,1]})). sc.pp.regress_out(adata, ""a""). ```. ### Error output. ```pytb. .../statsmodels/genmod/generalized_linear_model.py:1257: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified. warnings.warn(msg, category=PerfectSeparationWarning). ```. ### Versions. <details>. ```. anndata 0.10.4. scanpy 1.9.6. statsmodels 0.14.0. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3260
https://github.com/scverse/scanpy/issues/3261:390,availability,down,downloaded,390,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:731,availability,error,error,731,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:876,availability,Error,Error,876,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:232,deployability,version,version,232,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:1188,deployability,Version,Versions,1188," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:1225,deployability,log,logging,1225," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:2045,deployability,updat,updated,2045," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:993,energy efficiency,core,core,993,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:1823,energy efficiency,cpu,cpu,1823," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:232,integrability,version,version,232,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:1188,integrability,Version,Versions,1188," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:232,modifiability,version,version,232,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:977,modifiability,pac,packages,977,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:1188,modifiability,Version,Versions,1188," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:1622,modifiability,pac,packaging,1622," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:731,performance,error,error,731,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:876,performance,Error,Error,876,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:1823,performance,cpu,cpu,1823," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:731,safety,error,error,731,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:876,safety,Error,Error,876,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:1225,safety,log,logging,1225," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:2045,safety,updat,updated,2045," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:34,security,access,accessor,34,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:362,security,team,teams,362,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:1006,security,access,accessor,1006,"rror: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.190",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:1084,security,access,accessor,1084,"se conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:1150,security,access,accessor,1150," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:1225,security,log,logging,1225," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:2025,security,Session,Session,2025," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:2045,security,updat,updated,2045," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:1225,testability,log,logging,1225," conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-10-10.0.19041-SP0. -----. Session information updated at 2024-09-26 11:05. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:192,usability,confirm,confirmed,192,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:275,usability,confirm,confirmed,275,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:731,usability,error,error,731,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:746,usability,help,help,746,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:770,usability,Minim,Minimal,770,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:876,usability,Error,Error,876,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/issues/3261:908,usability,User,Users,908,"AttributeError: Can only use .str accessor with string values!; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Dear scanpy teams, research fellows,. I downloaded some scRNA-seq data from https://zenodo.org/records/3357167,. and when I was tring to use `anndata.AnnData.concatenate` to combine two read count data(I checked their dimensions and the result were `Baron_human: [2133,22758]` and `Segerstolpe: [8569,17500]` which means they certainly have different annotated genes), I got below error. Could u help. many thanks!! ### Minimal code sample. ```python. all_adata = anndata.AnnData.concatenate(train_adata,test_adata). ```. ### Error output. ```pytb. File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python310\lib\site-packages\pandas\core\strings\accessor.py"", line 245, in _validate. raise AttributeError(""Can only use .str accessor with string values!""). AttributeError: Can only use .str accessor with string values! ```. ### Versions. <details>. ```. >>> scanpy.logging.print_versions(). -----. anndata 0.8.0. scanpy 1.9.3. -----. CIForm NA. PIL 9.1.0. astunparse 1.6.3. cffi 1.15.1. colorama 0.4.6. cycler 0.10.0. cython_runtime NA. dateutil 2.8.2. google NA. h5py 3.11.0. igraph 0.10.4. joblib 1.2.0. kiwisolver 1.4.2. leidenalg 0.9.1. llvmlite 0.39.1. matplotlib 3.5.2. mpl_toolkits NA. natsort 8.3.1. nt NA. numba 0.56.4. numpy 1.23.5. opt_einsum v3.3.0. packaging 21.3. pandas 2.2.3. plotly 5.13.1. psutil 5.9.4. pyparsing 3.0.9. pytz 2022.1. scipy 1.10.0. session_info 1.0.0. six 1.16.0. sklearn 1.2.1. texttable 1.6.7. threadpoolctl 3.1.0. torch 1.13.1+cpu. tqdm 4.64.1. typing_extensions NA. yaml 6.0. zoneinfo NA. zope NA. -----. Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]. Windows-1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3261
https://github.com/scverse/scanpy/pull/3262:446,deployability,releas,release,446,Add sanitize to score_genes; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3262
https://github.com/scverse/scanpy/pull/3262:471,deployability,Releas,Release,471,Add sanitize to score_genes; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3262
https://github.com/scverse/scanpy/pull/3262:4,safety,sanit,sanitize,4,Add sanitize to score_genes; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3262
https://github.com/scverse/scanpy/pull/3262:248,safety,review,review,248,Add sanitize to score_genes; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3262
https://github.com/scverse/scanpy/pull/3262:348,safety,Test,Tests,348,Add sanitize to score_genes; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3262
https://github.com/scverse/scanpy/pull/3262:4,security,sanit,sanitize,4,Add sanitize to score_genes; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3262
https://github.com/scverse/scanpy/pull/3262:248,testability,review,review,248,Add sanitize to score_genes; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3262
https://github.com/scverse/scanpy/pull/3262:348,testability,Test,Tests,348,Add sanitize to score_genes; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3262
https://github.com/scverse/scanpy/pull/3262:99,usability,guid,guidelines,99,Add sanitize to score_genes; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3262
https://github.com/scverse/scanpy/pull/3262:130,usability,guid,guide,130,Add sanitize to score_genes; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3262
https://github.com/scverse/scanpy/pull/3262:226,usability,workflow,workflow,226,Add sanitize to score_genes; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3262
https://github.com/scverse/scanpy/pull/3262:332,usability,Close,Closes,332,Add sanitize to score_genes; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3262
https://github.com/scverse/scanpy/pull/3263:601,availability,error,erroring,601,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:472,deployability,releas,release,472,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:497,deployability,Releas,Release,497,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:701,deployability,integr,integrated,701,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:562,integrability,sub,subclass,562,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:701,integrability,integr,integrated,701,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:701,interoperability,integr,integrated,701,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:701,modifiability,integr,integrated,701,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:601,performance,error,erroring,601,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:701,reliability,integr,integrated,701,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:270,safety,review,review,270,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:374,safety,Test,Tests,374,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:601,safety,error,erroring,601,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:701,security,integr,integrated,701,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:270,testability,review,review,270,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:374,testability,Test,Tests,374,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:701,testability,integr,integrated,701,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:121,usability,guid,guidelines,121,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:152,usability,guid,guide,152,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:248,usability,workflow,workflow,248,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:354,usability,Close,Closes,354,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3263:601,usability,error,erroring,601,"Implement sparse `covariance_eigh` PCA using Dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3226. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. An alternative would be to subclass `PCA`, but that would involve erroring out or reimplementing all of its options. Ideally #3267 would be merged first and this one integrated into its improved decision tree.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3263
https://github.com/scverse/scanpy/pull/3264:305,deployability,api,api-wrap,305,"Fix compat typing and old_positionals usage; This PR overhauls the `_compat` submodule. 1. it fixes the typing exactly like scverse/anndata#1692. 2. it switches all functions and methods to a `legacy_api` wrapper that raises a `FutureWarning`, and paves the way for a potential making-optional of `legacy-api-wrap`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3264
https://github.com/scverse/scanpy/pull/3264:77,integrability,sub,submodule,77,"Fix compat typing and old_positionals usage; This PR overhauls the `_compat` submodule. 1. it fixes the typing exactly like scverse/anndata#1692. 2. it switches all functions and methods to a `legacy_api` wrapper that raises a `FutureWarning`, and paves the way for a potential making-optional of `legacy-api-wrap`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3264
https://github.com/scverse/scanpy/pull/3264:205,integrability,wrap,wrapper,205,"Fix compat typing and old_positionals usage; This PR overhauls the `_compat` submodule. 1. it fixes the typing exactly like scverse/anndata#1692. 2. it switches all functions and methods to a `legacy_api` wrapper that raises a `FutureWarning`, and paves the way for a potential making-optional of `legacy-api-wrap`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3264
https://github.com/scverse/scanpy/pull/3264:305,integrability,api,api-wrap,305,"Fix compat typing and old_positionals usage; This PR overhauls the `_compat` submodule. 1. it fixes the typing exactly like scverse/anndata#1692. 2. it switches all functions and methods to a `legacy_api` wrapper that raises a `FutureWarning`, and paves the way for a potential making-optional of `legacy-api-wrap`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3264
https://github.com/scverse/scanpy/pull/3264:205,interoperability,wrapper,wrapper,205,"Fix compat typing and old_positionals usage; This PR overhauls the `_compat` submodule. 1. it fixes the typing exactly like scverse/anndata#1692. 2. it switches all functions and methods to a `legacy_api` wrapper that raises a `FutureWarning`, and paves the way for a potential making-optional of `legacy-api-wrap`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3264
https://github.com/scverse/scanpy/pull/3264:305,interoperability,api,api-wrap,305,"Fix compat typing and old_positionals usage; This PR overhauls the `_compat` submodule. 1. it fixes the typing exactly like scverse/anndata#1692. 2. it switches all functions and methods to a `legacy_api` wrapper that raises a `FutureWarning`, and paves the way for a potential making-optional of `legacy-api-wrap`",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3264
https://github.com/scverse/scanpy/issues/3266:1648,availability,Error,Error,1648,"```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:241,deployability,version,version,241,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:395,deployability,version,versions,395,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:432,deployability,version,version,432,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:1869,deployability,modul,module,1869,"d_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by con",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:2121,deployability,Version,Versions,2121," gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.10. -----. Session information updated at 2024-09-26 13:44. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:3012,deployability,updat,updated,3012," gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.10. -----. Session information updated at 2024-09-26 13:44. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:2202,energy efficiency,cloud,cloudpickle,2202," gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.10. -----. Session information updated at 2024-09-26 13:44. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:241,integrability,version,version,241,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:395,integrability,version,versions,395,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:432,integrability,version,version,432,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:821,integrability,sub,subset,821,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:2121,integrability,Version,Versions,2121," gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.10. -----. Session information updated at 2024-09-26 13:44. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:241,modifiability,version,version,241,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:395,modifiability,version,versions,395,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:432,modifiability,version,version,432,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:1869,modifiability,modul,module,1869,"d_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by con",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:1933,modifiability,pac,packages,1933,"d""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:2121,modifiability,Version,Versions,2121," gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.10. -----. Session information updated at 2024-09-26 13:44. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:2563,modifiability,pac,packaging,2563," gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.10. -----. Session information updated at 2024-09-26 13:44. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:2857,modifiability,pac,packaged,2857," gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.10. -----. Session information updated at 2024-09-26 13:44. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:1648,performance,Error,Error,1648,"```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:36,safety,valid,valid,36,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:1648,safety,Error,Error,1648,"```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:1869,safety,modul,module,1869,"d_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by con",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:2020,safety,valid,valid,2020,"sl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.10. -----. Session information updated at ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:2075,safety,valid,valid,2075," gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.10. -----. Session information updated at 2024-09-26 13:44. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:3012,safety,updat,updated,3012," gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.10. -----. Session information updated at 2024-09-26 13:44. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:2992,security,Session,Session,2992," gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.10. -----. Session information updated at 2024-09-26 13:44. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:3012,security,updat,updated,3012," gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.10. -----. Session information updated at 2024-09-26 13:44. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:1805,testability,Trace,Traceback,1805,"# Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions N",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:201,usability,confirm,confirmed,201,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:284,usability,confirm,confirmed,284,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:533,usability,confirm,confirm,533,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:592,usability,help,help,592,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:630,usability,Minim,Minimal,630,"sc.tl.score_genes -- ValueError: No valid genes were passed for scoring; ### Please make sure these conditions are met. - [X] I have checked that this issue has not already been reported. - [X] I have confirmed this bug exists on the latest version of scanpy. - [X] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have found this issue in both old versions 1.9.3, 1.9.8 and the newest version 1.10.3. I am pretty sure my gene names are in the adata.var_names (see below code). . I also confirm that each gene-set has 2 or more genes. Can anyone help to debug? Thank you,. Holly. ### Minimal code sample. ```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. r",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:1648,usability,Error,Error,1648,"```python. print(sc.__version__). # 1.10.3. # Randomly select 1000 cell indices. selected_cells = np.random.choice(adata.obs.index, size=1000, replace=False). # Create a subset AnnData object. subset_adata = adata[selected_cells].copy(). subset_adata.write(save_fold + ""subset_adata.h5ad""). #subset_known_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:1949,usability,tool,tools,1949,"own_markers = dict(list(filtered_known_markers.items())[:2]). tmp = ['Isl1', 'Tcf21', 'Tlx1'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4.18.0-305.3.1.e",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/issues/3266:2777,usability,tool,toolz,2777," gene in tmp if gene in subset_adata.var_names] == tmp # True. tmp = ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2'] . [gene for gene in tmp if gene in subset_adata.var_names] == tmp # True. subset_known_markers = {. 'Anterior cardiopharyngeal progenitors_Imaz2024': ['Isl1', 'Tcf21', 'Tlx1'], . 'Cardiomyocytes FHF 1_Imaz2024': ['Gata4', 'Nkx2-5', 'Nr2f2', 'Osr1', 'Tbx5', 'Wnt2']. }. . tmp = sc.tl.score_genes(subset_adata, gene_list= subset_known_markers, copy=True . #,use_raw=True . #,n_bins = 150 , ctrl_size =100. ) # ctrl_size = 50 by default ; n_bins = 25 by default. ```. ### Error output. ```pytb. WARNING: genes are not in var_names and ignored: ['Anterior cardiopharyngeal progenitors_Imaz2024', 'Cardiomyocytes FHF 1_Imaz2024']. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/project/xyang2/anaconda/py38/lib/python3.8/site-packages/scanpy/tools/_score_genes.py"", line 115, in score_genes. raise ValueError(""No valid genes were passed for scoring.""). ValueError: No valid genes were passed for scoring. ```. ### Versions. <details>. ```. -----. anndata 0.9.2. scanpy 1.9.3. -----. PIL 10.4.0. cloudpickle 3.0.0. cycler 0.12.1. cython_runtime NA. cytoolz 0.12.3. dask 2023.5.0. dateutil 2.9.0.post0. h5py 3.11.0. igraph 0.11.6. importlib_resources NA. jinja2 3.1.4. joblib 1.4.2. kiwisolver 1.4.7. leidenalg 0.10.2. llvmlite 0.41.1. lz4 4.3.3. markupsafe 2.1.5. matplotlib 3.7.5. mpl_toolkits NA. natsort 8.4.0. numba 0.58.1. numexpr 2.8.6. numpy 1.24.4. packaging 24.1. pandas 2.0.3. psutil 6.0.0. pyarrow 17.0.0. pyparsing 3.1.4. pytz 2024.2. scipy 1.10.1. session_info 1.0.0. six 1.16.0. sklearn 1.3.2. tblib 3.0.0. texttable 1.7.0. threadpoolctl 3.5.0. tlz 0.12.3. toolz 0.12.1. typing_extensions NA. yaml 6.0.2. zipp NA. -----. Python 3.8.19 | packaged by conda-forge | (default, Mar 20 2024, 12:47:35) [GCC 12.3.0]. Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.10. -----. Session information updated at 2024-09-26 13:44. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3266
https://github.com/scverse/scanpy/pull/3267:235,deployability,log,logic,235,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:633,deployability,Depend,Depending,633,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:678,deployability,updat,update,678,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:790,deployability,depend,depend,790,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:808,deployability,version,version,808,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:633,integrability,Depend,Depending,633,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:790,integrability,depend,depend,790,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:808,integrability,version,version,808,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:254,modifiability,paramet,parameters,254,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:633,modifiability,Depend,Depending,633,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:790,modifiability,depend,depend,790,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:808,modifiability,version,version,808,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:557,performance,time,time,557,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:235,safety,log,logic,235,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:633,safety,Depend,Depending,633,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:678,safety,updat,update,678,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:790,safety,depend,depend,790,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:235,security,log,logic,235,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:678,security,updat,update,678,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:220,testability,simpl,simplifies,220,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:235,testability,log,logic,235,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:633,testability,Depend,Depending,633,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:790,testability,depend,depend,790,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:187,usability,learn,learn,187,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:200,usability,learn,learn,200,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:220,usability,simpl,simplifies,220,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:442,usability,learn,learn,442,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:455,usability,learn,learn,455,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:602,usability,learn,learn,602,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:615,usability,learn,learn,615,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3267:910,usability,support,support,910,"Use upstream sklearn PCA if possible; this PR fixes #2744 and. - moves `_pca_with_sparse` behind a check for scipy <1.4, which has @ivirshup’s port of that code https://github.com/scikit-learn/scikit-learn/pull/18689. - simplifies our logic around which parameters lead to which dispatch. this makes it useful to get this in before #3263. - throws a warning when people use the `lobpcg` solver, since the closure of https://github.com/scikit-learn/scikit-learn/issues/12794#issuecomment-2118064158 makes it unlikely that we can count on that getting in any time soon. I filed https://github.com/scikit-learn/scikit-learn/pull/30075. Depending on how that PR is received, we can update the warning here: either they like it, then we can remove the warning (we remove our legacy code once we depend on a scipy version that has lobpcg upstream), or they don’t, then we leave the warning for now and remove lobpcg support in the future.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3267
https://github.com/scverse/scanpy/pull/3268:101,modifiability,paramet,parametrize,101,Split up PCA tests; It’s 3 separate unit tests in a trenchcoat. I’m only not going with `pytest.mark.parametrize` since #3263 and #3267 are making things more complicated and might cause a split anyway. The PCA code is too branched for these tests to be helpful with debugging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3268
https://github.com/scverse/scanpy/pull/3268:13,safety,test,tests,13,Split up PCA tests; It’s 3 separate unit tests in a trenchcoat. I’m only not going with `pytest.mark.parametrize` since #3263 and #3267 are making things more complicated and might cause a split anyway. The PCA code is too branched for these tests to be helpful with debugging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3268
https://github.com/scverse/scanpy/pull/3268:41,safety,test,tests,41,Split up PCA tests; It’s 3 separate unit tests in a trenchcoat. I’m only not going with `pytest.mark.parametrize` since #3263 and #3267 are making things more complicated and might cause a split anyway. The PCA code is too branched for these tests to be helpful with debugging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3268
https://github.com/scverse/scanpy/pull/3268:159,safety,compl,complicated,159,Split up PCA tests; It’s 3 separate unit tests in a trenchcoat. I’m only not going with `pytest.mark.parametrize` since #3263 and #3267 are making things more complicated and might cause a split anyway. The PCA code is too branched for these tests to be helpful with debugging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3268
https://github.com/scverse/scanpy/pull/3268:242,safety,test,tests,242,Split up PCA tests; It’s 3 separate unit tests in a trenchcoat. I’m only not going with `pytest.mark.parametrize` since #3263 and #3267 are making things more complicated and might cause a split anyway. The PCA code is too branched for these tests to be helpful with debugging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3268
https://github.com/scverse/scanpy/pull/3268:159,security,compl,complicated,159,Split up PCA tests; It’s 3 separate unit tests in a trenchcoat. I’m only not going with `pytest.mark.parametrize` since #3263 and #3267 are making things more complicated and might cause a split anyway. The PCA code is too branched for these tests to be helpful with debugging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3268
https://github.com/scverse/scanpy/pull/3268:13,testability,test,tests,13,Split up PCA tests; It’s 3 separate unit tests in a trenchcoat. I’m only not going with `pytest.mark.parametrize` since #3263 and #3267 are making things more complicated and might cause a split anyway. The PCA code is too branched for these tests to be helpful with debugging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3268
https://github.com/scverse/scanpy/pull/3268:36,testability,unit,unit,36,Split up PCA tests; It’s 3 separate unit tests in a trenchcoat. I’m only not going with `pytest.mark.parametrize` since #3263 and #3267 are making things more complicated and might cause a split anyway. The PCA code is too branched for these tests to be helpful with debugging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3268
https://github.com/scverse/scanpy/pull/3268:41,testability,test,tests,41,Split up PCA tests; It’s 3 separate unit tests in a trenchcoat. I’m only not going with `pytest.mark.parametrize` since #3263 and #3267 are making things more complicated and might cause a split anyway. The PCA code is too branched for these tests to be helpful with debugging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3268
https://github.com/scverse/scanpy/pull/3268:242,testability,test,tests,242,Split up PCA tests; It’s 3 separate unit tests in a trenchcoat. I’m only not going with `pytest.mark.parametrize` since #3263 and #3267 are making things more complicated and might cause a split anyway. The PCA code is too branched for these tests to be helpful with debugging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3268
https://github.com/scverse/scanpy/pull/3268:254,usability,help,helpful,254,Split up PCA tests; It’s 3 separate unit tests in a trenchcoat. I’m only not going with `pytest.mark.parametrize` since #3263 and #3267 are making things more complicated and might cause a split anyway. The PCA code is too branched for these tests to be helpful with debugging.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3268
https://github.com/scverse/scanpy/pull/3269:49,safety,test,tests,49,Backport PR #3268 on branch 1.10.x (Split up PCA tests); Backport PR #3268: Split up PCA tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3269
https://github.com/scverse/scanpy/pull/3269:89,safety,test,tests,89,Backport PR #3268 on branch 1.10.x (Split up PCA tests); Backport PR #3268: Split up PCA tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3269
https://github.com/scverse/scanpy/pull/3269:49,testability,test,tests,49,Backport PR #3268 on branch 1.10.x (Split up PCA tests); Backport PR #3268: Split up PCA tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3269
https://github.com/scverse/scanpy/pull/3269:89,testability,test,tests,89,Backport PR #3268 on branch 1.10.x (Split up PCA tests); Backport PR #3268: Split up PCA tests,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3269
https://github.com/scverse/scanpy/issues/3272:31,deployability,depend,dependencies,31,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:101,deployability,build,building,101,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:226,deployability,manag,management,226,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:244,deployability,depend,dependencies,244,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:440,deployability,depend,dependencies,440,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:125,energy efficiency,current,currently,125,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:226,energy efficiency,manag,management,226,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:31,integrability,depend,dependencies,31,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:244,integrability,depend,dependencies,244,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:440,integrability,depend,dependencies,440,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:31,modifiability,depend,dependencies,31,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:244,modifiability,depend,dependencies,244,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:440,modifiability,depend,dependencies,440,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:31,safety,depend,dependencies,31,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:226,safety,manag,management,226,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:244,safety,depend,dependencies,244,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:440,safety,depend,dependencies,440,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:339,security,team,team,339,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:31,testability,depend,dependencies,31,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:244,testability,depend,dependencies,244,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:440,testability,depend,dependencies,440,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:501,testability,Understand,Understanding,501,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:135,usability,learn,learning,135,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3272:559,usability,help,help,559,"ScanPy's BSD license and GPLed dependencies; Hello there,. Thank you for the great work you're doing building ScanPy! . I am currently learning about open-source licenses and the intricacies of copyright. Especially regarding management of GPL dependencies and when the viral copyleft clause is triggerred or not. It looks like the ScanPy team explored this question already as the projet is licensed under BSD while it is leveraging GPLed dependencies like `leidenalg`, `python-igraph` or `louvain`. Understanding how you handled this question would greatly help me, could you tell me? Maybe there are discussions recorded somewhere? Best,. Fabien",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3272
https://github.com/scverse/scanpy/issues/3273:7,deployability,integr,integrate,7,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:371,deployability,integr,integrate,371,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:7,integrability,integr,integrate,7,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:371,integrability,integr,integrate,371,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:7,interoperability,integr,integrate,7,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:371,interoperability,integr,integrate,371,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:533,interoperability,platform,platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells,533,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:7,modifiability,integr,integrate,7,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:179,modifiability,paramet,parameters,179,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:371,modifiability,integr,integrate,371,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:7,reliability,integr,integrate,7,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:371,reliability,integr,integrate,371,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:7,security,integr,integrate,7,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:371,security,integr,integrate,371,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:7,testability,integr,integrate,7,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:371,testability,integr,integrate,371,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/issues/3273:479,usability,document,documents,479,How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ?; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hello: . I have multiome snRNA+snATAC data and the snRNA seq data were analyzed with `scanpy`. . How to integrate the snRNA seq data generated by `scanpy` with the snATAC seq data generated by `ArchR` ? From the documents https://www.archrproject.com/bookdown/cross-platform-linkage-of-scatac-seq-cells-with-scrna-seq-cells.html. How to generate the `RangedSummarizedExperiment` data set ? Is it possible to convert the `anndata` used by `scanpy` to `RangedSummarizedExperiment` data? Thanks a lot,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3273
https://github.com/scverse/scanpy/pull/3275:593,availability,error,error,593,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:642,deployability,log,logic,642,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:677,deployability,version,versions,677,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:677,integrability,version,versions,677,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:430,modifiability,maintain,maintain,430,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:677,modifiability,version,versions,677,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:593,performance,error,error,593,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:270,safety,review,review,270,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:374,safety,Test,Tests,374,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:430,safety,maintain,maintain,430,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:593,safety,error,error,593,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:642,safety,log,logic,642,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:642,security,log,logic,642,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:270,testability,review,review,270,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:374,testability,Test,Tests,374,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:642,testability,log,logic,642,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:121,usability,guid,guidelines,121,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:152,usability,guid,guide,152,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:248,usability,workflow,workflow,248,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:354,usability,Close,Closes,354,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/pull/3275:593,usability,error,error,593,"Catch PerfectSeparationWarning during regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3260. - [x] Tests included or not required because: minor change to maintain compat with `statsmodels`>=14.0. Inspired by @mwaskom's [fix for seaborn](https://github.com/mwaskom/seaborn/pull/3356), which promotes the warning to an error and catches it (in this case with the same logic `scanpy` was using for prior versions of `statsmodels`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3275
https://github.com/scverse/scanpy/issues/3276:999,availability,Error,Error,999,"nnot save to defined path; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:197,deployability,version,version,197,"Cannot save to defined path; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=sa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:1833,deployability,log,log,1833,"ode sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:1866,deployability,log,log,1866," ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10.1 . anndata==0.10.7 . umap==0.5.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:2797,deployability,Version,Versions,2797,"----------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10.1 . anndata==0.10.7 . umap==0.5.5 . numpy==1.26.4 . scipy==1.13.0 . pandas==2.2.2 . scikit-learn==1.4.2 . statsmodels==0.14.1 . igraph==0.11.5 . pynndescent==0.5.12. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:498,energy efficiency,current,current,498,"Cannot save to defined path; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=sa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:197,integrability,version,version,197,"Cannot save to defined path; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=sa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:1381,integrability,wrap,wrapper,1381,"I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handle",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:1434,integrability,wrap,wraps,1434,"ry. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:2797,integrability,Version,Versions,2797,"----------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10.1 . anndata==0.10.7 . umap==0.5.5 . numpy==1.26.4 . scipy==1.13.0 . pandas==2.2.2 . scikit-learn==1.4.2 . statsmodels==0.14.1 . igraph==0.11.5 . pynndescent==0.5.12. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:592,interoperability,specif,specific,592,"Cannot save to defined path; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=sa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:1381,interoperability,wrapper,wrapper,1381,"I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handle",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:197,modifiability,version,version,197,"Cannot save to defined path; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=sa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:1317,modifiability,pac,packages,1317,"Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:1730,modifiability,pac,packages,1730,"508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable elemen",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:2088,modifiability,pac,packages,2088,"----------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10.1 . anndata==0.10.7 . umap==0.5.5 . numpy==1.26.4 . scipy==1.13.0 . pandas==2.2.2 . scikit-learn==1.4.2 . statsmodels==0.14.1 . igraph==0.11.5 . pynndescent==0.5.12. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:2797,modifiability,Version,Versions,2797,"----------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10.1 . anndata==0.10.7 . umap==0.5.5 . numpy==1.26.4 . scipy==1.13.0 . pandas==2.2.2 . scikit-learn==1.4.2 . statsmodels==0.14.1 . igraph==0.11.5 . pynndescent==0.5.12. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:644,performance,time,time,644,"Cannot save to defined path; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=sa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:999,performance,Error,Error,999,"nnot save to defined path; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:444,reliability,doe,doesn,444,"Cannot save to defined path; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=sa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:999,safety,Error,Error,999,"nnot save to defined path; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:1833,safety,log,log,1833,"ode sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:1866,safety,log,log,1866," ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10.1 . anndata==0.10.7 . umap==0.5.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:1833,security,log,log,1833,"ode sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:1866,security,log,log,1866," ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10.1 . anndata==0.10.7 . umap==0.5.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:1108,testability,Trace,Traceback,1108,"has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:1833,testability,log,log,1833,"ode sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:1866,testability,log,log,1866," ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10.1 . anndata==0.10.7 . umap==0.5.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:157,usability,confirm,confirmed,157,"Cannot save to defined path; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=sa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:240,usability,confirm,confirmed,240,"Cannot save to defined path; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=sa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:826,usability,Minim,Minimal,826,"Cannot save to defined path; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=sa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:999,usability,Error,Error,999,"nnot save to defined path; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi, . Apologies if I've missed this addressed somewhere else, however I would like to save my figured to a defined directory. It doesn't seem I can do that without first changing the current working directory outside the line of code. What is the best way to save my plot to a specific directory without having to change it each time using os.chdir? . I have seen this [issue](https://github.com/scverse/scanpy/issues/1508#issue-750736685) from 2 years ago but wondered if any changes have been made since. ### Minimal code sample. ```. output_dir_fig = ""chosen/path/to/directory"". sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). ```. ### Error output. ---------------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:2466,usability,User,Users,2466,"----------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10.1 . anndata==0.10.7 . umap==0.5.5 . numpy==1.26.4 . scipy==1.13.0 . pandas==2.2.2 . scikit-learn==1.4.2 . statsmodels==0.14.1 . igraph==0.11.5 . pynndescent==0.5.12. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:2583,usability,User,Users,2583,"----------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10.1 . anndata==0.10.7 . umap==0.5.5 . numpy==1.26.4 . scipy==1.13.0 . pandas==2.2.2 . scikit-learn==1.4.2 . statsmodels==0.14.1 . igraph==0.11.5 . pynndescent==0.5.12. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/issues/3276:2927,usability,learn,learn,2927,"----------------------------------------------------------------------. FileNotFoundError Traceback (most recent call last). Cell In[85], line 1. ----> 1 sc.pl.highest_expr_genes(adata, n_top=10, save= f""{output_dir_fig}/highest_expr_genes.png""). File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/legacy_api_wrap/__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_qc.py:105, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 103 ax.set_xscale(""log""). 104 show = settings.autoshow if show is None else show. --> 105 _utils.savefig_or_show(""highest_expr_genes"", show=show, save=save). 106 if show:. 107 return None. File /opt/anaconda3/envs/scanpy/lib/python3.11/site-packages/scanpy/plotting/_utils.py:339, in savefig_or_show(writekey, show, dpi, ext, save). 337 show = settings.autoshow if show is None else show. 338 if save:. --> 339 savefig(writekey, dpi=dpi, ext=ext). 340 if show:. ... -> 2456 fp = builtins.open(filename, ""w+b""). 2458 try:. 2459 save_handler(self, fp, filename). FileNotFoundError: [Errno 2] No such file or directory: '/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/scripts_new/Python_notebooks/figures/highest_expr_genes/Users/sarapatti/Desktop/PhD_projects/Llyod_lab/Col1_scRNAseq/analysis/figures/highest_expr_genes.png'. Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings... ### Versions. <details>. ```. scanpy==1.10.1 . anndata==0.10.7 . umap==0.5.5 . numpy==1.26.4 . scipy==1.13.0 . pandas==2.2.2 . scikit-learn==1.4.2 . statsmodels==0.14.1 . igraph==0.11.5 . pynndescent==0.5.12. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3276
https://github.com/scverse/scanpy/pull/3278:569,deployability,releas,release,569,Extending gex_only option to Visium function; Extended gex_only function to visium. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3113. - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: not a big change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278
https://github.com/scverse/scanpy/pull/3278:594,deployability,Releas,Release,594,Extending gex_only option to Visium function; Extended gex_only function to visium. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3113. - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: not a big change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278
https://github.com/scverse/scanpy/pull/3278:0,modifiability,Exten,Extending,0,Extending gex_only option to Visium function; Extended gex_only function to visium. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3113. - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: not a big change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278
https://github.com/scverse/scanpy/pull/3278:46,modifiability,Exten,Extended,46,Extending gex_only option to Visium function; Extended gex_only function to visium. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3113. - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: not a big change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278
https://github.com/scverse/scanpy/pull/3278:303,safety,review,review,303,Extending gex_only option to Visium function; Extended gex_only function to visium. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3113. - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: not a big change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278
https://github.com/scverse/scanpy/pull/3278:407,safety,Test,Tests,407,Extending gex_only option to Visium function; Extended gex_only function to visium. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3113. - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: not a big change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278
https://github.com/scverse/scanpy/pull/3278:303,testability,review,review,303,Extending gex_only option to Visium function; Extended gex_only function to visium. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3113. - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: not a big change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278
https://github.com/scverse/scanpy/pull/3278:407,testability,Test,Tests,407,Extending gex_only option to Visium function; Extended gex_only function to visium. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3113. - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: not a big change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278
https://github.com/scverse/scanpy/pull/3278:154,usability,guid,guidelines,154,Extending gex_only option to Visium function; Extended gex_only function to visium. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3113. - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: not a big change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278
https://github.com/scverse/scanpy/pull/3278:185,usability,guid,guide,185,Extending gex_only option to Visium function; Extended gex_only function to visium. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3113. - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: not a big change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278
https://github.com/scverse/scanpy/pull/3278:281,usability,workflow,workflow,281,Extending gex_only option to Visium function; Extended gex_only function to visium. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3113. - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: not a big change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278
https://github.com/scverse/scanpy/pull/3278:387,usability,Close,Closes,387,Extending gex_only option to Visium function; Extended gex_only function to visium. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3113. - [x] Tests included or not required because: It's a straightforward argument pass from visium to read_10x_h5. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: not a big change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3278
https://github.com/scverse/scanpy/pull/3279:531,availability,cluster,cluster,531,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:768,availability,Down,Downloading,768,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:803,availability,down,download,803,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1602,availability,cluster,clusters,1602,"lterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1716,availability,cluster,cluster,1716,"ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # F",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:531,deployability,cluster,cluster,531,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1602,deployability,cluster,clusters,1602,"lterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1716,deployability,cluster,cluster,1716,"ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # F",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2421,deployability,log,log,2421,"n. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3244,deployability,scale,scale,3244,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3273,deployability,scale,scale,3273,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3314,deployability,scale,scale,3314,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:6,energy efficiency,optim,optimization,6,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1982,energy efficiency,reduc,reduce,1982,"ial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3244,energy efficiency,scale,scale,3244,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3273,energy efficiency,scale,scale,3273,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3314,energy efficiency,scale,scale,3314,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:604,integrability,filter,filterwarnings,604,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1084,integrability,filter,filtering,1084,"thub.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1128,integrability,Filter,Filter,1128,"e is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_ge",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1211,integrability,Filter,Filter,1211,"s t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1267,integrability,filter,filtering,1267,"in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1309,integrability,Filter,Filter,1309,"nt for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1473,integrability,compon,components,1473,"port pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly varia",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1543,integrability,compon,components,1543,"eans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2319,integrability,filter,filter,2319,"nes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale ti",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2719,integrability,Filter,Filter,2719,"r. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1473,interoperability,compon,components,1473,"port pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly varia",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1543,interoperability,compon,components,1543,"eans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:173,modifiability,maintain,maintainers,173,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:351,modifiability,pac,package,351,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1400,modifiability,variab,variable,1400,"as run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1473,modifiability,compon,components,1473,"port pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly varia",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1543,modifiability,compon,components,1543,"eans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2473,modifiability,variab,variable,2473,"onents to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2741,modifiability,variab,variable,2741," jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm[",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3244,modifiability,scal,scale,3244,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3273,modifiability,scal,scale,3273,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3314,modifiability,scal,scale,3314,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:6,performance,optimiz,optimization,6,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:450,performance,time,time,450,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1737,performance,parallel,parallel,1737,"xists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1809,performance,time,time,1809,"('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1814,performance,time,time,1814,"ps://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1825,performance,time,time,1825,"-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1830,performance,time,time,1830,"le-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1929,performance,time,time,1929,"enes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1943,performance,time,time,1943,"E_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1948,performance,time,time,1948,"FIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1964,performance,time,time,1964,"efix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1969,performance,time,time,1969,"for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_coun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2340,performance,time,time,2340,"er cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.tim",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2354,performance,time,time,2354,"this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2359,performance,time,time,2359," n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2375,performance,time,time,2375,"000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2380,performance,time,time,2380," Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2425,performance,time,time,2425," PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2439,performance,time,time,2439,"ents = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.Scan",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2444,performance,time,time,2444,"= 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyCon",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2805,performance,time,time,2805,"t=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2810,performance,time,time,2810,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3211,performance,time,time,3211,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3225,performance,time,time,3225,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3230,performance,time,time,3230,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3244,performance,scale,scale,3244,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3254,performance,time,time,3254,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3259,performance,time,time,3259,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3273,performance,scale,scale,3273,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3314,performance,scale,scale,3314,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3320,performance,time,time,3320,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3334,performance,time,time,3334,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3339,performance,time,time,3339,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3355,performance,time,time,3355,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3360,performance,time,time,3360,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3411,performance,time,time,3411,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3416,performance,time,time,3416,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3782,performance,time,time,3782,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3790,performance,time,time,3790,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3795,performance,time,time,3795,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:173,safety,maintain,maintainers,173,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2421,safety,log,log,2421,"n. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2421,security,log,log,2421,"n. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:652,testability,simpl,simplefilter,652,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:998,testability,regress,regress,998,"SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2421,testability,log,log,2421,"n. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time().",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:2819,testability,Regress,Regress,2819,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3199,testability,regress,regress,3199,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:32,usability,learn,learn-intelex,32,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:239,usability,learn,learn-intelex,239,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:652,usability,simpl,simplefilter,652,"t-SNE optimization using scikit-learn-intelex; We have created this PR earlier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:1067,usability,visual,visualization,1067,"lier https://github.com/scverse/scanpy/pull/3061. This one is the same PR with editing enabled for maintainers. This pull request accelerates t-SNE using the scikit-learn-intelex library, resulting in approximately a 10x runtime improvement for the t-SNE implementation in the package for the given example below. The experiment was run on AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3279:3584,usability,tool,tools,3584,"ime.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata, max_value=10). print(""Total scale time : %s"" % (time.time()-ts)). t0=time.time(). sc.tl.pca(adata, n_comps=n_components). t0=time.time(). sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). sc.tl.tsne(adata, n_pcs=tsne_n_pcs, use_fast_tsne=True). '''. from sklearn.manifold import TSNE. from scanpy.tools._utils import _choose_representation. X = _choose_representation(adata, n_pcs=tsne_n_pcs). X_tsne = TSNE().fit_transform(X.astype(np.float32)). adata.obsm['X_tsne'] = X_tsne. '''. print(""Tsne time:"", time.time()-t0). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3279
https://github.com/scverse/scanpy/pull/3280:517,availability,cluster,cluster,517,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:754,availability,Down,Downloading,754,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:789,availability,down,download,789,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1588,availability,cluster,clusters,1588,"lterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1702,availability,cluster,cluster,1702,"ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # F",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:0,deployability,scale,scale,0,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:30,deployability,updat,updated,30,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:337,deployability,Updat,Updated,337,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:517,deployability,cluster,cluster,517,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1588,deployability,cluster,clusters,1588,"lterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1702,deployability,cluster,cluster,1702,"ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # F",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2407,deployability,log,log,2407,"n. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3230,deployability,scale,scale,3230," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3259,deployability,scale,scale,3259," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3286,deployability,scale,scale,3286," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3590,deployability,scale,scale,3590," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:0,energy efficiency,scale,scale,0,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1968,energy efficiency,reduc,reduce,1968,"ial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3230,energy efficiency,scale,scale,3230," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3259,energy efficiency,scale,scale,3259," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3286,energy efficiency,scale,scale,3286," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3590,energy efficiency,scale,scale,3590," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:225,integrability,sub,submitting,225,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:590,integrability,filter,filterwarnings,590,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1070,integrability,filter,filtering,1070," We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1114,integrability,Filter,Filter,1114,"hub.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_ge",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1197,integrability,Filter,Filter,1197,"maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1253,integrability,filter,filtering,1253," _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1295,integrability,Filter,Filter,1295,". | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1459,integrability,compon,components,1459,"port pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly varia",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1529,integrability,compon,components,1529,"eans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2305,integrability,filter,filter,2305,"nes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (ti",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2705,integrability,Filter,Filter,2705," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1459,interoperability,compon,components,1459,"port pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly varia",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1529,interoperability,compon,components,1529,"eans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:0,modifiability,scal,scale,0,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:200,modifiability,maintain,maintainers,200,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1386,modifiability,variab,variable,1386,"t setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1459,modifiability,compon,components,1459,"port pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly varia",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1529,modifiability,compon,components,1529,"eans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2459,modifiability,variab,variable,2459,"onents to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocess",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2727,modifiability,variab,variable,2727," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3230,modifiability,scal,scale,3230," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3259,modifiability,scal,scale,3259," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3286,modifiability,scal,scale,3286," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3590,modifiability,scal,scale,3590," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:0,performance,scale,scale,0,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:287,performance,Time,Time,287,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:436,performance,time,time,436,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1723,performance,parallel,parallel,1723,"xists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1795,performance,time,time,1795,"('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.t",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1800,performance,time,time,1800,"ps://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time()",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1811,performance,time,time,1811,"-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1816,performance,time,time,1816,"le-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1915,performance,time,time,1915,"enes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1929,performance,time,time,1929,"E_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1934,performance,time,time,1934,"FIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1950,performance,time,time,1950,"efix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1955,performance,time,time,1955,"for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_coun",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2326,performance,time,time,2326,"er cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2340,performance,time,time,2340,"this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer aro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2345,performance,time,time,2345," n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2361,performance,time,time,2361,"000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var cal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2366,performance,time,time,2366," Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. ht",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2411,performance,time,time,2411," PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef6",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2425,performance,time,time,2425,"ents = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b7888",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2430,performance,time,time,2430,"= 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7f",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2791,performance,time,time,2791," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2796,performance,time,time,2796," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3197,performance,time,time,3197," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3211,performance,time,time,3211," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3216,performance,time,time,3216," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3230,performance,scale,scale,3230," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3240,performance,time,time,3240," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3245,performance,time,time,3245," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3259,performance,scale,scale,3259," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3286,performance,scale,scale,3286," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3292,performance,time,time,3292," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3306,performance,time,time,3306," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3311,performance,time,time,3311," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3333,performance,time,timer,3333," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3590,performance,scale,scale,3590," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:30,safety,updat,updated,30,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:200,safety,maintain,maintainers,200,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:337,safety,Updat,Updated,337,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2407,safety,log,log,2407,"n. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:30,security,updat,updated,30,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:337,security,Updat,Updated,337,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2407,security,log,log,2407,"n. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:638,testability,simpl,simplefilter,638,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:984,testability,regress,regress,984,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2407,testability,log,log,2407,"n. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:2805,testability,Regress,Regress,2805," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:3185,testability,regress,regress,3185," ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. ts=time.time(). #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). #scale. ts=time.time(). sc.pp.scale(adata). print(""Total scale time : %s"" % (time.time()-ts)). ```. add timer around _get_mean_var call. https://github.com/scverse/scanpy/blob/706d4ef65e5d65e04b788831e7fd65dbe6b2a61f/scanpy/preprocessing/_scale.py#L167. we can also create _get_mean_var_std function that return std as well so we don't require to compute it in scale function(L168-L169).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:638,usability,simpl,simplefilter,638,"scale function(_get_mean_var) updated for dense array, speedup upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3280:1053,usability,visual,visualization,1053,"up upto ~4.65x; We had created this PR before https://github.com/scverse/scanpy/pull/3099. This one is the same PR with editing enabled for maintainers. Hi,. We are submitting PR for speed up of the _get_mean_var function. | | Time(sec) |. | -- | -- |. | Original | 18.49 |. | Updated | 3.97 |. | Speedup | 4.65743073 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3280
https://github.com/scverse/scanpy/pull/3281:0,deployability,Updat,Update,0,Update pyproject.toml; Test new upperlimit for dask for rapids 24.10. . https://github.com/scverse/rapids_singlecell/actions/runs/11325725585/job/31493220507?pr=277,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3281
https://github.com/scverse/scanpy/pull/3281:0,safety,Updat,Update,0,Update pyproject.toml; Test new upperlimit for dask for rapids 24.10. . https://github.com/scverse/rapids_singlecell/actions/runs/11325725585/job/31493220507?pr=277,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3281
https://github.com/scverse/scanpy/pull/3281:23,safety,Test,Test,23,Update pyproject.toml; Test new upperlimit for dask for rapids 24.10. . https://github.com/scverse/rapids_singlecell/actions/runs/11325725585/job/31493220507?pr=277,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3281
https://github.com/scverse/scanpy/pull/3281:0,security,Updat,Update,0,Update pyproject.toml; Test new upperlimit for dask for rapids 24.10. . https://github.com/scverse/rapids_singlecell/actions/runs/11325725585/job/31493220507?pr=277,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3281
https://github.com/scverse/scanpy/pull/3281:23,testability,Test,Test,23,Update pyproject.toml; Test new upperlimit for dask for rapids 24.10. . https://github.com/scverse/rapids_singlecell/actions/runs/11325725585/job/31493220507?pr=277,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3281
https://github.com/scverse/scanpy/issues/3282:0,deployability,Updat,Update,0,Update minimum Python version to 3.10; See https://scientific-python.org/specs/spec-0000/,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3282
https://github.com/scverse/scanpy/issues/3282:22,deployability,version,version,22,Update minimum Python version to 3.10; See https://scientific-python.org/specs/spec-0000/,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3282
https://github.com/scverse/scanpy/issues/3282:22,integrability,version,version,22,Update minimum Python version to 3.10; See https://scientific-python.org/specs/spec-0000/,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3282
https://github.com/scverse/scanpy/issues/3282:22,modifiability,version,version,22,Update minimum Python version to 3.10; See https://scientific-python.org/specs/spec-0000/,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3282
https://github.com/scverse/scanpy/issues/3282:0,safety,Updat,Update,0,Update minimum Python version to 3.10; See https://scientific-python.org/specs/spec-0000/,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3282
https://github.com/scverse/scanpy/issues/3282:0,security,Updat,Update,0,Update minimum Python version to 3.10; See https://scientific-python.org/specs/spec-0000/,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3282
https://github.com/scverse/scanpy/issues/3282:7,usability,minim,minimum,7,Update minimum Python version to 3.10; See https://scientific-python.org/specs/spec-0000/,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3282
https://github.com/scverse/scanpy/pull/3283:441,deployability,releas,release,441,Remove 3.9 support; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3282. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3283
https://github.com/scverse/scanpy/pull/3283:466,deployability,Releas,Release,466,Remove 3.9 support; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3282. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3283
https://github.com/scverse/scanpy/pull/3283:239,safety,review,review,239,Remove 3.9 support; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3282. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3283
https://github.com/scverse/scanpy/pull/3283:343,safety,Test,Tests,343,Remove 3.9 support; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3282. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3283
https://github.com/scverse/scanpy/pull/3283:239,testability,review,review,239,Remove 3.9 support; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3282. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3283
https://github.com/scverse/scanpy/pull/3283:343,testability,Test,Tests,343,Remove 3.9 support; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3282. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3283
https://github.com/scverse/scanpy/pull/3283:11,usability,support,support,11,Remove 3.9 support; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3282. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3283
https://github.com/scverse/scanpy/pull/3283:90,usability,guid,guidelines,90,Remove 3.9 support; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3282. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3283
https://github.com/scverse/scanpy/pull/3283:121,usability,guid,guide,121,Remove 3.9 support; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3282. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3283
https://github.com/scverse/scanpy/pull/3283:217,usability,workflow,workflow,217,Remove 3.9 support; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3282. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3283
https://github.com/scverse/scanpy/pull/3283:323,usability,Close,Closes,323,Remove 3.9 support; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3282. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3283
https://github.com/scverse/scanpy/pull/3284:605,availability,cluster,cluster,605,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:842,availability,Down,Downloading,842,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:877,availability,down,download,877,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1676,availability,cluster,clusters,1676,"lterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1790,availability,cluster,cluster,1790,"ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # F",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:429,deployability,Updat,Updated,429,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:605,deployability,cluster,cluster,605,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1676,deployability,cluster,clusters,1676,"lterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1790,deployability,cluster,cluster,1790,"ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # F",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2495,deployability,log,log,2495,"n. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did no",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:3507,deployability,releas,release,3507," # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:3532,deployability,Releas,Release,3532," # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2056,energy efficiency,reduc,reduce,2056,"ial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:204,integrability,sub,submitting,204,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:678,integrability,filter,filterwarnings,678,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1158,integrability,filter,filtering,1158,"llowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1202,integrability,Filter,Filter,1202,"ubmitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_ge",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1285,integrability,Filter,Filter,1285,"using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1341,integrability,filter,filtering,1341," GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1383,integrability,Filter,Filter,1383,"sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1547,integrability,compon,components,1547,"port pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly varia",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1617,integrability,compon,components,1617,"eans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2393,integrability,filter,filter,2393,"nes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Clo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2793,integrability,Filter,Filter,2793," # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1547,interoperability,compon,components,1547,"port pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly varia",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1617,interoperability,compon,components,1617,"eans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:179,modifiability,maintain,maintainers,179,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1474,modifiability,variab,variable,1474,"t setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1547,modifiability,compon,components,1547,"port pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly varia",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1617,modifiability,compon,components,1617,"eans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2547,modifiability,variab,variable,2547,"onents to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2815,modifiability,variab,variable,2815," # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:381,performance,Time,Time,381,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:524,performance,time,time,524,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1811,performance,parallel,parallel,1811,"xists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1883,performance,time,time,1883,"('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1888,performance,time,time,1888,"ps://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1899,performance,time,time,1899,"-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1904,performance,time,time,1904,"le-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding facto",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2003,performance,time,time,2003,"enes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(M",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2017,performance,time,time,2017,"E_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFI",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2022,performance,time,time,2022,"FIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2038,performance,time,time,2038,"efix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.arr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2043,performance,time,time,2043,"for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(ad",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2414,performance,time,time,2414,"er cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests i",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2428,performance,time,time,2428,"this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2433,performance,time,time,2433," n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not requ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2449,performance,time,time,2449,"000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2454,performance,time,time,2454," Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- O",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2499,performance,time,time,2499," PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2513,performance,time,time,2513,"ents = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2518,performance,time,time,2518,"= 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:3191,performance,time,time,3191," # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:3196,performance,time,time,3196," # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:3285,performance,time,time,3285," # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:3299,performance,time,time,3299," # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:3304,performance,time,time,3304," # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:179,safety,maintain,maintainers,179,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:429,safety,Updat,Updated,429,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2495,safety,log,log,2495,"n. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did no",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:3409,safety,Test,Tests,3409," # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:429,security,Updat,Updated,429,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2495,security,log,log,2495,"n. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did no",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:301,testability,regress,regression,301,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:726,testability,simpl,simplefilter,726,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1072,testability,regress,regress,1072,"ethod.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2495,testability,log,log,2495,"n. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did no",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:2877,testability,Regress,Regress,2877," # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:3273,testability,regress,regress,3273," # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:3409,testability,Test,Tests,3409," # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:726,usability,simpl,simplefilter,726,"Speedup (~20x) of scanpy.pp.regress_out function using Linear Least Square method.; This pull request is same as https://github.com/scverse/scanpy/pull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total re",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:1141,usability,visual,visualization,1141,"ull/3110 with allowed edits to maintainers. Hi,. We are submitting PR for speed up of the regress_out function. Here we finding coefficient using Linear regression (Linear Least Squares) rather then GLM for non categorical data. | | Time(sec) |. | -- | -- |. | Original | 297 |. | Updated | 14.91 |. | Speedup | 19.91 |. experiment setup : AWS r7i.24xlarge. ```python. import time. import numpy as np. import pandas as pd. import scanpy as sc. from sklearn.cluster import KMeans. import os. import wget. import warnings. warnings.filterwarnings('ignore', 'Expected '). warnings.simplefilter('ignore'). input_file = ""./1M_brain_cells_10X.sparse.h5ad"". if not os.path.exists(input_file):. print('Downloading import file...'). wget.download('https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad',input_file). # marker genes. MITO_GENE_PREFIX = ""mt-"" # Prefix for mitochondrial genes to regress out. markers = [""Stmn2"", ""Hes1"", ""Olig1""] # Marker genes for visualization. # filtering cells. min_genes_per_cell = 200 # Filter out cells with fewer genes than this expressed. max_genes_per_cell = 6000 # Filter out cells with more genes than this expressed. # filtering genes. min_cells_per_gene = 1 # Filter out genes expressed in fewer cells than this. n_top_genes = 4000 # Number of highly variable genes to retain. # PCA. n_components = 50 # Number of principal components to compute. # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3284:3393,usability,Close,Closes,3393," # t-SNE. tsne_n_pcs = 20 # Number of principal components to use for t-SNE. # k-means. k = 35 # Number of clusters for k-means. # Gene ranking. ranking_n_top_genes = 50 # Number of differential genes to compute for each cluster. # Number of parallel jobs. sc._settings.ScanpyConfig.n_jobs = os.cpu_count(). start=time.time(). tr=time.time(). adata = sc.read(input_file). adata.var_names_make_unique(). adata.shape. print(""Total read time : %s"" % (time.time()-tr)). tr=time.time(). # To reduce the number of cells:. USE_FIRST_N_CELLS = 1300000. adata = adata[0:USE_FIRST_N_CELLS]. adata.shape. sc.pp.filter_cells(adata, min_genes=min_genes_per_cell). sc.pp.filter_cells(adata, max_genes=max_genes_per_cell). sc.pp.filter_genes(adata, min_cells=min_cells_per_gene). sc.pp.normalize_total(adata, target_sum=1e4). print(""Total filter and normalize time : %s"" % (time.time()-tr)). tr=time.time(). sc.pp.log1p(adata). print(""Total log time : %s"" % (time.time()-tr)). # Select highly variable genes. sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor = ""cell_ranger""). # Retain marker gene expression. for marker in markers:. adata.obs[marker + ""_raw""] = adata.X[:, adata.var.index == marker].toarray().ravel(). # Filter matrix to only variable genes. adata = adata[:, adata.var.highly_variable]. #Regress out confounding factors (number of counts, mitochondrial gene expression). mito_genes = adata.var_names.str.startswith(MITO_GENE_PREFIX). n_counts = np.array(adata.X.sum(axis=1)). adata.obs['percent_mito'] = np.array(np.sum(adata[:, mito_genes].X, axis=1)) / n_counts. adata.obs['n_counts'] = n_counts. ts=time.time(). sc.pp.regress_out(adata, ['n_counts', 'percent_mito']). print(""Total regress out time : %s"" % (time.time()-ts)). ```. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3284
https://github.com/scverse/scanpy/pull/3285:0,deployability,Updat,Update,0,Update `test_rank_genes_groups.py` reference; # PR Summary. Small PR. Commit f03d4f40594a1f1b3216212dc60273c809882a4c moved the location of `test_rank_genes_groups.py`. This PR adjusts sources to changes.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3285
https://github.com/scverse/scanpy/pull/3285:0,safety,Updat,Update,0,Update `test_rank_genes_groups.py` reference; # PR Summary. Small PR. Commit f03d4f40594a1f1b3216212dc60273c809882a4c moved the location of `test_rank_genes_groups.py`. This PR adjusts sources to changes.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3285
https://github.com/scverse/scanpy/pull/3285:0,security,Updat,Update,0,Update `test_rank_genes_groups.py` reference; # PR Summary. Small PR. Commit f03d4f40594a1f1b3216212dc60273c809882a4c moved the location of `test_rank_genes_groups.py`. This PR adjusts sources to changes.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3285
https://github.com/scverse/scanpy/pull/3286:449,deployability,releas,release,449,Fix HVG with 1-obs batches; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2236. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3286
https://github.com/scverse/scanpy/pull/3286:474,deployability,Releas,Release,474,Fix HVG with 1-obs batches; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2236. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3286
https://github.com/scverse/scanpy/pull/3286:19,integrability,batch,batches,19,Fix HVG with 1-obs batches; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2236. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3286
https://github.com/scverse/scanpy/pull/3286:19,performance,batch,batches,19,Fix HVG with 1-obs batches; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2236. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3286
https://github.com/scverse/scanpy/pull/3286:247,safety,review,review,247,Fix HVG with 1-obs batches; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2236. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3286
https://github.com/scverse/scanpy/pull/3286:351,safety,Test,Tests,351,Fix HVG with 1-obs batches; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2236. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3286
https://github.com/scverse/scanpy/pull/3286:247,testability,review,review,247,Fix HVG with 1-obs batches; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2236. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3286
https://github.com/scverse/scanpy/pull/3286:351,testability,Test,Tests,351,Fix HVG with 1-obs batches; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2236. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3286
https://github.com/scverse/scanpy/pull/3286:98,usability,guid,guidelines,98,Fix HVG with 1-obs batches; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2236. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3286
https://github.com/scverse/scanpy/pull/3286:129,usability,guid,guide,129,Fix HVG with 1-obs batches; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2236. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3286
https://github.com/scverse/scanpy/pull/3286:225,usability,workflow,workflow,225,Fix HVG with 1-obs batches; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2236. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3286
https://github.com/scverse/scanpy/pull/3286:331,usability,Close,Closes,331,Fix HVG with 1-obs batches; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2236. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3286
https://github.com/scverse/scanpy/pull/3288:48,deployability,releas,release,48,Backport PR #3287 on branch 1.10.x (Fix #3206’s release note); Backport PR #3287: Fix #3206’s release note,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3288
https://github.com/scverse/scanpy/pull/3288:94,deployability,releas,release,94,Backport PR #3287 on branch 1.10.x (Fix #3206’s release note); Backport PR #3287: Fix #3206’s release note,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3288
https://github.com/scverse/scanpy/pull/3289:532,deployability,releas,release,532,(fix): conditional imports to avoid `anndata.io` warning; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Handle warnings from https://github.com/scverse/anndata/pull/1682. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: basically a dev change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3289
https://github.com/scverse/scanpy/pull/3289:557,deployability,Releas,Release,557,(fix): conditional imports to avoid `anndata.io` warning; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Handle warnings from https://github.com/scverse/anndata/pull/1682. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: basically a dev change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3289
https://github.com/scverse/scanpy/pull/3289:30,safety,avoid,avoid,30,(fix): conditional imports to avoid `anndata.io` warning; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Handle warnings from https://github.com/scverse/anndata/pull/1682. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: basically a dev change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3289
https://github.com/scverse/scanpy/pull/3289:277,safety,review,review,277,(fix): conditional imports to avoid `anndata.io` warning; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Handle warnings from https://github.com/scverse/anndata/pull/1682. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: basically a dev change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3289
https://github.com/scverse/scanpy/pull/3289:434,safety,Test,Tests,434,(fix): conditional imports to avoid `anndata.io` warning; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Handle warnings from https://github.com/scverse/anndata/pull/1682. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: basically a dev change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3289
https://github.com/scverse/scanpy/pull/3289:277,testability,review,review,277,(fix): conditional imports to avoid `anndata.io` warning; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Handle warnings from https://github.com/scverse/anndata/pull/1682. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: basically a dev change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3289
https://github.com/scverse/scanpy/pull/3289:434,testability,Test,Tests,434,(fix): conditional imports to avoid `anndata.io` warning; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Handle warnings from https://github.com/scverse/anndata/pull/1682. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: basically a dev change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3289
https://github.com/scverse/scanpy/pull/3289:128,usability,guid,guidelines,128,(fix): conditional imports to avoid `anndata.io` warning; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Handle warnings from https://github.com/scverse/anndata/pull/1682. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: basically a dev change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3289
https://github.com/scverse/scanpy/pull/3289:159,usability,guid,guide,159,(fix): conditional imports to avoid `anndata.io` warning; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Handle warnings from https://github.com/scverse/anndata/pull/1682. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: basically a dev change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3289
https://github.com/scverse/scanpy/pull/3289:255,usability,workflow,workflow,255,(fix): conditional imports to avoid `anndata.io` warning; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Handle warnings from https://github.com/scverse/anndata/pull/1682. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: basically a dev change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3289
https://github.com/scverse/scanpy/pull/3290:66,safety,avoid,avoid,66,Backport PR #3289 on branch 1.10.x ((fix): conditional imports to avoid `anndata.io` warning); Backport PR #3289: (fix): conditional imports to avoid `anndata.io` warning,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3290
https://github.com/scverse/scanpy/pull/3290:144,safety,avoid,avoid,144,Backport PR #3289 on branch 1.10.x ((fix): conditional imports to avoid `anndata.io` warning); Backport PR #3289: (fix): conditional imports to avoid `anndata.io` warning,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3290
https://github.com/scverse/scanpy/issues/3291:32,deployability,API,API,32,"Gearys C and Morans I As Public API; ### What kind of feature would you like to request? A new public analysis function. ### Please describe your wishes. Currently, these are implemented here but only have a public API in squidpy, so we should expose the API here as well",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3291
https://github.com/scverse/scanpy/issues/3291:215,deployability,API,API,215,"Gearys C and Morans I As Public API; ### What kind of feature would you like to request? A new public analysis function. ### Please describe your wishes. Currently, these are implemented here but only have a public API in squidpy, so we should expose the API here as well",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3291
https://github.com/scverse/scanpy/issues/3291:255,deployability,API,API,255,"Gearys C and Morans I As Public API; ### What kind of feature would you like to request? A new public analysis function. ### Please describe your wishes. Currently, these are implemented here but only have a public API in squidpy, so we should expose the API here as well",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3291
https://github.com/scverse/scanpy/issues/3291:154,energy efficiency,Current,Currently,154,"Gearys C and Morans I As Public API; ### What kind of feature would you like to request? A new public analysis function. ### Please describe your wishes. Currently, these are implemented here but only have a public API in squidpy, so we should expose the API here as well",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3291
https://github.com/scverse/scanpy/issues/3291:25,integrability,Pub,Public,25,"Gearys C and Morans I As Public API; ### What kind of feature would you like to request? A new public analysis function. ### Please describe your wishes. Currently, these are implemented here but only have a public API in squidpy, so we should expose the API here as well",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3291
https://github.com/scverse/scanpy/issues/3291:32,integrability,API,API,32,"Gearys C and Morans I As Public API; ### What kind of feature would you like to request? A new public analysis function. ### Please describe your wishes. Currently, these are implemented here but only have a public API in squidpy, so we should expose the API here as well",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3291
https://github.com/scverse/scanpy/issues/3291:95,integrability,pub,public,95,"Gearys C and Morans I As Public API; ### What kind of feature would you like to request? A new public analysis function. ### Please describe your wishes. Currently, these are implemented here but only have a public API in squidpy, so we should expose the API here as well",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3291
https://github.com/scverse/scanpy/issues/3291:208,integrability,pub,public,208,"Gearys C and Morans I As Public API; ### What kind of feature would you like to request? A new public analysis function. ### Please describe your wishes. Currently, these are implemented here but only have a public API in squidpy, so we should expose the API here as well",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3291
https://github.com/scverse/scanpy/issues/3291:215,integrability,API,API,215,"Gearys C and Morans I As Public API; ### What kind of feature would you like to request? A new public analysis function. ### Please describe your wishes. Currently, these are implemented here but only have a public API in squidpy, so we should expose the API here as well",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3291
https://github.com/scverse/scanpy/issues/3291:255,integrability,API,API,255,"Gearys C and Morans I As Public API; ### What kind of feature would you like to request? A new public analysis function. ### Please describe your wishes. Currently, these are implemented here but only have a public API in squidpy, so we should expose the API here as well",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3291
https://github.com/scverse/scanpy/issues/3291:32,interoperability,API,API,32,"Gearys C and Morans I As Public API; ### What kind of feature would you like to request? A new public analysis function. ### Please describe your wishes. Currently, these are implemented here but only have a public API in squidpy, so we should expose the API here as well",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3291
https://github.com/scverse/scanpy/issues/3291:215,interoperability,API,API,215,"Gearys C and Morans I As Public API; ### What kind of feature would you like to request? A new public analysis function. ### Please describe your wishes. Currently, these are implemented here but only have a public API in squidpy, so we should expose the API here as well",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3291
https://github.com/scverse/scanpy/issues/3291:255,interoperability,API,API,255,"Gearys C and Morans I As Public API; ### What kind of feature would you like to request? A new public analysis function. ### Please describe your wishes. Currently, these are implemented here but only have a public API in squidpy, so we should expose the API here as well",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3291
https://github.com/scverse/scanpy/issues/3291:244,security,expos,expose,244,"Gearys C and Morans I As Public API; ### What kind of feature would you like to request? A new public analysis function. ### Please describe your wishes. Currently, these are implemented here but only have a public API in squidpy, so we should expose the API here as well",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3291
https://github.com/scverse/scanpy/pull/3294:594,availability,slo,slower,594,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:286,deployability,depend,depends,286,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:360,deployability,depend,depending,360,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:286,integrability,depend,depends,286,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:360,integrability,depend,depending,360,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:286,modifiability,depend,depends,286,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:334,modifiability,variab,variable,334,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:360,modifiability,depend,depending,360,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:594,reliability,slo,slower,594,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:0,safety,Test,Test,0,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:201,safety,test,test,201,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:286,safety,depend,depends,286,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:360,safety,depend,depending,360,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:0,testability,Test,Test,0,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:201,testability,test,test,201,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:286,testability,depend,depends,286,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3294:360,testability,depend,depending,360,"Test all PCA param combinations; This replaces the random choice with iterating over all combinations. That way, if you want to debug a certain combination, you can just do so instead of rerunning the test and hoping it gets picked. Sadly AFAIK it’s not possible to have a fixture that depends on other fixture values and generates a variable amount of values depending on their arguments: either you have `fixture(params=some_list)` which creates `len(some_list)` values or not, then it creates one. Therefore I had to get rid of the fixtures and use a static list instead. It’s not that much slower to run them all:. before: 24 passed, 6 xfailed in 1.81s . after: 56 passed, 14 xfailed in 3.41s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3294
https://github.com/scverse/scanpy/pull/3295:17,performance,parallel,parallel,17,lets umap run in parallel; Closes #3211. Lets you run all parts of UMAP in Parallel. Default will still be False for reproducibility. . Benchmarks (95k Cells AMD5950X). `parallel = False` 33 s. `parallel = True` 18 s. I copied the doc string from UMAP to explain the impact of parallel execution.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3295
https://github.com/scverse/scanpy/pull/3295:75,performance,Parallel,Parallel,75,lets umap run in parallel; Closes #3211. Lets you run all parts of UMAP in Parallel. Default will still be False for reproducibility. . Benchmarks (95k Cells AMD5950X). `parallel = False` 33 s. `parallel = True` 18 s. I copied the doc string from UMAP to explain the impact of parallel execution.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3295
https://github.com/scverse/scanpy/pull/3295:170,performance,parallel,parallel,170,lets umap run in parallel; Closes #3211. Lets you run all parts of UMAP in Parallel. Default will still be False for reproducibility. . Benchmarks (95k Cells AMD5950X). `parallel = False` 33 s. `parallel = True` 18 s. I copied the doc string from UMAP to explain the impact of parallel execution.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3295
https://github.com/scverse/scanpy/pull/3295:195,performance,parallel,parallel,195,lets umap run in parallel; Closes #3211. Lets you run all parts of UMAP in Parallel. Default will still be False for reproducibility. . Benchmarks (95k Cells AMD5950X). `parallel = False` 33 s. `parallel = True` 18 s. I copied the doc string from UMAP to explain the impact of parallel execution.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3295
https://github.com/scverse/scanpy/pull/3295:277,performance,parallel,parallel,277,lets umap run in parallel; Closes #3211. Lets you run all parts of UMAP in Parallel. Default will still be False for reproducibility. . Benchmarks (95k Cells AMD5950X). `parallel = False` 33 s. `parallel = True` 18 s. I copied the doc string from UMAP to explain the impact of parallel execution.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3295
https://github.com/scverse/scanpy/pull/3295:27,usability,Close,Closes,27,lets umap run in parallel; Closes #3211. Lets you run all parts of UMAP in Parallel. Default will still be False for reproducibility. . Benchmarks (95k Cells AMD5950X). `parallel = False` 33 s. `parallel = True` 18 s. I copied the doc string from UMAP to explain the impact of parallel execution.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3295
https://github.com/scverse/scanpy/pull/3296:13,usability,support,support,13,Add explicit support to PCA for `'covariance_eigh'` svd_solver; Extracted from #3263. Also deprecates `lobpcg` in docs which we forgot in a previous PR,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3296
https://github.com/scverse/scanpy/issues/3297:55,modifiability,layer,layer,55,"negative values after score_genes with log1P or counts layer; Hi,. I got negative values after run score_genes with log1P or counts layer like below:. using layers['counts']:. ![Image](https://github.com/user-attachments/assets/c081f6d7-520e-409a-b04f-c1ec6a68a847). using layers['counts']:. ![Image](https://github.com/user-attachments/assets/fad82918-1a05-400d-a2ac-1cfb0bd12e05). Is this reasonable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3297
https://github.com/scverse/scanpy/issues/3297:132,modifiability,layer,layer,132,"negative values after score_genes with log1P or counts layer; Hi,. I got negative values after run score_genes with log1P or counts layer like below:. using layers['counts']:. ![Image](https://github.com/user-attachments/assets/c081f6d7-520e-409a-b04f-c1ec6a68a847). using layers['counts']:. ![Image](https://github.com/user-attachments/assets/fad82918-1a05-400d-a2ac-1cfb0bd12e05). Is this reasonable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3297
https://github.com/scverse/scanpy/issues/3297:157,modifiability,layer,layers,157,"negative values after score_genes with log1P or counts layer; Hi,. I got negative values after run score_genes with log1P or counts layer like below:. using layers['counts']:. ![Image](https://github.com/user-attachments/assets/c081f6d7-520e-409a-b04f-c1ec6a68a847). using layers['counts']:. ![Image](https://github.com/user-attachments/assets/fad82918-1a05-400d-a2ac-1cfb0bd12e05). Is this reasonable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3297
https://github.com/scverse/scanpy/issues/3297:273,modifiability,layer,layers,273,"negative values after score_genes with log1P or counts layer; Hi,. I got negative values after run score_genes with log1P or counts layer like below:. using layers['counts']:. ![Image](https://github.com/user-attachments/assets/c081f6d7-520e-409a-b04f-c1ec6a68a847). using layers['counts']:. ![Image](https://github.com/user-attachments/assets/fad82918-1a05-400d-a2ac-1cfb0bd12e05). Is this reasonable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3297
https://github.com/scverse/scanpy/issues/3297:204,usability,user,user-attachments,204,"negative values after score_genes with log1P or counts layer; Hi,. I got negative values after run score_genes with log1P or counts layer like below:. using layers['counts']:. ![Image](https://github.com/user-attachments/assets/c081f6d7-520e-409a-b04f-c1ec6a68a847). using layers['counts']:. ![Image](https://github.com/user-attachments/assets/fad82918-1a05-400d-a2ac-1cfb0bd12e05). Is this reasonable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3297
https://github.com/scverse/scanpy/issues/3297:320,usability,user,user-attachments,320,"negative values after score_genes with log1P or counts layer; Hi,. I got negative values after run score_genes with log1P or counts layer like below:. using layers['counts']:. ![Image](https://github.com/user-attachments/assets/c081f6d7-520e-409a-b04f-c1ec6a68a847). using layers['counts']:. ![Image](https://github.com/user-attachments/assets/fad82918-1a05-400d-a2ac-1cfb0bd12e05). Is this reasonable?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3297
https://github.com/scverse/scanpy/pull/3298:23,deployability,releas,release,23,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:395,deployability,releas,release-notes,395,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:648,deployability,releas,release,648,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:673,deployability,Releas,Release,673,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:467,modifiability,pac,package,467,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:266,safety,review,review,266,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:507,safety,Test,Tests,507,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:547,safety,Test,Tested,547,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:580,safety,test,test,580,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:266,testability,review,review,266,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:507,testability,Test,Tests,507,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:547,testability,Test,Tested,547,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:580,testability,test,test,580,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:117,usability,guid,guidelines,117,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:148,usability,guid,guide,148,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3298:244,usability,workflow,workflow,244,"(fix): correct anndata release for `io` usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] See https://anndata.readthedocs.io/en/latest/release-notes/index.html#rc2-2024-09-24, introduced by #3289. This `io` package did not exist previously. - [x] Tests included or not required because: Tested locally, no way to really test this. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: This is a bug fix of a warning suppression.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3298
https://github.com/scverse/scanpy/pull/3299:478,deployability,releas,release,478,Allow specifying a collection of colors to scatterplots; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #1986. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. This PR moves conversion from `str` to `list[str]` up and changes the check if `color` is in `obs.columns`/`var_names` to a check that can handle collections instead of just a single `color` value.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3299
https://github.com/scverse/scanpy/pull/3299:503,deployability,Releas,Release,503,Allow specifying a collection of colors to scatterplots; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #1986. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. This PR moves conversion from `str` to `list[str]` up and changes the check if `color` is in `obs.columns`/`var_names` to a check that can handle collections instead of just a single `color` value.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3299
https://github.com/scverse/scanpy/pull/3299:6,interoperability,specif,specifying,6,Allow specifying a collection of colors to scatterplots; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #1986. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. This PR moves conversion from `str` to `list[str]` up and changes the check if `color` is in `obs.columns`/`var_names` to a check that can handle collections instead of just a single `color` value.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3299
https://github.com/scverse/scanpy/pull/3299:555,interoperability,convers,conversion,555,Allow specifying a collection of colors to scatterplots; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #1986. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. This PR moves conversion from `str` to `list[str]` up and changes the check if `color` is in `obs.columns`/`var_names` to a check that can handle collections instead of just a single `color` value.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3299
https://github.com/scverse/scanpy/pull/3299:276,safety,review,review,276,Allow specifying a collection of colors to scatterplots; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #1986. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. This PR moves conversion from `str` to `list[str]` up and changes the check if `color` is in `obs.columns`/`var_names` to a check that can handle collections instead of just a single `color` value.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3299
https://github.com/scverse/scanpy/pull/3299:380,safety,Test,Tests,380,Allow specifying a collection of colors to scatterplots; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #1986. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. This PR moves conversion from `str` to `list[str]` up and changes the check if `color` is in `obs.columns`/`var_names` to a check that can handle collections instead of just a single `color` value.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3299
https://github.com/scverse/scanpy/pull/3299:276,testability,review,review,276,Allow specifying a collection of colors to scatterplots; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #1986. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. This PR moves conversion from `str` to `list[str]` up and changes the check if `color` is in `obs.columns`/`var_names` to a check that can handle collections instead of just a single `color` value.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3299
https://github.com/scverse/scanpy/pull/3299:380,testability,Test,Tests,380,Allow specifying a collection of colors to scatterplots; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #1986. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. This PR moves conversion from `str` to `list[str]` up and changes the check if `color` is in `obs.columns`/`var_names` to a check that can handle collections instead of just a single `color` value.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3299
https://github.com/scverse/scanpy/pull/3299:127,usability,guid,guidelines,127,Allow specifying a collection of colors to scatterplots; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #1986. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. This PR moves conversion from `str` to `list[str]` up and changes the check if `color` is in `obs.columns`/`var_names` to a check that can handle collections instead of just a single `color` value.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3299
https://github.com/scverse/scanpy/pull/3299:158,usability,guid,guide,158,Allow specifying a collection of colors to scatterplots; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #1986. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. This PR moves conversion from `str` to `list[str]` up and changes the check if `color` is in `obs.columns`/`var_names` to a check that can handle collections instead of just a single `color` value.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3299
https://github.com/scverse/scanpy/pull/3299:254,usability,workflow,workflow,254,Allow specifying a collection of colors to scatterplots; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #1986. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. This PR moves conversion from `str` to `list[str]` up and changes the check if `color` is in `obs.columns`/`var_names` to a check that can handle collections instead of just a single `color` value.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3299
https://github.com/scverse/scanpy/pull/3299:360,usability,Close,Closes,360,Allow specifying a collection of colors to scatterplots; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #1986. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:. This PR moves conversion from `str` to `list[str]` up and changes the check if `color` is in `obs.columns`/`var_names` to a check that can handle collections instead of just a single `color` value.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3299
https://github.com/scverse/scanpy/pull/3300:55,integrability,batch,batches,55,Backport PR #3286 on branch 1.10.x (Fix HVG with 1-obs batches); Backport PR #3286: Fix HVG with 1-obs batches,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3300
https://github.com/scverse/scanpy/pull/3300:103,integrability,batch,batches,103,Backport PR #3286 on branch 1.10.x (Fix HVG with 1-obs batches); Backport PR #3286: Fix HVG with 1-obs batches,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3300
https://github.com/scverse/scanpy/pull/3300:55,performance,batch,batches,55,Backport PR #3286 on branch 1.10.x (Fix HVG with 1-obs batches); Backport PR #3286: Fix HVG with 1-obs batches,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3300
https://github.com/scverse/scanpy/pull/3300:103,performance,batch,batches,103,Backport PR #3286 on branch 1.10.x (Fix HVG with 1-obs batches); Backport PR #3286: Fix HVG with 1-obs batches,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3300
https://github.com/scverse/scanpy/pull/3302:477,deployability,releas,release,477,Fix sc.pl.highest_expr_genes with a categorical column; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3158. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3302
https://github.com/scverse/scanpy/pull/3302:502,deployability,Releas,Release,502,Fix sc.pl.highest_expr_genes with a categorical column; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3158. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3302
https://github.com/scverse/scanpy/pull/3302:275,safety,review,review,275,Fix sc.pl.highest_expr_genes with a categorical column; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3158. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3302
https://github.com/scverse/scanpy/pull/3302:379,safety,Test,Tests,379,Fix sc.pl.highest_expr_genes with a categorical column; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3158. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3302
https://github.com/scverse/scanpy/pull/3302:275,testability,review,review,275,Fix sc.pl.highest_expr_genes with a categorical column; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3158. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3302
https://github.com/scverse/scanpy/pull/3302:379,testability,Test,Tests,379,Fix sc.pl.highest_expr_genes with a categorical column; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3158. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3302
https://github.com/scverse/scanpy/pull/3302:126,usability,guid,guidelines,126,Fix sc.pl.highest_expr_genes with a categorical column; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3158. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3302
https://github.com/scverse/scanpy/pull/3302:157,usability,guid,guide,157,Fix sc.pl.highest_expr_genes with a categorical column; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3158. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3302
https://github.com/scverse/scanpy/pull/3302:253,usability,workflow,workflow,253,Fix sc.pl.highest_expr_genes with a categorical column; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3158. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3302
https://github.com/scverse/scanpy/pull/3302:359,usability,Close,Closes,359,Fix sc.pl.highest_expr_genes with a categorical column; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3158. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3302
https://github.com/scverse/scanpy/pull/3304:59,deployability,releas,release,59,Backport PR #3298 on branch 1.10.x ((fix): correct anndata release for `io` usage); Backport PR #3298: (fix): correct anndata release for `io` usage,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3304
https://github.com/scverse/scanpy/pull/3304:126,deployability,releas,release,126,Backport PR #3298 on branch 1.10.x ((fix): correct anndata release for `io` usage); Backport PR #3298: (fix): correct anndata release for `io` usage,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3304
https://github.com/scverse/scanpy/pull/3306:1201,availability,state,stated,1201,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:903,deployability,modul,modulo,903,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:1233,deployability,updat,update,1233,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:1387,deployability,releas,release,1387,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:1412,deployability,Releas,Release,1412,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:936,energy efficiency,load,load,936,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:1201,integrability,state,stated,1201,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:1021,interoperability,format,format,1021,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:903,modifiability,modul,modulo,903,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:854,performance,disk,disk,854,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:936,performance,load,load,936,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:963,performance,memor,memory,963,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:1016,performance,disk,disk,1016,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:252,safety,review,review,252,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:903,safety,modul,modulo,903,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:1163,safety,reme,remembered,1163,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:1233,safety,updat,update,1233,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:1289,safety,Test,Tests,1289,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:1233,security,updat,update,1233,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:252,testability,review,review,252,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:1289,testability,Test,Tests,1289,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:103,usability,guid,guidelines,103,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:134,usability,guid,guide,134,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:230,usability,workflow,workflow,230,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:312,usability,clear,clear,312,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:328,usability,support,support,328,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:963,usability,memor,memory,963,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3306:1208,usability,clear,clearly,1208,"(fix): clarify sparse pca usage; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. This PR clarifies the docs/handling to make it clear we _only_ support this for correctly chunked CSR-dask. I think that not handling the other case is fine for a few reasons:. 1. CSC chunking would basically require multiple passes at the data. Every chunk (of size `(adata.shape[0], N)`) would need to be `X.T @ Y`-ed over the entire matrix where `X` is the chunk and `Y` is any given column-chunk from the matrix. I can't think of a way around this. 2. Given the above, I don't think there is any reason why we should implement this algorithm. People should just re-write their data to disk as CSR. I can't imagine its worse than this modulo the fact that you need to load the whole matrix into memory. This might be a good reason to change our on-disk format but at the moment, this is about all I think we should do. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Just remembered that this fact needs to be stated clearly. @Intron7 please update the RSC PR as well if you haven't already! - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: edited.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3306
https://github.com/scverse/scanpy/pull/3307:465,deployability,releas,release,465,(feat): `calculate_qc_metrics` with `dask`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2937. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307
https://github.com/scverse/scanpy/pull/3307:490,deployability,Releas,Release,490,(feat): `calculate_qc_metrics` with `dask`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2937. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307
https://github.com/scverse/scanpy/pull/3307:263,safety,review,review,263,(feat): `calculate_qc_metrics` with `dask`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2937. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307
https://github.com/scverse/scanpy/pull/3307:367,safety,Test,Tests,367,(feat): `calculate_qc_metrics` with `dask`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2937. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307
https://github.com/scverse/scanpy/pull/3307:263,testability,review,review,263,(feat): `calculate_qc_metrics` with `dask`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2937. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307
https://github.com/scverse/scanpy/pull/3307:367,testability,Test,Tests,367,(feat): `calculate_qc_metrics` with `dask`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2937. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307
https://github.com/scverse/scanpy/pull/3307:114,usability,guid,guidelines,114,(feat): `calculate_qc_metrics` with `dask`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2937. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307
https://github.com/scverse/scanpy/pull/3307:145,usability,guid,guide,145,(feat): `calculate_qc_metrics` with `dask`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2937. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307
https://github.com/scverse/scanpy/pull/3307:241,usability,workflow,workflow,241,(feat): `calculate_qc_metrics` with `dask`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2937. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307
https://github.com/scverse/scanpy/pull/3307:347,usability,Close,Closes,347,(feat): `calculate_qc_metrics` with `dask`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2937. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3307
https://github.com/scverse/scanpy/issues/3309:416,availability,replic,replicates,416,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:516,availability,replic,replicates,516,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:643,availability,replic,replicates,643,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:821,availability,replic,replicates,821,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:900,availability,replic,replicate,900,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:929,availability,replic,replicate,929,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:986,availability,replic,replicate,986,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:1015,availability,replic,replicate,1015,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:1235,deployability,scale,scale,1235,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:1235,energy efficiency,scale,scale,1235,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:1336,energy efficiency,current,current,1336,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:33,integrability,batch,batch,33,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:319,integrability,batch,batch,319,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:579,integrability,batch,batch,579,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:794,integrability,batch,batches,794,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:1270,integrability,batch,batch,1270,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:1291,integrability,batch,batch,1291,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:570,interoperability,specif,specific,570,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:836,interoperability,distribut,distributed,836,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:126,modifiability,paramet,parameters,126,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:1205,modifiability,pac,package,1205,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:1235,modifiability,scal,scale,1235,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:33,performance,batch,batch,33,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:307,performance,perform,perform,307,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:319,performance,batch,batch,319,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:579,performance,batch,batch,579,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:794,performance,batch,batches,794,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:1235,performance,scale,scale,1235,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:1270,performance,batch,batch,1270,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:1291,performance,batch,batch,1291,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:682,reliability,doe,doesn,682,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:479,testability,Simpl,Simply,479,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:307,usability,perform,perform,307,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3309:479,usability,Simpl,Simply,479,"Multibatchnorm / between library batch normalization; ### What kind of feature would you like to request? Additional function parameters / changed functionality / changed defaults? ### Please describe your wishes. Hi,. Is there an equivalent function to multiBatchNorm in Python, or another method that can perform per-batch normalization? My goal is to compute psuedobulk per indiviudal, Each individual sample has replicates that are processed across different libraries, . a- Simply summing the raw counts across replicates would likely introduce bias due to library-specific batch effects. . b- Taking the mean of normalized counts across replicates (scranPY normalized counts) doesn’t account for differences in size factors across the libraries, making normalization inconsistent between batches. important note :. replicates are distributed across different libraries. Individual x might have replicate 1 in library 1 and replicate 2 in library 3, while. Individual y might have replicate 1 in library 1 but replicate 2 in library 4. so thats why summing raw / normalized counts directly seem inaccurate . I’d greatly appreciate any advice. In R, I’ve previously used multiBatchNorm from the scran package, which normalizes and scale the size factors within each batch to handle such batch effects. However, given the size of my current dataset, using R is not feasible.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3309
https://github.com/scverse/scanpy/issues/3310:466,availability,error,errors,466,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:491,availability,error,errors,491,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:716,availability,error,error,716,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:2058,availability,error,error,2058,"ritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function gives no error but the `obsm` column is concatenated as an extra row. Example:. ```py. sc.get.obs_df(adata, keys='louvain', obsm_keys = [('X_pca', 1)]). ```. ```pytb. index. AAACATACAACCAC-1 CD4 T cells. AAACATTGAGCTAC-1 B cells. AAACATTGATCAGC-1 CD4 T cells. AAACCGTGCTTCCG-1 CD14+ Monocytes. AAACCGTGTATGCG-1 NK cells. ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,... Name: louvain, Length: 2639, dtype: object. ```. You get the expected output if you pass the keys as a List. ```py. sc.get.obs_df(adata, keys=['louvain'], obsm_keys = [('X_pca', 1)]). ```. Again, the quick fix would be to change the parameter description for `keys` to `List of keys`. . ### Versions. <details>. ```. -----. anndata 0.10.8. scanpy 1.10.1. -----. PIL 10.3.0. anyio NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. chars",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:218,deployability,version,version,218,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:2833,deployability,Version,Versions,2833," if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function gives no error but the `obsm` column is concatenated as an extra row. Example:. ```py. sc.get.obs_df(adata, keys='louvain', obsm_keys = [('X_pca', 1)]). ```. ```pytb. index. AAACATACAACCAC-1 CD4 T cells. AAACATTGAGCTAC-1 B cells. AAACATTGATCAGC-1 CD4 T cells. AAACCGTGCTTCCG-1 CD14+ Monocytes. AAACCGTGTATGCG-1 NK cells. ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,... Name: louvain, Length: 2639, dtype: object. ```. You get the expected output if you pass the keys as a List. ```py. sc.get.obs_df(adata, keys=['louvain'], obsm_keys = [('X_pca', 1)]). ```. Again, the quick fix would be to change the parameter description for `keys` to `List of keys`. . ### Versions. <details>. ```. -----. anndata 0.10.8. scanpy 1.10.1. -----. PIL 10.3.0. anyio NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. pro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:4933,deployability,updat,updated,4933,". babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 15.0.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2024.1. referencing NA. requests 2.32.2. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rich NA. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. setuptools 69.5.1. setuptools_scm NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. texttable 1.7.0. threadpoolctl 3.5.0. tomli 2.0.1. torch 2.3.1+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 2.2.1. vscode NA. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. yaml 6.0.1. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.0. notebook 7.2.0. -----. Python 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]. Linux-3.10.0-1160.119.1.el7.tuxcare.els2.x86_64-x86_64-with-glibc2.17. -----. Session information updated at 2024-10-23 08:47. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:218,integrability,version,version,218,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:2833,integrability,Version,Versions,2833," if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function gives no error but the `obsm` column is concatenated as an extra row. Example:. ```py. sc.get.obs_df(adata, keys='louvain', obsm_keys = [('X_pca', 1)]). ```. ```pytb. index. AAACATACAACCAC-1 CD4 T cells. AAACATTGAGCTAC-1 B cells. AAACATTGATCAGC-1 CD4 T cells. AAACCGTGCTTCCG-1 CD14+ Monocytes. AAACCGTGTATGCG-1 NK cells. ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,... Name: louvain, Length: 2639, dtype: object. ```. You get the expected output if you pass the keys as a List. ```py. sc.get.obs_df(adata, keys=['louvain'], obsm_keys = [('X_pca', 1)]). ```. Again, the quick fix would be to change the parameter description for `keys` to `List of keys`. . ### Versions. <details>. ```. -----. anndata 0.10.8. scanpy 1.10.1. -----. PIL 10.3.0. anyio NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. pro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:3814,interoperability,platform,platformdirs,3814,"keys`. . ### Versions. <details>. ```. -----. anndata 0.10.8. scanpy 1.10.1. -----. PIL 10.3.0. anyio NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 15.0.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2024.1. referencing NA. requests 2.32.2. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rich NA. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. setuptools 69.5.1. setuptools_scm NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. texttable 1.7.0. threadpoolctl 3.5.0. tomli 2.0.1. torch 2.3.1+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 2.2.1. vscode NA. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. yaml 6.0.1. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.0. notebook 7.2.0. -----. Python 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:218,modifiability,version,version,218,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:1136,modifiability,pac,packages,1136,"t already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function gives no error but the `obsm` column is concatenated as an extra row. Example:. ```py. sc.g",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:1202,modifiability,layer,layer,1202,"n the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function gives no error but the `obsm` column is concatenated as an extra row. Example:. ```py. sc.get.obs_df(adata, keys='louvain', obsm_keys = [('X_pca', 1)]). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:1535,modifiability,paramet,parameter,1535,"sm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function gives no error but the `obsm` column is concatenated as an extra row. Example:. ```py. sc.get.obs_df(adata, keys='louvain', obsm_keys = [('X_pca', 1)]). ```. ```pytb. index. AAACATACAACCAC-1 CD4 T cells. AAACATTGAGCTAC-1 B cells. AAACATTGATCAGC-1 CD4 T cells. AAACCGTGCTTCCG-1 CD14+ Monocytes. AAACCGTGTATGCG-1 NK cells. ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,..",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:2775,modifiability,paramet,parameter,2775,"546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function gives no error but the `obsm` column is concatenated as an extra row. Example:. ```py. sc.get.obs_df(adata, keys='louvain', obsm_keys = [('X_pca', 1)]). ```. ```pytb. index. AAACATACAACCAC-1 CD4 T cells. AAACATTGAGCTAC-1 B cells. AAACATTGATCAGC-1 CD4 T cells. AAACCGTGCTTCCG-1 CD14+ Monocytes. AAACCGTGTATGCG-1 NK cells. ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,... Name: louvain, Length: 2639, dtype: object. ```. You get the expected output if you pass the keys as a List. ```py. sc.get.obs_df(adata, keys=['louvain'], obsm_keys = [('X_pca', 1)]). ```. Again, the quick fix would be to change the parameter description for `keys` to `List of keys`. . ### Versions. <details>. ```. -----. anndata 0.10.8. scanpy 1.10.1. -----. PIL 10.3.0. anyio NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:2833,modifiability,Version,Versions,2833," if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function gives no error but the `obsm` column is concatenated as an extra row. Example:. ```py. sc.get.obs_df(adata, keys='louvain', obsm_keys = [('X_pca', 1)]). ```. ```pytb. index. AAACATACAACCAC-1 CD4 T cells. AAACATTGAGCTAC-1 B cells. AAACATTGATCAGC-1 CD4 T cells. AAACCGTGCTTCCG-1 CD14+ Monocytes. AAACCGTGTATGCG-1 NK cells. ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,... Name: louvain, Length: 2639, dtype: object. ```. You get the expected output if you pass the keys as a List. ```py. sc.get.obs_df(adata, keys=['louvain'], obsm_keys = [('X_pca', 1)]). ```. Again, the quick fix would be to change the parameter description for `keys` to `List of keys`. . ### Versions. <details>. ```. -----. anndata 0.10.8. scanpy 1.10.1. -----. PIL 10.3.0. anyio NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. pro",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:3175,modifiability,deco,decorator,3175,"ys = [('X_pca', 1)]). ```. ```pytb. index. AAACATACAACCAC-1 CD4 T cells. AAACATTGAGCTAC-1 B cells. AAACATTGATCAGC-1 CD4 T cells. AAACCGTGCTTCCG-1 CD14+ Monocytes. AAACCGTGTATGCG-1 NK cells. ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,... Name: louvain, Length: 2639, dtype: object. ```. You get the expected output if you pass the keys as a List. ```py. sc.get.obs_df(adata, keys=['louvain'], obsm_keys = [('X_pca', 1)]). ```. Again, the quick fix would be to change the parameter description for `keys` to `List of keys`. . ### Versions. <details>. ```. -----. anndata 0.10.8. scanpy 1.10.1. -----. PIL 10.3.0. anyio NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 15.0.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2024.1. referencing NA. requests 2.32.2. rfc3339_validator 0.1.4. rfc3986_validator 0",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:3771,modifiability,pac,packaging,3771,arameter description for `keys` to `List of keys`. . ### Versions. <details>. ```. -----. anndata 0.10.8. scanpy 1.10.1. -----. PIL 10.3.0. anyio NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 15.0.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2024.1. referencing NA. requests 2.32.2. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rich NA. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. setuptools 69.5.1. setuptools_scm NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. texttable 1.7.0. threadpoolctl 3.5.0. tomli 2.0.1. torch 2.3.1+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 2.2.1. vscode NA. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. yaml 6.0.1. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.0. notebook 7.2.0. -----. Python 3.10.14 | packaged by,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:4765,modifiability,pac,packaged,4765,". babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 15.0.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2024.1. referencing NA. requests 2.32.2. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rich NA. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. setuptools 69.5.1. setuptools_scm NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. texttable 1.7.0. threadpoolctl 3.5.0. tomli 2.0.1. torch 2.3.1+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 2.2.1. vscode NA. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. yaml 6.0.1. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.0. notebook 7.2.0. -----. Python 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]. Linux-3.10.0-1160.119.1.el7.tuxcare.els2.x86_64-x86_64-with-glibc2.17. -----. Session information updated at 2024-10-23 08:47. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:466,performance,error,errors,466,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:491,performance,error,errors,491,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:716,performance,error,error,716,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:2058,performance,error,error,2058,"ritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function gives no error but the `obsm` column is concatenated as an extra row. Example:. ```py. sc.get.obs_df(adata, keys='louvain', obsm_keys = [('X_pca', 1)]). ```. ```pytb. index. AAACATACAACCAC-1 CD4 T cells. AAACATTGAGCTAC-1 B cells. AAACATTGATCAGC-1 CD4 T cells. AAACCGTGCTTCCG-1 CD14+ Monocytes. AAACCGTGTATGCG-1 NK cells. ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,... Name: louvain, Length: 2639, dtype: object. ```. You get the expected output if you pass the keys as a List. ```py. sc.get.obs_df(adata, keys=['louvain'], obsm_keys = [('X_pca', 1)]). ```. Again, the quick fix would be to change the parameter description for `keys` to `List of keys`. . ### Versions. <details>. ```. -----. anndata 0.10.8. scanpy 1.10.1. -----. PIL 10.3.0. anyio NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. chars",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:8,safety,input,input,8,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:372,safety,input,input,372,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:427,safety,avoid,avoid,427,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:466,safety,error,errors,466,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:491,safety,error,errors,491,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:716,safety,error,error,716,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:1674,safety,input,input,1674,"t/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function gives no error but the `obsm` column is concatenated as an extra row. Example:. ```py. sc.get.obs_df(adata, keys='louvain', obsm_keys = [('X_pca', 1)]). ```. ```pytb. index. AAACATACAACCAC-1 CD4 T cells. AAACATTGAGCTAC-1 B cells. AAACATTGATCAGC-1 CD4 T cells. AAACCGTGCTTCCG-1 CD14+ Monocytes. AAACCGTGTATGCG-1 NK cells. ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,... Name: louvain, Length: 2639, dtype: object. ```. You get the expected output if you pass the keys as a List. ```py. sc.get.obs_df(adata",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:2058,safety,error,error,2058,"ritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function gives no error but the `obsm` column is concatenated as an extra row. Example:. ```py. sc.get.obs_df(adata, keys='louvain', obsm_keys = [('X_pca', 1)]). ```. ```pytb. index. AAACATACAACCAC-1 CD4 T cells. AAACATTGAGCTAC-1 B cells. AAACATTGATCAGC-1 CD4 T cells. AAACCGTGCTTCCG-1 CD14+ Monocytes. AAACCGTGTATGCG-1 NK cells. ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,... Name: louvain, Length: 2639, dtype: object. ```. You get the expected output if you pass the keys as a List. ```py. sc.get.obs_df(adata, keys=['louvain'], obsm_keys = [('X_pca', 1)]). ```. Again, the quick fix would be to change the parameter description for `keys` to `List of keys`. . ### Versions. <details>. ```. -----. anndata 0.10.8. scanpy 1.10.1. -----. PIL 10.3.0. anyio NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. chars",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:3210,safety,except,exceptiongroup,3210,"ndex. AAACATACAACCAC-1 CD4 T cells. AAACATTGAGCTAC-1 B cells. AAACATTGATCAGC-1 CD4 T cells. AAACCGTGCTTCCG-1 CD14+ Monocytes. AAACCGTGTATGCG-1 NK cells. ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,... Name: louvain, Length: 2639, dtype: object. ```. You get the expected output if you pass the keys as a List. ```py. sc.get.obs_df(adata, keys=['louvain'], obsm_keys = [('X_pca', 1)]). ```. Again, the quick fix would be to change the parameter description for `keys` to `List of keys`. . ### Versions. <details>. ```. -----. anndata 0.10.8. scanpy 1.10.1. -----. PIL 10.3.0. anyio NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 15.0.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2024.1. referencing NA. requests 2.32.2. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rich NA. rpds NA. scipy 1.13.0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:4933,safety,updat,updated,4933,". babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 15.0.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2024.1. referencing NA. requests 2.32.2. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rich NA. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. setuptools 69.5.1. setuptools_scm NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. texttable 1.7.0. threadpoolctl 3.5.0. tomli 2.0.1. torch 2.3.1+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 2.2.1. vscode NA. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. yaml 6.0.1. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.0. notebook 7.2.0. -----. Python 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]. Linux-3.10.0-1160.119.1.el7.tuxcare.els2.x86_64-x86_64-with-glibc2.17. -----. Session information updated at 2024-10-23 08:47. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:3008,security,certif,certifi,3008,"eys`. When you do that, the function gives no error but the `obsm` column is concatenated as an extra row. Example:. ```py. sc.get.obs_df(adata, keys='louvain', obsm_keys = [('X_pca', 1)]). ```. ```pytb. index. AAACATACAACCAC-1 CD4 T cells. AAACATTGAGCTAC-1 B cells. AAACATTGATCAGC-1 CD4 T cells. AAACCGTGCTTCCG-1 CD14+ Monocytes. AAACCGTGTATGCG-1 NK cells. ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,... Name: louvain, Length: 2639, dtype: object. ```. You get the expected output if you pass the keys as a List. ```py. sc.get.obs_df(adata, keys=['louvain'], obsm_keys = [('X_pca', 1)]). ```. Again, the quick fix would be to change the parameter description for `keys` to `List of keys`. . ### Versions. <details>. ```. -----. anndata 0.10.8. scanpy 1.10.1. -----. PIL 10.3.0. anyio NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 15.0.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:3362,security,iso,isoduration,3362,". ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,... Name: louvain, Length: 2639, dtype: object. ```. You get the expected output if you pass the keys as a List. ```py. sc.get.obs_df(adata, keys=['louvain'], obsm_keys = [('X_pca', 1)]). ```. Again, the quick fix would be to change the parameter description for `keys` to `List of keys`. . ### Versions. <details>. ```. -----. anndata 0.10.8. scanpy 1.10.1. -----. PIL 10.3.0. anyio NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 15.0.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2024.1. referencing NA. requests 2.32.2. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rich NA. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. setuptools 69.5.1. setuptools_scm NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. text",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:4333,security,soc,socks,4333,". babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 15.0.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2024.1. referencing NA. requests 2.32.2. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rich NA. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. setuptools 69.5.1. setuptools_scm NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. texttable 1.7.0. threadpoolctl 3.5.0. tomli 2.0.1. torch 2.3.1+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 2.2.1. vscode NA. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. yaml 6.0.1. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.0. notebook 7.2.0. -----. Python 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]. Linux-3.10.0-1160.119.1.el7.tuxcare.els2.x86_64-x86_64-with-glibc2.17. -----. Session information updated at 2024-10-23 08:47. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:4913,security,Session,Session,4913,". babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 15.0.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2024.1. referencing NA. requests 2.32.2. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rich NA. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. setuptools 69.5.1. setuptools_scm NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. texttable 1.7.0. threadpoolctl 3.5.0. tomli 2.0.1. torch 2.3.1+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 2.2.1. vscode NA. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. yaml 6.0.1. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.0. notebook 7.2.0. -----. Python 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]. Linux-3.10.0-1160.119.1.el7.tuxcare.els2.x86_64-x86_64-with-glibc2.17. -----. Session information updated at 2024-10-23 08:47. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:4933,security,updat,updated,4933,". babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. charset_normalizer 3.3.2. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.1. decorator 5.1.1. defusedxml 0.7.1. exceptiongroup 1.2.0. executing 2.0.1. fastjsonschema NA. fqdn NA. google NA. h5py 3.11.0. idna 3.7. igraph 0.11.5. ipykernel 6.29.4. ipywidgets 8.1.2. isoduration NA. jedi 0.19.1. jinja2 3.1.4. joblib 1.4.2. json5 0.9.25. jsonpointer 2.4. jsonschema 4.22.0. jsonschema_specifications NA. jupyter_events 0.10.0. jupyter_server 2.14.0. jupyterlab_server 2.27.1. kiwisolver 1.4.5. legacy_api_wrap NA. leidenalg 0.10.2. llvmlite 0.43.0. markupsafe 2.1.5. matplotlib 3.9.0. mpl_toolkits NA. natsort 8.4.0. nbformat 5.10.4. numba 0.60.0. numpy 1.26.4. overrides NA. packaging 24.0. pandas 2.2.2. parso 0.8.4. platformdirs 4.2.2. prometheus_client NA. prompt_toolkit 3.0.43. psutil 5.9.8. pure_eval 0.2.2. pyarrow 15.0.2. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.2. pythonjsonlogger NA. pytz 2024.1. referencing NA. requests 2.32.2. rfc3339_validator 0.1.4. rfc3986_validator 0.1.1. rich NA. rpds NA. scipy 1.13.0. send2trash NA. session_info 1.0.0. setuptools 69.5.1. setuptools_scm NA. six 1.16.0. sklearn 1.4.2. sniffio 1.3.1. socks 1.7.1. stack_data 0.6.3. texttable 1.7.0. threadpoolctl 3.5.0. tomli 2.0.1. torch 2.3.1+cu121. torchgen NA. tornado 6.4. tqdm 4.66.4. traitlets 5.14.3. typing_extensions NA. uri_template NA. urllib3 2.2.1. vscode NA. wcwidth 0.2.13. webcolors 1.13. websocket 1.8.0. yaml 6.0.1. zmq 26.0.3. zoneinfo NA. -----. IPython 8.24.0. jupyter_client 8.6.1. jupyter_core 5.7.2. jupyterlab 4.2.0. notebook 7.2.0. -----. Python 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]. Linux-3.10.0-1160.119.1.el7.tuxcare.els2.x86_64-x86_64-with-glibc2.17. -----. Session information updated at 2024-10-23 08:47. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:922,testability,Trace,Traceback,922,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:8,usability,input,input,8,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:178,usability,confirm,confirmed,178,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:261,usability,confirm,confirmed,261,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:372,usability,input,input,372,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:466,usability,error,errors,466,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:491,usability,error,errors,491,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:716,usability,error,error,716,"Improve input descriptions for scanpy.get.obs_df; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass somethin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:1067,usability,user,users,1067," these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? There are a few instances where the input for `sc.get.obs_df` could be described better to avoid some edge cases that don't throw errors (or throw cryptic errors). . 1. The param descriptions say that `obsm_keys` expects a [Tuple of (key, column)](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function gives no error but th",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:1674,usability,input,input,1674,"t/get.py#L240-L241), but this gives an error:. ```py. adata = sc.datasets.pbmc3k_processed(). sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). ``` . ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[15], line 1. ----> 1 sc.get.obs_df(adata, obsm_keys = ('X_pca', 1)). File /oak/stanford/groups/pritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function gives no error but the `obsm` column is concatenated as an extra row. Example:. ```py. sc.get.obs_df(adata, keys='louvain', obsm_keys = [('X_pca', 1)]). ```. ```pytb. index. AAACATACAACCAC-1 CD4 T cells. AAACATTGAGCTAC-1 B cells. AAACATTGATCAGC-1 CD4 T cells. AAACCGTGCTTCCG-1 CD14+ Monocytes. AAACCGTGTATGCG-1 NK cells. ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,... Name: louvain, Length: 2639, dtype: object. ```. You get the expected output if you pass the keys as a List. ```py. sc.get.obs_df(adata",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/issues/3310:2058,usability,error,error,2058,"ritch/users/emma/miniforge3/envs/perturb-vs-tissue-env/lib/python3.10/site-packages/scanpy/get/get.py:328, in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 325 if keys:. 326 df = df[keys]. --> 328 for k, idx in obsm_keys:. 329 added_k = f""{k}-{idx}"". 330 val = adata.obsm[k]. ValueError: too many values to unpack (expected 2). ```. The function works if you pass a list of Tuples:. ```. sc.get.obs_df(adata, obsm_keys = [('X_pca', 1)]). ```. So perhaps the parameter descriptions should say `List of Tuples of (key, column)`? Or the case of extracting a single column should be handled. . 2. The input for the `keys` is described as [""keys""](https://github.com/scverse/scanpy/blob/39c6532d276ca83cc0548546c3d73ebee6eec0c1/src/scanpy/get/get.py#L238-L239), but if you pass only one key as a string, the function returns a `pd.Series` instead of a `pd.DataFrame`. This is not a massive problem, unless you also pass something to `obsm_keys`. When you do that, the function gives no error but the `obsm` column is concatenated as an extra row. Example:. ```py. sc.get.obs_df(adata, keys='louvain', obsm_keys = [('X_pca', 1)]). ```. ```pytb. index. AAACATACAACCAC-1 CD4 T cells. AAACATTGAGCTAC-1 B cells. AAACATTGATCAGC-1 CD4 T cells. AAACCGTGCTTCCG-1 CD14+ Monocytes. AAACCGTGTATGCG-1 NK cells. ... . TTTCTACTGAGGCA-1 B cells. TTTCTACTTCCTCG-1 B cells. TTTGCATGAGAGGC-1 B cells. TTTGCATGCCTCAC-1 CD4 T cells. X_pca-1 [0.2577139, 7.4819846, -1.5836583, -1.3685299,... Name: louvain, Length: 2639, dtype: object. ```. You get the expected output if you pass the keys as a List. ```py. sc.get.obs_df(adata, keys=['louvain'], obsm_keys = [('X_pca', 1)]). ```. Again, the quick fix would be to change the parameter description for `keys` to `List of keys`. . ### Versions. <details>. ```. -----. anndata 0.10.8. scanpy 1.10.1. -----. PIL 10.3.0. anyio NA. arrow 1.3.0. asttokens NA. attr 23.2.0. attrs 23.2.0. babel 2.15.0. brotli 1.1.0. certifi 2024.07.04. cffi 1.16.0. chardet 5.2.0. chars",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3310
https://github.com/scverse/scanpy/pull/3311:128,deployability,Releas,Release,128,Fix some `Returns` docstrs re: `inplace` semantics; - [x] Tests included or not required because: comment/docstring nits. - [x] Release notes not necessary because: comment/docstring nits.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3311
https://github.com/scverse/scanpy/pull/3311:41,interoperability,semant,semantics,41,Fix some `Returns` docstrs re: `inplace` semantics; - [x] Tests included or not required because: comment/docstring nits. - [x] Release notes not necessary because: comment/docstring nits.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3311
https://github.com/scverse/scanpy/pull/3311:58,safety,Test,Tests,58,Fix some `Returns` docstrs re: `inplace` semantics; - [x] Tests included or not required because: comment/docstring nits. - [x] Release notes not necessary because: comment/docstring nits.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3311
https://github.com/scverse/scanpy/pull/3311:58,testability,Test,Tests,58,Fix some `Returns` docstrs re: `inplace` semantics; - [x] Tests included or not required because: comment/docstring nits. - [x] Release notes not necessary because: comment/docstring nits.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3311
https://github.com/scverse/scanpy/pull/3312:77,interoperability,semant,semantics,77,Backport PR #3311 on branch 1.10.x (Fix some `Returns` docstrs re: `inplace` semantics); Backport PR #3311: Fix some `Returns` docstrs re: `inplace` semantics,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3312
https://github.com/scverse/scanpy/pull/3312:149,interoperability,semant,semantics,149,Backport PR #3311 on branch 1.10.x (Fix some `Returns` docstrs re: `inplace` semantics); Backport PR #3311: Fix some `Returns` docstrs re: `inplace` semantics,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3312
https://github.com/scverse/scanpy/pull/3313:527,availability,error,error,527,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:638,availability,error,error,638,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:958,availability,error,error,958,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:755,deployability,releas,release,755,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:780,deployability,Releas,Release,780,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:43,integrability,batch,batch,43,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:605,integrability,batch,batch,605,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:1023,integrability,batch,batch,1023,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:43,performance,batch,batch,43,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:527,performance,error,error,527,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:605,performance,batch,batch,605,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:638,performance,error,error,638,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:958,performance,error,error,958,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:1023,performance,batch,batch,1023,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:298,safety,review,review,298,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:398,safety,Test,Tests,398,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:527,safety,error,error,527,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:638,safety,error,error,638,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:958,safety,error,error,958,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:298,testability,review,review,298,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:398,testability,Test,Tests,398,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:149,usability,guid,guidelines,149,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:180,usability,guid,guide,180,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:276,usability,workflow,workflow,276,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:382,usability,Close,Closes,382,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:527,usability,error,error,527,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:638,usability,error,error,638,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3313:958,usability,error,error,958,"fix bug for partial_fit when the number of batch samples is less than n_comp ; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because: n_components must be less or equal to the number of samples, otherwise it would throw an error, for example, ValueError: n_components=100 must be less or equal to the batch number of samples 40. This error usually happens on the last chunk of the partial_fit. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because:. For example, my adata.shape[0] is 1041 and I run IncrementalPCA `sc.tl.pca(adata, n_comps=100, chunked=True,chunk_size=1000)`, and I got an error: ValueError: n_components=100 must be less or equal to the batch number of samples 40 on scanpy/preprocessing/_pca.py:256",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3313
https://github.com/scverse/scanpy/pull/3315:266,availability,error,error,266,"Move PSE outside try except; Follow up to https://github.com/scverse/scanpy/pull/3275#pullrequestreview-2392213666. I think that this woulda worked regardless because the `try` will define it initially anyways. However, it's best practice to only have code that can error in the `try` statement. I'll allow myself to skip a release note etc because this is minor and there hasn't been a release yet with this fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3315
https://github.com/scverse/scanpy/pull/3315:285,availability,state,statement,285,"Move PSE outside try except; Follow up to https://github.com/scverse/scanpy/pull/3275#pullrequestreview-2392213666. I think that this woulda worked regardless because the `try` will define it initially anyways. However, it's best practice to only have code that can error in the `try` statement. I'll allow myself to skip a release note etc because this is minor and there hasn't been a release yet with this fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3315
https://github.com/scverse/scanpy/pull/3315:324,deployability,releas,release,324,"Move PSE outside try except; Follow up to https://github.com/scverse/scanpy/pull/3275#pullrequestreview-2392213666. I think that this woulda worked regardless because the `try` will define it initially anyways. However, it's best practice to only have code that can error in the `try` statement. I'll allow myself to skip a release note etc because this is minor and there hasn't been a release yet with this fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3315
https://github.com/scverse/scanpy/pull/3315:387,deployability,releas,release,387,"Move PSE outside try except; Follow up to https://github.com/scverse/scanpy/pull/3275#pullrequestreview-2392213666. I think that this woulda worked regardless because the `try` will define it initially anyways. However, it's best practice to only have code that can error in the `try` statement. I'll allow myself to skip a release note etc because this is minor and there hasn't been a release yet with this fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3315
https://github.com/scverse/scanpy/pull/3315:285,integrability,state,statement,285,"Move PSE outside try except; Follow up to https://github.com/scverse/scanpy/pull/3275#pullrequestreview-2392213666. I think that this woulda worked regardless because the `try` will define it initially anyways. However, it's best practice to only have code that can error in the `try` statement. I'll allow myself to skip a release note etc because this is minor and there hasn't been a release yet with this fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3315
https://github.com/scverse/scanpy/pull/3315:266,performance,error,error,266,"Move PSE outside try except; Follow up to https://github.com/scverse/scanpy/pull/3275#pullrequestreview-2392213666. I think that this woulda worked regardless because the `try` will define it initially anyways. However, it's best practice to only have code that can error in the `try` statement. I'll allow myself to skip a release note etc because this is minor and there hasn't been a release yet with this fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3315
https://github.com/scverse/scanpy/pull/3315:230,reliability,pra,practice,230,"Move PSE outside try except; Follow up to https://github.com/scverse/scanpy/pull/3275#pullrequestreview-2392213666. I think that this woulda worked regardless because the `try` will define it initially anyways. However, it's best practice to only have code that can error in the `try` statement. I'll allow myself to skip a release note etc because this is minor and there hasn't been a release yet with this fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3315
https://github.com/scverse/scanpy/pull/3315:21,safety,except,except,21,"Move PSE outside try except; Follow up to https://github.com/scverse/scanpy/pull/3275#pullrequestreview-2392213666. I think that this woulda worked regardless because the `try` will define it initially anyways. However, it's best practice to only have code that can error in the `try` statement. I'll allow myself to skip a release note etc because this is minor and there hasn't been a release yet with this fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3315
https://github.com/scverse/scanpy/pull/3315:266,safety,error,error,266,"Move PSE outside try except; Follow up to https://github.com/scverse/scanpy/pull/3275#pullrequestreview-2392213666. I think that this woulda worked regardless because the `try` will define it initially anyways. However, it's best practice to only have code that can error in the `try` statement. I'll allow myself to skip a release note etc because this is minor and there hasn't been a release yet with this fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3315
https://github.com/scverse/scanpy/pull/3315:266,usability,error,error,266,"Move PSE outside try except; Follow up to https://github.com/scverse/scanpy/pull/3275#pullrequestreview-2392213666. I think that this woulda worked regardless because the `try` will define it initially anyways. However, it's best practice to only have code that can error in the `try` statement. I'll allow myself to skip a release note etc because this is minor and there hasn't been a release yet with this fix.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3315
https://github.com/scverse/scanpy/pull/3316:439,deployability,releas,release,439,Refactor regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316
https://github.com/scverse/scanpy/pull/3316:464,deployability,Releas,Release,464,Refactor regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316
https://github.com/scverse/scanpy/pull/3316:0,modifiability,Refact,Refactor,0,Refactor regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316
https://github.com/scverse/scanpy/pull/3316:0,performance,Refactor,Refactor,0,Refactor regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316
https://github.com/scverse/scanpy/pull/3316:241,safety,review,review,241,Refactor regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316
https://github.com/scverse/scanpy/pull/3316:341,safety,Test,Tests,341,Refactor regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316
https://github.com/scverse/scanpy/pull/3316:241,testability,review,review,241,Refactor regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316
https://github.com/scverse/scanpy/pull/3316:341,testability,Test,Tests,341,Refactor regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316
https://github.com/scverse/scanpy/pull/3316:92,usability,guid,guidelines,92,Refactor regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316
https://github.com/scverse/scanpy/pull/3316:123,usability,guid,guide,123,Refactor regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316
https://github.com/scverse/scanpy/pull/3316:219,usability,workflow,workflow,219,Refactor regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316
https://github.com/scverse/scanpy/pull/3316:325,usability,Close,Closes,325,Refactor regress_out; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3316
https://github.com/scverse/scanpy/pull/3317:457,deployability,releas,release,457,Fix zappy compatibility for clip_array; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3317
https://github.com/scverse/scanpy/pull/3317:482,deployability,Releas,Release,482,Fix zappy compatibility for clip_array; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3317
https://github.com/scverse/scanpy/pull/3317:10,interoperability,compatib,compatibility,10,Fix zappy compatibility for clip_array; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3317
https://github.com/scverse/scanpy/pull/3317:259,safety,review,review,259,Fix zappy compatibility for clip_array; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3317
https://github.com/scverse/scanpy/pull/3317:359,safety,Test,Tests,359,Fix zappy compatibility for clip_array; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3317
https://github.com/scverse/scanpy/pull/3317:259,testability,review,review,259,Fix zappy compatibility for clip_array; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3317
https://github.com/scverse/scanpy/pull/3317:359,testability,Test,Tests,359,Fix zappy compatibility for clip_array; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3317
https://github.com/scverse/scanpy/pull/3317:110,usability,guid,guidelines,110,Fix zappy compatibility for clip_array; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3317
https://github.com/scverse/scanpy/pull/3317:141,usability,guid,guide,141,Fix zappy compatibility for clip_array; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3317
https://github.com/scverse/scanpy/pull/3317:237,usability,workflow,workflow,237,Fix zappy compatibility for clip_array; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3317
https://github.com/scverse/scanpy/pull/3317:343,usability,Close,Closes,343,Fix zappy compatibility for clip_array; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3317
https://github.com/scverse/scanpy/issues/3318:38,availability,error,errors,38,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1116,availability,Error,Error,1116,"this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U06",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:214,deployability,version,version,214,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:467,deployability,fail,fails,467,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:663,deployability,observ,observation,663,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:2271,deployability,log,log,2271,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:2497,deployability,log,log,2497,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:3056,deployability,Version,Versions,3056,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:214,integrability,version,version,214,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1805,integrability,wrap,wrapper,1805,"x=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1858,integrability,wrap,wraps,1858,"eate the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:2335,integrability,sub,subplots,2335,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:3056,integrability,Version,Versions,3056,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1805,interoperability,wrapper,wrapper,1805,"x=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:2788,interoperability,format,formatter,2788,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:32,modifiability,layer,layer,32,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:214,modifiability,version,version,214,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:371,modifiability,layer,layer,371,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:411,modifiability,layer,layer,411,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:482,modifiability,layer,layer,482,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:488,modifiability,paramet,parameter,488,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:686,modifiability,variab,variable,686,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:939,modifiability,layer,layer,939,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:966,modifiability,layer,layers,966,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1086,modifiability,layer,layer,1086,"et. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1368,modifiability,layer,layer,1368,"layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1398,modifiability,layer,layers,1398,"from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1526,modifiability,layer,layer,1526,". import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Mini",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1557,modifiability,layer,layer,1557,"ndas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1587,modifiability,layer,layers,1587,"ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\cate",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1741,modifiability,pac,packages,1741,"ll_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fli",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:2168,modifiability,pac,packages,2168,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:2569,modifiability,pac,packages,2569,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:3039,modifiability,layer,layer,3039,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:3056,modifiability,Version,Versions,3056,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:38,performance,error,errors,38,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1116,performance,Error,Error,1116,"this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U06",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:467,reliability,fail,fails,467,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:38,safety,error,errors,38,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:934,safety,Test,Test,934,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1048,safety,test,test,1048,"Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1116,safety,Error,Error,1116,"this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U06",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1363,safety,Test,Test,1363," in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1480,safety,test,test,1480,"layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1552,safety,Test,Test,1552,"ort pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_sp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1669,safety,test,test,1669,"ion (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, pal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:2271,safety,log,log,2271,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:2497,safety,log,log,2497,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:2271,security,log,log,2271,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:2497,security,log,log,2497,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:663,testability,observ,observation,663,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:934,testability,Test,Test,934,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1048,testability,test,test,1048,"Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1301,testability,Trace,Traceback,1301,"canpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1363,testability,Test,Test,1363," in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1480,testability,test,test,1480,"layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total co",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1552,testability,Test,Test,1552,"ort pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_sp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1669,testability,test,test,1669,"ion (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, pal",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:2271,testability,log,log,2271,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:2497,testability,log,log,2497,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:38,usability,error,errors,38,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:174,usability,confirm,confirmed,174,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:257,usability,confirm,confirmed,257,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:503,usability,Minim,Minimal,503,"sc.pl.highest_expr_genes() with layer errors; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, *",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1116,usability,Error,Error,1116,"this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Trying to store normalised values in a layer 'normalised', then plot from that layer with sc.pl.highest_expr_genes(). But the function fails with the layer parameter. ### Minimal code sample. ```py. import numpy as np. import pandas as pd. import anndata as ad. # Create a small data matrix. data = np.random.rand(10, 5). # Create observation (cell) and variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U06",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:1683,usability,User,Users,1683,"variable (gene) annotations. obs = pd.DataFrame(index=[f'Cell_{i}' for i in range(data.shape[0])]). var = pd.DataFrame(index=[f'Gene_{i}' for i in range(data.shape[1])]). # Create the AnnData object. adata = ad.AnnData(X=data, obs=obs, var=var). # Test layer call function. adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. sc.pl.highest_expr_genes(adata, layer='normalised'). ```. ### Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturatio",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:2110,usability,User,Users,2110,"## Error output. ```pytb. Output exceeds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3318:2511,usability,User,Users,2511,"ds the size limit. Open the full output data in a text editor. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). Cell In[32], line 17. 15 # Test layer call function. 16 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. ---> 17 sc.pl.highest_expr_genes(adata, layer='normalised'). 19 # Test layer call function. 20 adata.layers['normalised'] = adata.X # not doing any manipulation of X at the moment to test. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\legacy_api_wrap\__init__.py:80, in legacy_api.<locals>.wrapper.<locals>.fn_compatible(*args_all, **kw). 77 @wraps(fn). 78 def fn_compatible(*args_all: P.args, **kw: P.kwargs) -> R:. 79 if len(args_all) <= n_positional:. ---> 80 return fn(*args_all, **kw). 82 args_pos: P.args. 83 args_pos, args_rest = args_all[:n_positional], args_all[n_positional:]. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\scanpy\plotting\_qc.py:100, in highest_expr_genes(adata, n_top, show, save, ax, gene_symbols, log, **kwds). 98 height = (n_top * 0.2) + 1.5. 99 fig, ax = plt.subplots(figsize=(5, height)). --> 100 sns.boxplot(data=counts_top_genes, orient=""h"", ax=ax, fliersize=1, **kwds). 101 ax.set_xlabel(""% of total counts""). 102 if log:. File c:\Users\U062951\Miniconda3\envs\imc_ome_v1_spatial\lib\site-packages\seaborn\categorical.py:1634, in boxplot(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, dodge, width, gap, whis, linecolor, linewidth, fliersize, hue_norm, native_scale, log_scale, formatter, legend, ax, **kwargs). ... --> 700 artists = ax.bxp(**boxplot_kws). 702 # Reset artist widths after adding so everything stays positive. 703 ori_idx = [""x"", ""y""].index(self.orient). TypeError: Axes.bxp() got an unexpected keyword argument 'layer'. ```. ### Versions. <details>. ```. scanpy -v 1.10.3. anndata -v 0.10.9. python -v 3.10. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3318
https://github.com/scverse/scanpy/issues/3320:1076,availability,cluster,clustering,1076,"t of genes; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1434,availability,cluster,cluster,1434,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1727,availability,cluster,cluster,1727,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1819,availability,Error,Error,1819,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:261,deployability,version,version,261,"scanpy.pl.stacked_violin - remove ""marker label"" whitespace if var_names is a list of genes; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:405,deployability,stack,stacked,405,"scanpy.pl.stacked_violin - remove ""marker label"" whitespace if var_names is a list of genes; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1076,deployability,cluster,clustering,1076,"t of genes; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1434,deployability,cluster,cluster,1434,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1727,deployability,cluster,cluster,1727,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1934,deployability,Version,Versions,1934,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1987,deployability,version,versions,1987,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:754,energy efficiency,current,currently,754,"scanpy.pl.stacked_violin - remove ""marker label"" whitespace if var_names is a list of genes; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:261,integrability,version,version,261,"scanpy.pl.stacked_violin - remove ""marker label"" whitespace if var_names is a list of genes; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1934,integrability,Version,Versions,1934,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1987,integrability,version,versions,1987,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:261,modifiability,version,version,261,"scanpy.pl.stacked_violin - remove ""marker label"" whitespace if var_names is a list of genes; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1934,modifiability,Version,Versions,1934,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1979,modifiability,pac,package,1979,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1987,modifiability,version,versions,1987,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1819,performance,Error,Error,1819,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1125,safety,test,testing,1125,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1819,safety,Error,Error,1819,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1125,testability,test,testing,1125,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:221,usability,confirm,confirmed,221,"scanpy.pl.stacked_violin - remove ""marker label"" whitespace if var_names is a list of genes; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:304,usability,confirm,confirmed,304,"scanpy.pl.stacked_violin - remove ""marker label"" whitespace if var_names is a list of genes; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:856,usability,undo,undo,856,"scanpy.pl.stacked_violin - remove ""marker label"" whitespace if var_names is a list of genes; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:962,usability,Minim,Minimal,962,"scanpy.pl.stacked_violin - remove ""marker label"" whitespace if var_names is a list of genes; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/issues/3320:1819,usability,Error,Error,1819,"ons are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [x] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Basically I am creating a stacked violin plot that uses a list of marker genes for the ""var_names"" argument. But whenever I create the plot, it has extra whitespace at the top of the plot where the marker gene labels should go. This is very evident when you add a figure title, which gets place above the padding whitespace. I have not really found a way around this, and am currently looking through the scanpy internal code to see if there is some padding setting that I can undo. If there is a workaround to this or some option that I am missing I would like to know. Thanks! ### Minimal code sample. ```py. # I have an AnnData object that has undergone the Seurat analysis. I named the Leiden clustering output ""spatial_clusters"" since I was testing a spatial dataset read in via spatialdata then converted to AnnData with ""spatialdata_io.experimental.to_legacy_anndata"". marker_genes = [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1"", ""Sox2""] # 5 random Cochlear HCs P7 + Sox2. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ### COMPARISON TO MARKER GENES WITH LABELS. marker_genes = {""IHC"": [""Pou4f3"", ""Calb2"", ""Pvalb"", ""Smpx"", ""Mlf1""], ""Random"": [""Sox2""]}. sc.pl.stacked_violin(adata, marker_genes, title=""Marker gene expression per cluster"", groupby=""spatial_clusters"", cmap=""YlOrRd"", show=False, return_fig=True). ```. ### Error output. Will post in the next comment on this thread. Seems I cannot drag-n-drop images into this block. ### Versions. <details>. I am including relevant package versions. Can provide more if needed. Python 3.12.7. ```. anndata==0.10.6. matplotlib==3.9.0. pandas==2.2.1. scanpy-1.10.3. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3320
https://github.com/scverse/scanpy/pull/3321:70,deployability,releas,release,70,"Enforce `np.bool_` usage via Ruff; Fix is in unreleased code, thus no release notes",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3321
https://github.com/scverse/scanpy/pull/3322:36,modifiability,Refact,Refactor,36,Backport PR #3316 on branch 1.10.x (Refactor regress_out); Backport PR #3316: Refactor regress_out,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3322
https://github.com/scverse/scanpy/pull/3322:78,modifiability,Refact,Refactor,78,Backport PR #3316 on branch 1.10.x (Refactor regress_out); Backport PR #3316: Refactor regress_out,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3322
https://github.com/scverse/scanpy/pull/3322:36,performance,Refactor,Refactor,36,Backport PR #3316 on branch 1.10.x (Refactor regress_out); Backport PR #3316: Refactor regress_out,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3322
https://github.com/scverse/scanpy/pull/3322:78,performance,Refactor,Refactor,78,Backport PR #3316 on branch 1.10.x (Refactor regress_out); Backport PR #3316: Refactor regress_out,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3322
https://github.com/scverse/scanpy/pull/3324:468,deployability,releas,release,468,Support `layer` in `sc.pl.highest_expr_genes`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3318. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3324
https://github.com/scverse/scanpy/pull/3324:493,deployability,Releas,Release,493,Support `layer` in `sc.pl.highest_expr_genes`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3318. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3324
https://github.com/scverse/scanpy/pull/3324:9,modifiability,layer,layer,9,Support `layer` in `sc.pl.highest_expr_genes`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3318. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3324
https://github.com/scverse/scanpy/pull/3324:266,safety,review,review,266,Support `layer` in `sc.pl.highest_expr_genes`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3318. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3324
https://github.com/scverse/scanpy/pull/3324:370,safety,Test,Tests,370,Support `layer` in `sc.pl.highest_expr_genes`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3318. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3324
https://github.com/scverse/scanpy/pull/3324:266,testability,review,review,266,Support `layer` in `sc.pl.highest_expr_genes`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3318. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3324
https://github.com/scverse/scanpy/pull/3324:370,testability,Test,Tests,370,Support `layer` in `sc.pl.highest_expr_genes`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3318. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3324
https://github.com/scverse/scanpy/pull/3324:0,usability,Support,Support,0,Support `layer` in `sc.pl.highest_expr_genes`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3318. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3324
https://github.com/scverse/scanpy/pull/3324:117,usability,guid,guidelines,117,Support `layer` in `sc.pl.highest_expr_genes`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3318. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3324
https://github.com/scverse/scanpy/pull/3324:148,usability,guid,guide,148,Support `layer` in `sc.pl.highest_expr_genes`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3318. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3324
https://github.com/scverse/scanpy/pull/3324:244,usability,workflow,workflow,244,Support `layer` in `sc.pl.highest_expr_genes`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3318. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3324
https://github.com/scverse/scanpy/pull/3324:350,usability,Close,Closes,350,Support `layer` in `sc.pl.highest_expr_genes`; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3318. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3324
https://github.com/scverse/scanpy/issues/3326:381,deployability,continu,continuous,381,"Feature Request: Option to Plot Highest Absolute Values on Top in sc.pl.umap; **Description**. Currently, sc.pl.umap provides an option to plot cells with higher values on top using an internal sorting step (np.argsort). However, for some use cases, it’s desirable to plot cells with the highest absolute values on top to highlight both large positive and large negative values in continuous color mappings. **Proposed Solution**. Add a new option, such as sort_order='abs', to plot the highest absolute values on top, while preserving the original behavior as the default. **Example**. Here’s how the option could be used in practice:. ```. sc.pl.umap(adata, color='example_feature', cmap='coolwarm', sort_order='abs'). ```. With this feature, cells with the largest magnitudes (either positive or negative) would be displayed on top, making them more visually prominent when using diverging color maps. **Suggested Implementation**. > [scanpy/source/scanpy/plotting/_tools/scatterplots.py - line 293-295](https://github.com/scverse/scanpy/blob/0cde4cf14c129f4ba3226e4b45c7794ce6f16ef3/src/scanpy/plotting/_tools/scatterplots.py#L293-L295). Instead of. ```. if sort_order and value_to_plot is not None and color_type == ""cont"":. # Higher values plotted on top, null values on bottom. order = np.argsort(-color_vector, kind=""stable"")[::-1]. ```. I suggest. ```. if sort_order and value_to_plot is not None and color_type == ""cont"":. if sort_order == ""abs"":. order = np.argsort(-np.abs(color_vector), kind=""stable"")[::-1]. else:. order = np.argsort(-color_vector, kind=""stable"")[::-1]. ```. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3326
https://github.com/scverse/scanpy/issues/3326:95,energy efficiency,Current,Currently,95,"Feature Request: Option to Plot Highest Absolute Values on Top in sc.pl.umap; **Description**. Currently, sc.pl.umap provides an option to plot cells with higher values on top using an internal sorting step (np.argsort). However, for some use cases, it’s desirable to plot cells with the highest absolute values on top to highlight both large positive and large negative values in continuous color mappings. **Proposed Solution**. Add a new option, such as sort_order='abs', to plot the highest absolute values on top, while preserving the original behavior as the default. **Example**. Here’s how the option could be used in practice:. ```. sc.pl.umap(adata, color='example_feature', cmap='coolwarm', sort_order='abs'). ```. With this feature, cells with the largest magnitudes (either positive or negative) would be displayed on top, making them more visually prominent when using diverging color maps. **Suggested Implementation**. > [scanpy/source/scanpy/plotting/_tools/scatterplots.py - line 293-295](https://github.com/scverse/scanpy/blob/0cde4cf14c129f4ba3226e4b45c7794ce6f16ef3/src/scanpy/plotting/_tools/scatterplots.py#L293-L295). Instead of. ```. if sort_order and value_to_plot is not None and color_type == ""cont"":. # Higher values plotted on top, null values on bottom. order = np.argsort(-color_vector, kind=""stable"")[::-1]. ```. I suggest. ```. if sort_order and value_to_plot is not None and color_type == ""cont"":. if sort_order == ""abs"":. order = np.argsort(-np.abs(color_vector), kind=""stable"")[::-1]. else:. order = np.argsort(-color_vector, kind=""stable"")[::-1]. ```. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3326
https://github.com/scverse/scanpy/issues/3326:691,energy efficiency,cool,coolwarm,691,"Feature Request: Option to Plot Highest Absolute Values on Top in sc.pl.umap; **Description**. Currently, sc.pl.umap provides an option to plot cells with higher values on top using an internal sorting step (np.argsort). However, for some use cases, it’s desirable to plot cells with the highest absolute values on top to highlight both large positive and large negative values in continuous color mappings. **Proposed Solution**. Add a new option, such as sort_order='abs', to plot the highest absolute values on top, while preserving the original behavior as the default. **Example**. Here’s how the option could be used in practice:. ```. sc.pl.umap(adata, color='example_feature', cmap='coolwarm', sort_order='abs'). ```. With this feature, cells with the largest magnitudes (either positive or negative) would be displayed on top, making them more visually prominent when using diverging color maps. **Suggested Implementation**. > [scanpy/source/scanpy/plotting/_tools/scatterplots.py - line 293-295](https://github.com/scverse/scanpy/blob/0cde4cf14c129f4ba3226e4b45c7794ce6f16ef3/src/scanpy/plotting/_tools/scatterplots.py#L293-L295). Instead of. ```. if sort_order and value_to_plot is not None and color_type == ""cont"":. # Higher values plotted on top, null values on bottom. order = np.argsort(-color_vector, kind=""stable"")[::-1]. ```. I suggest. ```. if sort_order and value_to_plot is not None and color_type == ""cont"":. if sort_order == ""abs"":. order = np.argsort(-np.abs(color_vector), kind=""stable"")[::-1]. else:. order = np.argsort(-color_vector, kind=""stable"")[::-1]. ```. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3326
https://github.com/scverse/scanpy/issues/3326:626,reliability,pra,practice,626,"Feature Request: Option to Plot Highest Absolute Values on Top in sc.pl.umap; **Description**. Currently, sc.pl.umap provides an option to plot cells with higher values on top using an internal sorting step (np.argsort). However, for some use cases, it’s desirable to plot cells with the highest absolute values on top to highlight both large positive and large negative values in continuous color mappings. **Proposed Solution**. Add a new option, such as sort_order='abs', to plot the highest absolute values on top, while preserving the original behavior as the default. **Example**. Here’s how the option could be used in practice:. ```. sc.pl.umap(adata, color='example_feature', cmap='coolwarm', sort_order='abs'). ```. With this feature, cells with the largest magnitudes (either positive or negative) would be displayed on top, making them more visually prominent when using diverging color maps. **Suggested Implementation**. > [scanpy/source/scanpy/plotting/_tools/scatterplots.py - line 293-295](https://github.com/scverse/scanpy/blob/0cde4cf14c129f4ba3226e4b45c7794ce6f16ef3/src/scanpy/plotting/_tools/scatterplots.py#L293-L295). Instead of. ```. if sort_order and value_to_plot is not None and color_type == ""cont"":. # Higher values plotted on top, null values on bottom. order = np.argsort(-color_vector, kind=""stable"")[::-1]. ```. I suggest. ```. if sort_order and value_to_plot is not None and color_type == ""cont"":. if sort_order == ""abs"":. order = np.argsort(-np.abs(color_vector), kind=""stable"")[::-1]. else:. order = np.argsort(-color_vector, kind=""stable"")[::-1]. ```. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3326
https://github.com/scverse/scanpy/issues/3326:549,usability,behavi,behavior,549,"Feature Request: Option to Plot Highest Absolute Values on Top in sc.pl.umap; **Description**. Currently, sc.pl.umap provides an option to plot cells with higher values on top using an internal sorting step (np.argsort). However, for some use cases, it’s desirable to plot cells with the highest absolute values on top to highlight both large positive and large negative values in continuous color mappings. **Proposed Solution**. Add a new option, such as sort_order='abs', to plot the highest absolute values on top, while preserving the original behavior as the default. **Example**. Here’s how the option could be used in practice:. ```. sc.pl.umap(adata, color='example_feature', cmap='coolwarm', sort_order='abs'). ```. With this feature, cells with the largest magnitudes (either positive or negative) would be displayed on top, making them more visually prominent when using diverging color maps. **Suggested Implementation**. > [scanpy/source/scanpy/plotting/_tools/scatterplots.py - line 293-295](https://github.com/scverse/scanpy/blob/0cde4cf14c129f4ba3226e4b45c7794ce6f16ef3/src/scanpy/plotting/_tools/scatterplots.py#L293-L295). Instead of. ```. if sort_order and value_to_plot is not None and color_type == ""cont"":. # Higher values plotted on top, null values on bottom. order = np.argsort(-color_vector, kind=""stable"")[::-1]. ```. I suggest. ```. if sort_order and value_to_plot is not None and color_type == ""cont"":. if sort_order == ""abs"":. order = np.argsort(-np.abs(color_vector), kind=""stable"")[::-1]. else:. order = np.argsort(-color_vector, kind=""stable"")[::-1]. ```. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3326
https://github.com/scverse/scanpy/issues/3326:853,usability,visual,visually,853,"Feature Request: Option to Plot Highest Absolute Values on Top in sc.pl.umap; **Description**. Currently, sc.pl.umap provides an option to plot cells with higher values on top using an internal sorting step (np.argsort). However, for some use cases, it’s desirable to plot cells with the highest absolute values on top to highlight both large positive and large negative values in continuous color mappings. **Proposed Solution**. Add a new option, such as sort_order='abs', to plot the highest absolute values on top, while preserving the original behavior as the default. **Example**. Here’s how the option could be used in practice:. ```. sc.pl.umap(adata, color='example_feature', cmap='coolwarm', sort_order='abs'). ```. With this feature, cells with the largest magnitudes (either positive or negative) would be displayed on top, making them more visually prominent when using diverging color maps. **Suggested Implementation**. > [scanpy/source/scanpy/plotting/_tools/scatterplots.py - line 293-295](https://github.com/scverse/scanpy/blob/0cde4cf14c129f4ba3226e4b45c7794ce6f16ef3/src/scanpy/plotting/_tools/scatterplots.py#L293-L295). Instead of. ```. if sort_order and value_to_plot is not None and color_type == ""cont"":. # Higher values plotted on top, null values on bottom. order = np.argsort(-color_vector, kind=""stable"")[::-1]. ```. I suggest. ```. if sort_order and value_to_plot is not None and color_type == ""cont"":. if sort_order == ""abs"":. order = np.argsort(-np.abs(color_vector), kind=""stable"")[::-1]. else:. order = np.argsort(-color_vector, kind=""stable"")[::-1]. ```. Thank you.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3326
https://github.com/scverse/scanpy/pull/3327:36,deployability,Updat,Update,36,Backport PR #3285 on branch 1.10.x (Update `test_rank_genes_groups.py` reference); Backport PR #3285: Update `test_rank_genes_groups.py` reference,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3327
https://github.com/scverse/scanpy/pull/3327:102,deployability,Updat,Update,102,Backport PR #3285 on branch 1.10.x (Update `test_rank_genes_groups.py` reference); Backport PR #3285: Update `test_rank_genes_groups.py` reference,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3327
https://github.com/scverse/scanpy/pull/3327:36,safety,Updat,Update,36,Backport PR #3285 on branch 1.10.x (Update `test_rank_genes_groups.py` reference); Backport PR #3285: Update `test_rank_genes_groups.py` reference,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3327
https://github.com/scverse/scanpy/pull/3327:102,safety,Updat,Update,102,Backport PR #3285 on branch 1.10.x (Update `test_rank_genes_groups.py` reference); Backport PR #3285: Update `test_rank_genes_groups.py` reference,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3327
https://github.com/scverse/scanpy/pull/3327:36,security,Updat,Update,36,Backport PR #3285 on branch 1.10.x (Update `test_rank_genes_groups.py` reference); Backport PR #3285: Update `test_rank_genes_groups.py` reference,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3327
https://github.com/scverse/scanpy/pull/3327:102,security,Updat,Update,102,Backport PR #3285 on branch 1.10.x (Update `test_rank_genes_groups.py` reference); Backport PR #3285: Update `test_rank_genes_groups.py` reference,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3327
https://github.com/scverse/scanpy/pull/3328:462,deployability,releas,release,462,Align `get.obs_df`’s docs with its code; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3310. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: real small change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3328
https://github.com/scverse/scanpy/pull/3328:487,deployability,Releas,Release,487,Align `get.obs_df`’s docs with its code; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3310. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: real small change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3328
https://github.com/scverse/scanpy/pull/3328:260,safety,review,review,260,Align `get.obs_df`’s docs with its code; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3310. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: real small change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3328
https://github.com/scverse/scanpy/pull/3328:364,safety,Test,Tests,364,Align `get.obs_df`’s docs with its code; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3310. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: real small change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3328
https://github.com/scverse/scanpy/pull/3328:260,testability,review,review,260,Align `get.obs_df`’s docs with its code; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3310. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: real small change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3328
https://github.com/scverse/scanpy/pull/3328:364,testability,Test,Tests,364,Align `get.obs_df`’s docs with its code; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3310. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: real small change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3328
https://github.com/scverse/scanpy/pull/3328:111,usability,guid,guidelines,111,Align `get.obs_df`’s docs with its code; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3310. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: real small change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3328
https://github.com/scverse/scanpy/pull/3328:142,usability,guid,guide,142,Align `get.obs_df`’s docs with its code; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3310. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: real small change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3328
https://github.com/scverse/scanpy/pull/3328:238,usability,workflow,workflow,238,Align `get.obs_df`’s docs with its code; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3310. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: real small change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3328
https://github.com/scverse/scanpy/pull/3328:344,usability,Close,Closes,344,Align `get.obs_df`’s docs with its code; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #3310. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: real small change.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3328
https://github.com/scverse/scanpy/pull/3330:129,deployability,Updat,Updated,129,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:629,deployability,releas,release,629,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:654,deployability,Releas,Release,654,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:16,integrability,filter,filtering,16,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:34,integrability,sub,submitting,34,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:68,integrability,filter,filtering,68,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:83,performance,Time,Time,83,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:129,safety,Updat,Updated,129,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:431,safety,review,review,431,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:531,safety,Test,Tests,531,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:129,security,Updat,Updated,129,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:431,testability,review,review,431,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:531,testability,Test,Tests,531,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:282,usability,guid,guidelines,282,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:313,usability,guid,guide,313,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:409,usability,workflow,workflow,409,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/pull/3330:515,usability,Close,Closes,515,~ 35.63% faster filtering; We are submitting PR for speed up of the filtering. | | Time |. | -- | -- |. | Original | 290.59 |. | Updated | 187.03 |. | Speedup | 35.63% |. Experiment was done on AWS r7i.24xlarge. <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [ ] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3330
https://github.com/scverse/scanpy/issues/3331:404,availability,error,error,404,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:484,availability,error,error,484,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:640,availability,error,error,640,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:816,availability,Error,Error,816,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:222,deployability,version,version,222,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:849,deployability,Version,Versions,849,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:2023,deployability,updat,updated,2023,"e sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. -----. Session information updated at 2024-10-30 08:33. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:222,integrability,version,version,222,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:849,integrability,Version,Versions,849,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:29,interoperability,compatib,compatible,29,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:1376,interoperability,platform,platformdirs,1376,"e sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. -----. Session information updated at 2024-10-30 08:33. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:222,modifiability,version,version,222,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:849,modifiability,Version,Versions,849,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:1065,modifiability,deco,decorator,1065,"e sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. -----. Session information updated at 2024-10-30 08:33. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:1314,modifiability,pac,packaging,1314,"e sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. -----. Session information updated at 2024-10-30 08:33. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:1878,modifiability,pac,packaged,1878,"e sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. -----. Session information updated at 2024-10-30 08:33. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:404,performance,error,error,404,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:484,performance,error,error,484,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:640,performance,error,error,640,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:816,performance,Error,Error,816,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:45,safety,input,inputs,45,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:404,safety,error,error,404,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:484,safety,error,error,484,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:640,safety,error,error,640,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:816,safety,Error,Error,816,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:2023,safety,updat,updated,2023,"e sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. -----. Session information updated at 2024-10-30 08:33. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:2003,security,Session,Session,2003,"e sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. -----. Session information updated at 2024-10-30 08:33. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:2023,security,updat,updated,2023,"e sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. -----. Session information updated at 2024-10-30 08:33. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:473,testability,trace,traced,473,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:45,usability,input,inputs,45,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:182,usability,confirm,confirmed,182,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:265,usability,confirm,confirmed,265,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:404,usability,error,error,404,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:484,usability,error,error,484,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:640,usability,error,error,640,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:693,usability,Minim,Minimal,693,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3331:816,usability,Error,Error,816,"ValueError: Output dtype not compatible with inputs.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? When we tried to compute QC metrics for our dataset we got this error (see title). Produced by the X.eliminate_zeros() call. We also traced the error back to genes that have only zeros in them, which would remove these columns and thus change the shape of the matrix. When we removed these genes the error vanished and the program ran successfully. ### Minimal code sample. sc.pp.calculate_qc_metrics(. raw_data, qc_vars=[""mt"", ""ribo"", ""hb""], inplace=True, log1p=True. ). ### Error output. _No response_. ### Versions. <details>. ```. -----. anndata 0.10.9. scanpy 1.10.3. -----. PIL 10.4.0. asttokens NA. charset_normalizer 3.4.0. colorama 0.4.6. comm 0.2.2. cycler 0.12.1. cython_runtime NA. dateutil 2.9.0. debugpy 1.8.5. decorator 5.1.1. executing 2.1.0. h5py 3.11.0. ipykernel 6.29.5. jedi 0.19.1. joblib 1.4.2. kiwisolver 1.4.7. legacy_api_wrap NA. llvmlite 0.43.0. matplotlib 3.9.2. matplotlib_inline 0.1.7. mpl_toolkits NA. natsort 8.4.0. numba 0.60.0. numpy 2.0.2. packaging 24.1. pandas 2.2.3. parso 0.8.4. pickleshare 0.7.5. platformdirs 4.3.6. prompt_toolkit 3.0.47. psutil 6.0.0. pure_eval 0.2.3. pydev_ipython NA. pydevconsole NA. pydevd 2.9.5. pydevd_file_utils NA. pydevd_plugins NA. pydevd_tracing NA. pygments 2.18.0. pyparsing 3.1.4. pytz 2024.1. scipy 1.14.1. session_info 1.0.0. six 1.16.0. sklearn 1.5.2. stack_data 0.6.2. threadpoolctl 3.5.0. tornado 6.4.1. traitlets 5.14.3. vscode NA. wcwidth 0.2.13. yaml 6.0.2. zmq 26.2.0. -----. IPython 8.27.0. jupyter_client 8.6.3. jupyter_core 5.7.2. -----. Python 3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:16:49) [GCC 13.3.0]. Linux-5.15.0-122-generic-x86_64-with-glibc2.35. ----",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3331
https://github.com/scverse/scanpy/issues/3332:353,deployability,Depend,Depending,353,"Documentation typo for scanpy.pp.filter_cells; The function documentation for [`filter_cells`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_cells.html#scanpy.pp.filter_cells) has the following under ""Returns"":. > **number_per_cell** : [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray). > Depending on what was thresholded (counts or genes), the array stores `n_counts` or `n_cells` per gene. I have a feeling this is copied from [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) and should read . > the array stores `n_counts` or `n_cells` per **_cell_**. If not, perhaps more explanation is needed as to why this output is identical to the [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) function.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3332
https://github.com/scverse/scanpy/issues/3332:353,integrability,Depend,Depending,353,"Documentation typo for scanpy.pp.filter_cells; The function documentation for [`filter_cells`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_cells.html#scanpy.pp.filter_cells) has the following under ""Returns"":. > **number_per_cell** : [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray). > Depending on what was thresholded (counts or genes), the array stores `n_counts` or `n_cells` per gene. I have a feeling this is copied from [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) and should read . > the array stores `n_counts` or `n_cells` per **_cell_**. If not, perhaps more explanation is needed as to why this output is identical to the [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) function.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3332
https://github.com/scverse/scanpy/issues/3332:353,modifiability,Depend,Depending,353,"Documentation typo for scanpy.pp.filter_cells; The function documentation for [`filter_cells`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_cells.html#scanpy.pp.filter_cells) has the following under ""Returns"":. > **number_per_cell** : [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray). > Depending on what was thresholded (counts or genes), the array stores `n_counts` or `n_cells` per gene. I have a feeling this is copied from [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) and should read . > the array stores `n_counts` or `n_cells` per **_cell_**. If not, perhaps more explanation is needed as to why this output is identical to the [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) function.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3332
https://github.com/scverse/scanpy/issues/3332:353,safety,Depend,Depending,353,"Documentation typo for scanpy.pp.filter_cells; The function documentation for [`filter_cells`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_cells.html#scanpy.pp.filter_cells) has the following under ""Returns"":. > **number_per_cell** : [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray). > Depending on what was thresholded (counts or genes), the array stores `n_counts` or `n_cells` per gene. I have a feeling this is copied from [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) and should read . > the array stores `n_counts` or `n_cells` per **_cell_**. If not, perhaps more explanation is needed as to why this output is identical to the [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) function.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3332
https://github.com/scverse/scanpy/issues/3332:756,security,ident,identical,756,"Documentation typo for scanpy.pp.filter_cells; The function documentation for [`filter_cells`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_cells.html#scanpy.pp.filter_cells) has the following under ""Returns"":. > **number_per_cell** : [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray). > Depending on what was thresholded (counts or genes), the array stores `n_counts` or `n_cells` per gene. I have a feeling this is copied from [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) and should read . > the array stores `n_counts` or `n_cells` per **_cell_**. If not, perhaps more explanation is needed as to why this output is identical to the [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) function.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3332
https://github.com/scverse/scanpy/issues/3332:353,testability,Depend,Depending,353,"Documentation typo for scanpy.pp.filter_cells; The function documentation for [`filter_cells`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_cells.html#scanpy.pp.filter_cells) has the following under ""Returns"":. > **number_per_cell** : [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray). > Depending on what was thresholded (counts or genes), the array stores `n_counts` or `n_cells` per gene. I have a feeling this is copied from [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) and should read . > the array stores `n_counts` or `n_cells` per **_cell_**. If not, perhaps more explanation is needed as to why this output is identical to the [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) function.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3332
https://github.com/scverse/scanpy/issues/3332:0,usability,Document,Documentation,0,"Documentation typo for scanpy.pp.filter_cells; The function documentation for [`filter_cells`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_cells.html#scanpy.pp.filter_cells) has the following under ""Returns"":. > **number_per_cell** : [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray). > Depending on what was thresholded (counts or genes), the array stores `n_counts` or `n_cells` per gene. I have a feeling this is copied from [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) and should read . > the array stores `n_counts` or `n_cells` per **_cell_**. If not, perhaps more explanation is needed as to why this output is identical to the [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) function.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3332
https://github.com/scverse/scanpy/issues/3332:60,usability,document,documentation,60,"Documentation typo for scanpy.pp.filter_cells; The function documentation for [`filter_cells`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_cells.html#scanpy.pp.filter_cells) has the following under ""Returns"":. > **number_per_cell** : [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray). > Depending on what was thresholded (counts or genes), the array stores `n_counts` or `n_cells` per gene. I have a feeling this is copied from [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) and should read . > the array stores `n_counts` or `n_cells` per **_cell_**. If not, perhaps more explanation is needed as to why this output is identical to the [filter_genes](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.filter_genes.html#scanpy.pp.filter_genes) function.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3332
https://github.com/scverse/scanpy/pull/3333:344,deployability,roll,rolling,344,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:702,deployability,releas,release,702,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:727,deployability,Releas,Release,727,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:16,safety,test,test,16,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:246,safety,review,review,246,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:604,safety,Test,Tests,604,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:781,safety,test,tests,781,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:318,security,control,control,318,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:16,testability,test,test,16,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:246,testability,review,review,246,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:318,testability,control,control,318,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:604,testability,Test,Tests,604,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:781,testability,test,tests,781,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:97,usability,guid,guidelines,97,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:128,usability,guid,guide,128,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:224,usability,workflow,workflow,224,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3333:588,usability,Close,Closes,588,"(fix): sort pca test args; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. Since `gen_pca_params` is a generator, not sure how we can control the order besides rolling back the use of sets. So I landed on just sorting the args based on `id` of the pytest. . I was running in to https://github.com/pytest-dev/pytest-xdist/issues/432. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [ ] Closes #. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [x] Release notes not necessary because: just fixing some tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3333
https://github.com/scverse/scanpy/pull/3335:436,interoperability,specif,specific,436,"move all `njit` calls into a decorator; @ilan-gold said. > I found https://github.com/numba/numba/issues/9288 which would suggest that the issue really is threading (they claim it from `guvectorize` but it would seem we are seeing the same thing). It seems like there are a few things for us to try from there, especially if you really saw that using dask in processing mode is not the fix (strange but ok). The issue appears to be mac specific as well, so a bit less critical. See: https://github.com/numbagg/numbagg/pull/201. I can try some of these out",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335
https://github.com/scverse/scanpy/pull/3335:29,modifiability,deco,decorator,29,"move all `njit` calls into a decorator; @ilan-gold said. > I found https://github.com/numba/numba/issues/9288 which would suggest that the issue really is threading (they claim it from `guvectorize` but it would seem we are seeing the same thing). It seems like there are a few things for us to try from there, especially if you really saw that using dask in processing mode is not the fix (strange but ok). The issue appears to be mac specific as well, so a bit less critical. See: https://github.com/numbagg/numbagg/pull/201. I can try some of these out",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3335
https://github.com/scverse/scanpy/pull/3337:204,availability,reboot,reboots,204,"Actually working min-deps job; Hia everyone, this should actually make`hatch test -i deps=min` work even on Macbooks. Maybe there’s a way to put the constraint file inside of the venv, then it’ll survive reboots on Linux. I tagged you all because I thought you might be interested in this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3337
https://github.com/scverse/scanpy/pull/3337:77,safety,test,test,77,"Actually working min-deps job; Hia everyone, this should actually make`hatch test -i deps=min` work even on Macbooks. Maybe there’s a way to put the constraint file inside of the venv, then it’ll survive reboots on Linux. I tagged you all because I thought you might be interested in this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3337
https://github.com/scverse/scanpy/pull/3337:77,testability,test,test,77,"Actually working min-deps job; Hia everyone, this should actually make`hatch test -i deps=min` work even on Macbooks. Maybe there’s a way to put the constraint file inside of the venv, then it’ll survive reboots on Linux. I tagged you all because I thought you might be interested in this.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3337
https://github.com/scverse/scanpy/pull/3340:450,deployability,releas,release,450,(feat): seurat v3 with dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2808. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3340
https://github.com/scverse/scanpy/pull/3340:475,deployability,Releas,Release,475,(feat): seurat v3 with dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2808. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3340
https://github.com/scverse/scanpy/pull/3340:248,safety,review,review,248,(feat): seurat v3 with dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2808. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3340
https://github.com/scverse/scanpy/pull/3340:352,safety,Test,Tests,352,(feat): seurat v3 with dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2808. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3340
https://github.com/scverse/scanpy/pull/3340:248,testability,review,review,248,(feat): seurat v3 with dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2808. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3340
https://github.com/scverse/scanpy/pull/3340:352,testability,Test,Tests,352,(feat): seurat v3 with dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2808. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3340
https://github.com/scverse/scanpy/pull/3340:99,usability,guid,guidelines,99,(feat): seurat v3 with dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2808. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3340
https://github.com/scverse/scanpy/pull/3340:130,usability,guid,guide,130,(feat): seurat v3 with dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2808. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3340
https://github.com/scverse/scanpy/pull/3340:226,usability,workflow,workflow,226,(feat): seurat v3 with dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2808. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3340
https://github.com/scverse/scanpy/pull/3340:332,usability,Close,Closes,332,(feat): seurat v3 with dask; <!--. Thanks for opening a PR to scanpy! Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review. -->. <!-- Please check (“- [x]”) and fill in the following boxes -->. - [x] Closes #2808. - [x] Tests included or not required because:. <!-- Only check the following box if you did not include release notes -->. - [ ] Release notes not necessary because:.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/3340
https://github.com/scverse/scanpy/issues/3341:439,availability,error,error,439,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:599,availability,Error,Error,599,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:233,deployability,version,version,233,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:852,deployability,Version,Versions,852,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:233,integrability,version,version,233,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:852,integrability,Version,Versions,852,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:32,interoperability,format,format,32,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:417,interoperability,format,format,417,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:649,interoperability,specif,specified,649,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:713,interoperability,specif,specifying,713,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:233,modifiability,version,version,233,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:852,modifiability,Version,Versions,852,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:439,performance,error,error,439,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:599,performance,Error,Error,599,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:828,reliability,doe,does,828,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:439,safety,error,error,439,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:599,safety,Error,Error,599,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:193,usability,confirm,confirmed,193,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:276,usability,confirm,confirmed,276,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:439,usability,error,error,439,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:463,usability,Minim,Minimal,463,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3341:599,usability,Error,Error,599,"Issues with reading in non zarr format Visium HD data in python; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? Hi. I am trying to read in my Visium HD file which is in non-zarr format. . Here is the error that I get. . ### Minimal code sample. from spatialdata_io import visium_hd. import spatialdata as sd. path_read ='...'. sdata= visium_hd(path_read). ### Error output. Its says the dataset_id needs to be specified, but there isn't a dataset_id in my folder. . I tried specifying dataset_id=""None"" or just blank or some possible names from my parent folder that could be it. . But it does not read it. . ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3341
https://github.com/scverse/scanpy/issues/3342:59,availability,error,error,59,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:696,availability,error,error,696,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:839,availability,Down,Downloads,839,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:907,availability,Error,Error,907,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:2003,availability,sli,slice,2003,"g exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the feature slice file in {path}, please pass `dataset_id` as an argument."". 363 ). 364 return files[0].replace(suffix, """"). ValueError: Cannot infer `dataset_id` from the feature slice file in /Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs, please pass `dataset_id` as an argument. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:2171,availability,sli,slice,2171,"g exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the feature slice file in {path}, please pass `dataset_id` as an argument."". 363 ). 364 return files[0].replace(suffix, """"). ValueError: Cannot infer `dataset_id` from the feature slice file in /Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs, please pass `dataset_id` as an argument. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:2201,availability,Down,Downloads,2201,"g exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the feature slice file in {path}, please pass `dataset_id` as an argument."". 363 ). 364 return files[0].replace(suffix, """"). ValueError: Cannot infer `dataset_id` from the feature slice file in /Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs, please pass `dataset_id` as an argument. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:261,deployability,version,version,261,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:2279,deployability,Version,Versions,2279,"g exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the feature slice file in {path}, please pass `dataset_id` as an argument."". 363 ). 364 return files[0].replace(suffix, """"). ValueError: Cannot infer `dataset_id` from the feature slice file in /Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs, please pass `dataset_id` as an argument. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:261,integrability,version,version,261,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:2279,integrability,Version,Versions,2279,"g exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the feature slice file in {path}, please pass `dataset_id` as an argument."". 363 ). 364 return files[0].replace(suffix, """"). ValueError: Cannot infer `dataset_id` from the feature slice file in /Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs, please pass `dataset_id` as an argument. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:84,interoperability,format,format,84,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:395,interoperability,format,format,395,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:726,interoperability,specif,specifying,726,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:261,modifiability,version,version,261,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:1173,modifiability,pac,packages,1173,"has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the feature slice file in {path}, please pass `dataset_id` as an argument."". 363 ). 364 return files[0].replace(suffix, """"). ValueError: Cannot infer `dataset_id` from the feature slice ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:1699,modifiability,pac,packages,1699,"g exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the feature slice file in {path}, please pass `dataset_id` as an argument."". 363 ). 364 return files[0].replace(suffix, """"). ValueError: Cannot infer `dataset_id` from the feature slice file in /Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs, please pass `dataset_id` as an argument. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:2279,modifiability,Version,Versions,2279,"g exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the feature slice file in {path}, please pass `dataset_id` as an argument."". 363 ). 364 return files[0].replace(suffix, """"). ValueError: Cannot infer `dataset_id` from the feature slice file in /Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs, please pass `dataset_id` as an argument. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:59,performance,error,error,59,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:696,performance,error,error,696,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:907,performance,Error,Error,907,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:2003,reliability,sli,slice,2003,"g exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the feature slice file in {path}, please pass `dataset_id` as an argument."". 363 ). 364 return files[0].replace(suffix, """"). ValueError: Cannot infer `dataset_id` from the feature slice file in /Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs, please pass `dataset_id` as an argument. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:2171,reliability,sli,slice,2171,"g exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the feature slice file in {path}, please pass `dataset_id` as an argument."". 363 ). 364 return files[0].replace(suffix, """"). ValueError: Cannot infer `dataset_id` from the feature slice file in /Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs, please pass `dataset_id` as an argument. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:59,safety,error,error,59,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:696,safety,error,error,696,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:907,safety,Error,Error,907,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:1009,testability,Trace,Traceback,1009,"sium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the feature slice file ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:59,usability,error,error,59,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:221,usability,confirm,confirmed,221,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:304,usability,confirm,confirmed,304,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:681,usability,support,support,681,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:696,usability,error,error,696,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:789,usability,Minim,Minimal,789,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:824,usability,User,Users,824,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:907,usability,Error,Error,907,"Cannot read Visium HD data using spatialdata-io (Recurrent error). Data is non-zarr format.; ### Please make sure these conditions are met. - [x] I have checked that this issue has not already been reported. - [x] I have confirmed this bug exists on the latest version of scanpy. - [ ] (optional) I have confirmed this bug exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the featu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
https://github.com/scverse/scanpy/issues/3342:2186,usability,User,Users,2186,"g exists on the main branch of scanpy. ### What happened? I have non zarr format Visium HD data. . I tried reading it with sdata = visium_hd(path_read). it keeps asking me for a dataset_id which is not there in the feature_slice file name or my folder. . Nonetheless, I kept setting it to None or """" or other possible dataset id values. I cannot find any tech support on the error either. . (I also tried specifying the file path to the different binned folders). ### Minimal code sample. path_read = '/Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs'. sdata = visium_hd(path_read). ### Error output. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). Cell In[54], line 1. ----> 1 sdata = visium_hd(path_read). File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:95, in visium_hd(path, dataset_id, filtered_counts_file, bin_size, bins_as_squares, fullres_image_file, load_all_images, imread_kwargs, image_models_kwargs, anndata_kwargs). 92 images: dict[str, Any] = {}. 94 if dataset_id is None:. ---> 95 dataset_id = _infer_dataset_id(path). 96 filename_prefix = f""{dataset_id}_"". 98 def load_image(path: Path, suffix: str, scale_factors: list[int] | None = None) -> None:. File /Volumes/Ankitha/Conda/miniconda3/envs/myenv/lib/python3.12/site-packages/spatialdata_io/readers/visium_hd.py:361, in _infer_dataset_id(path). 359 files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f.endswith(suffix)]. 360 if len(files) == 0 or len(files) > 1:. --> 361 raise ValueError(. 362 f""Cannot infer `dataset_id` from the feature slice file in {path}, please pass `dataset_id` as an argument."". 363 ). 364 return files[0].replace(suffix, """"). ValueError: Cannot infer `dataset_id` from the feature slice file in /Users/DarthRNA/Downloads/1299_1_XS_VHD_v2_outs, please pass `dataset_id` as an argument. ### Versions. <details>. ```. ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/3342
