id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/786:386,usability,error,errored,386,"unable to install louvain on Windows; Hi all,. I'm having a trouble in running a code: . sc.tl.louvain(adata). So, when I try to run the code, it has an error saying that. ERROR: Failed building wheel for louvain. I tried to install louvain in anaconda prompt, and I can't install it. When I use:. pip install louvain. to install louvain, I have an error that . ```pytb. ERROR: Command errored out with exit status 1:. 'c:\users\prince and jacky\anaconda3\python.exe' \. -u \. -c '. import sys, setuptools, tokenize. sys.argv[0] = "".../louvain/setup.py"". __file__="".../louvain/setup.py"". f=getattr(tokenize, ""open"", open)(__file__). code=f.read().replace(""\r\n"", ""\n""). f.close(). exec(compile(code, __file__, ""exec"")). ' \. install \. --record '.../install-record.txt' \. --single-version-externally-managed \. --compile. Check the logs for full command output. ```. I also tried to install using different codes such as:. ```bash. conda install -c conda-forge louvain. ```. There's an error saying that:. PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:408,usability,statu,status,408,"unable to install louvain on Windows; Hi all,. I'm having a trouble in running a code: . sc.tl.louvain(adata). So, when I try to run the code, it has an error saying that. ERROR: Failed building wheel for louvain. I tried to install louvain in anaconda prompt, and I can't install it. When I use:. pip install louvain. to install louvain, I have an error that . ```pytb. ERROR: Command errored out with exit status 1:. 'c:\users\prince and jacky\anaconda3\python.exe' \. -u \. -c '. import sys, setuptools, tokenize. sys.argv[0] = "".../louvain/setup.py"". __file__="".../louvain/setup.py"". f=getattr(tokenize, ""open"", open)(__file__). code=f.read().replace(""\r\n"", ""\n""). f.close(). exec(compile(code, __file__, ""exec"")). ' \. install \. --record '.../install-record.txt' \. --single-version-externally-managed \. --compile. Check the logs for full command output. ```. I also tried to install using different codes such as:. ```bash. conda install -c conda-forge louvain. ```. There's an error saying that:. PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:423,usability,user,users,423,"unable to install louvain on Windows; Hi all,. I'm having a trouble in running a code: . sc.tl.louvain(adata). So, when I try to run the code, it has an error saying that. ERROR: Failed building wheel for louvain. I tried to install louvain in anaconda prompt, and I can't install it. When I use:. pip install louvain. to install louvain, I have an error that . ```pytb. ERROR: Command errored out with exit status 1:. 'c:\users\prince and jacky\anaconda3\python.exe' \. -u \. -c '. import sys, setuptools, tokenize. sys.argv[0] = "".../louvain/setup.py"". __file__="".../louvain/setup.py"". f=getattr(tokenize, ""open"", open)(__file__). code=f.read().replace(""\r\n"", ""\n""). f.close(). exec(compile(code, __file__, ""exec"")). ' \. install \. --record '.../install-record.txt' \. --single-version-externally-managed \. --compile. Check the logs for full command output. ```. I also tried to install using different codes such as:. ```bash. conda install -c conda-forge louvain. ```. There's an error saying that:. PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:672,usability,close,close,672,"unable to install louvain on Windows; Hi all,. I'm having a trouble in running a code: . sc.tl.louvain(adata). So, when I try to run the code, it has an error saying that. ERROR: Failed building wheel for louvain. I tried to install louvain in anaconda prompt, and I can't install it. When I use:. pip install louvain. to install louvain, I have an error that . ```pytb. ERROR: Command errored out with exit status 1:. 'c:\users\prince and jacky\anaconda3\python.exe' \. -u \. -c '. import sys, setuptools, tokenize. sys.argv[0] = "".../louvain/setup.py"". __file__="".../louvain/setup.py"". f=getattr(tokenize, ""open"", open)(__file__). code=f.read().replace(""\r\n"", ""\n""). f.close(). exec(compile(code, __file__, ""exec"")). ' \. install \. --record '.../install-record.txt' \. --single-version-externally-managed \. --compile. Check the logs for full command output. ```. I also tried to install using different codes such as:. ```bash. conda install -c conda-forge louvain. ```. There's an error saying that:. PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:847,usability,command,command,847,"unable to install louvain on Windows; Hi all,. I'm having a trouble in running a code: . sc.tl.louvain(adata). So, when I try to run the code, it has an error saying that. ERROR: Failed building wheel for louvain. I tried to install louvain in anaconda prompt, and I can't install it. When I use:. pip install louvain. to install louvain, I have an error that . ```pytb. ERROR: Command errored out with exit status 1:. 'c:\users\prince and jacky\anaconda3\python.exe' \. -u \. -c '. import sys, setuptools, tokenize. sys.argv[0] = "".../louvain/setup.py"". __file__="".../louvain/setup.py"". f=getattr(tokenize, ""open"", open)(__file__). code=f.read().replace(""\r\n"", ""\n""). f.close(). exec(compile(code, __file__, ""exec"")). ' \. install \. --record '.../install-record.txt' \. --single-version-externally-managed \. --compile. Check the logs for full command output. ```. I also tried to install using different codes such as:. ```bash. conda install -c conda-forge louvain. ```. There's an error saying that:. PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:987,usability,error,error,987,"unable to install louvain on Windows; Hi all,. I'm having a trouble in running a code: . sc.tl.louvain(adata). So, when I try to run the code, it has an error saying that. ERROR: Failed building wheel for louvain. I tried to install louvain in anaconda prompt, and I can't install it. When I use:. pip install louvain. to install louvain, I have an error that . ```pytb. ERROR: Command errored out with exit status 1:. 'c:\users\prince and jacky\anaconda3\python.exe' \. -u \. -c '. import sys, setuptools, tokenize. sys.argv[0] = "".../louvain/setup.py"". __file__="".../louvain/setup.py"". f=getattr(tokenize, ""open"", open)(__file__). code=f.read().replace(""\r\n"", ""\n""). f.close(). exec(compile(code, __file__, ""exec"")). ' \. install \. --record '.../install-record.txt' \. --single-version-externally-managed \. --compile. Check the logs for full command output. ```. I also tried to install using different codes such as:. ```bash. conda install -c conda-forge louvain. ```. There's an error saying that:. PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/786:1117,usability,help,help,1117,"unable to install louvain on Windows; Hi all,. I'm having a trouble in running a code: . sc.tl.louvain(adata). So, when I try to run the code, it has an error saying that. ERROR: Failed building wheel for louvain. I tried to install louvain in anaconda prompt, and I can't install it. When I use:. pip install louvain. to install louvain, I have an error that . ```pytb. ERROR: Command errored out with exit status 1:. 'c:\users\prince and jacky\anaconda3\python.exe' \. -u \. -c '. import sys, setuptools, tokenize. sys.argv[0] = "".../louvain/setup.py"". __file__="".../louvain/setup.py"". f=getattr(tokenize, ""open"", open)(__file__). code=f.read().replace(""\r\n"", ""\n""). f.close(). exec(compile(code, __file__, ""exec"")). ' \. install \. --record '.../install-record.txt' \. --single-version-externally-managed \. --compile. Check the logs for full command output. ```. I also tried to install using different codes such as:. ```bash. conda install -c conda-forge louvain. ```. There's an error saying that:. PackagesNotFoundError: The following packages are not available from current channels:. - louvain. Can anyone help me with solving this issue? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/786
https://github.com/scverse/scanpy/issues/787:2094,deployability,updat,update,2094,"ytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2523,deployability,patch,patch,2523,"box_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/si",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2936,deployability,Compos,Composite,2936,"6 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:3480,deployability,patch,patch,3480,"e.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)). 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]). 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok). 180 [1, 2, 3]]). 181 """""".",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:432,energy efficiency,core,core,432,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:672,energy efficiency,core,core,672,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:1006,energy efficiency,core,core,1006,"or comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with Rendere",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:1742,energy efficiency,draw,draw,1742," 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:1927,energy efficiency,draw,draw,1927,"tina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2040,energy efficiency,draw,draw,2040,"ig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2121,energy efficiency,draw,draw,2121,"as.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . --->",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2332,energy efficiency,draw,draw,2332,"orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorte",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2491,energy efficiency,draw,draw,2491,"2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enum",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2529,energy efficiency,draw,draw,2529,"xtra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-pa",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2903,energy efficiency,draw,draw,2903,"nds/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:3134,energy efficiency,draw,draw,3134," don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, z",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:3303,energy efficiency,draw,draw,3303,"ter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)). 848 return np.col",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:110,integrability,compon,components,110,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:186,integrability,compon,components,186,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:4450,integrability,sub,subok,4450,"apper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)). 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]). 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok). 180 [1, 2, 3]]). 181 """""". --> 182 return _broadcast_to(array, shape, subok=subok, readonly=True). 183 . 184 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in _broadcast_to(array, shape, subok, readonly). 127 it = np.nditer(. 128 (array,), flags=['multi_index', 'refs_ok', 'zerosize_ok'] + extras,. --> 129 op_flags=[op_flag], itershape=shape, order='C'). 130 with it:. 131 # never really has writebackifcopy semantics. ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (11272,4). ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:4527,integrability,sub,subok,4527,"apper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)). 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]). 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok). 180 [1, 2, 3]]). 181 """""". --> 182 return _broadcast_to(array, shape, subok=subok, readonly=True). 183 . 184 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in _broadcast_to(array, shape, subok, readonly). 127 it = np.nditer(. 128 (array,), flags=['multi_index', 'refs_ok', 'zerosize_ok'] + extras,. --> 129 op_flags=[op_flag], itershape=shape, order='C'). 130 with it:. 131 # never really has writebackifcopy semantics. ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (11272,4). ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:4533,integrability,sub,subok,4533,"apper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)). 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]). 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok). 180 [1, 2, 3]]). 181 """""". --> 182 return _broadcast_to(array, shape, subok=subok, readonly=True). 183 . 184 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in _broadcast_to(array, shape, subok, readonly). 127 it = np.nditer(. 128 (array,), flags=['multi_index', 'refs_ok', 'zerosize_ok'] + extras,. --> 129 op_flags=[op_flag], itershape=shape, order='C'). 130 with it:. 131 # never really has writebackifcopy semantics. ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (11272,4). ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:4663,integrability,sub,subok,4663,"apper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)). 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]). 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok). 180 [1, 2, 3]]). 181 """""". --> 182 return _broadcast_to(array, shape, subok=subok, readonly=True). 183 . 184 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in _broadcast_to(array, shape, subok, readonly). 127 it = np.nditer(. 128 (array,), flags=['multi_index', 'refs_ok', 'zerosize_ok'] + extras,. --> 129 op_flags=[op_flag], itershape=shape, order='C'). 130 with it:. 131 # never really has writebackifcopy semantics. ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (11272,4). ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:110,interoperability,compon,components,110,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:186,interoperability,compon,components,186,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:437,interoperability,format,formatters,437,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:731,interoperability,format,formats,731,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:848,interoperability,format,formats,848,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:870,interoperability,format,formats,870,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:1347,interoperability,format,format,1347,"raceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, rende",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:4885,interoperability,semant,semantics,4885,"apper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)). 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]). 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok). 180 [1, 2, 3]]). 181 """""". --> 182 return _broadcast_to(array, shape, subok=subok, readonly=True). 183 . 184 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in _broadcast_to(array, shape, subok, readonly). 127 it = np.nditer(. 128 (array,), flags=['multi_index', 'refs_ok', 'zerosize_ok'] + extras,. --> 129 op_flags=[op_flag], itershape=shape, order='C'). 130 with it:. 131 # never really has writebackifcopy semantics. ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (11272,4). ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:110,modifiability,compon,components,110,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:186,modifiability,compon,components,186,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:415,modifiability,pac,packages,415,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:655,modifiability,pac,packages,655,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:989,modifiability,pac,packages,989,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:1238,modifiability,pac,packages,1238,"follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:1581,modifiability,pac,packages,1581,"t_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_image",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:1880,modifiability,pac,packages,1880,"png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2202,modifiability,pac,packages,2202,"':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2458,modifiability,pac,packages,2458,"= self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offse",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2719,modifiability,pac,packages,2719,"27 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2936,modifiability,Compos,Composite,2936,"6 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:3004,modifiability,pac,packages,3004,"rAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:3260,modifiability,pac,packages,3260,", *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:3529,modifiability,pac,packages,3529,"(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)). 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]). 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok). 180 [1, 2, 3]]). 181 """""". --> 182 return _broadcast_to(array, shape, subok=",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:3789,modifiability,pac,packages,3789,"rent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)). 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]). 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok). 180 [1, 2, 3]]). 181 """""". --> 182 return _broadcast_to(array, shape, subok=subok, readonly=True). 183 . 184 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in _broadcast_to(array, shape, subok, readonly). 127 it = np.nditer(. 128 (array,), flags=['multi_index', 'refs_ok', 'zerosize_ok'] + extras,. --> 129 op_flags=[",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:4077,modifiability,pac,packages,4077,"apper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)). 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]). 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok). 180 [1, 2, 3]]). 181 """""". --> 182 return _broadcast_to(array, shape, subok=subok, readonly=True). 183 . 184 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in _broadcast_to(array, shape, subok, readonly). 127 it = np.nditer(. 128 (array,), flags=['multi_index', 'refs_ok', 'zerosize_ok'] + extras,. --> 129 op_flags=[op_flag], itershape=shape, order='C'). 130 with it:. 131 # never really has writebackifcopy semantics. ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (11272,4). ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:4384,modifiability,pac,packages,4384,"apper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)). 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]). 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok). 180 [1, 2, 3]]). 181 """""". --> 182 return _broadcast_to(array, shape, subok=subok, readonly=True). 183 . 184 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in _broadcast_to(array, shape, subok, readonly). 127 it = np.nditer(. 128 (array,), flags=['multi_index', 'refs_ok', 'zerosize_ok'] + extras,. --> 129 op_flags=[op_flag], itershape=shape, order='C'). 130 with it:. 131 # never really has writebackifcopy semantics. ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (11272,4). ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:4596,modifiability,pac,packages,4596,"apper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)). 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]). 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok). 180 [1, 2, 3]]). 181 """""". --> 182 return _broadcast_to(array, shape, subok=subok, readonly=True). 183 . 184 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in _broadcast_to(array, shape, subok, readonly). 127 it = np.nditer(. 128 (array,), flags=['multi_index', 'refs_ok', 'zerosize_ok'] + extras,. --> 129 op_flags=[op_flag], itershape=shape, order='C'). 130 with it:. 131 # never really has writebackifcopy semantics. ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (11272,4). ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:88,performance,time,time,88,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2013,performance,lock,lock,2013,"btools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/ma",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2094,safety,updat,update,2094,"ytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2523,safety,patch,patch,2523,"box_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/si",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:3480,safety,patch,patch,3480,"e.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)). 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]). 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok). 180 [1, 2, 3]]). 181 """""".",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2013,security,lock,lock,2013,"btools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/ma",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2094,security,updat,update,2094,"ytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:2523,security,patch,patch,2523,"box_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/si",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:3480,security,patch,patch,3480,"e.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in draw(self, renderer). 290 sorted(self.collections,. 291 key=lambda col: col.do_3d_projection(renderer),. --> 292 reverse=True)):. 293 col.zorder = zorder_offset + i. 294 for i, patch in enumerate(. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/axes3d.py in <lambda>(col). 289 for i, col in enumerate(. 290 sorted(self.collections,. --> 291 key=lambda col: col.do_3d_projection(renderer),. 292 reverse=True)):. 293 col.zorder = zorder_offset + i. ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in do_3d_projection(self, renderer). 543 self.set_facecolors(fcs). 544 . --> 545 ecs = (_zalpha(self._edgecolor3d, vzs) if self._depthshade else. 546 self._edgecolor3d). 547 ecs = mcolors.to_rgba_array(ecs, self._alpha). ~/.local/lib/python3.7/site-packages/mpl_toolkits/mplot3d/art3d.py in _zalpha(colors, zs). 845 norm = Normalize(min(zs), max(zs)). 846 sats = 1 - norm(zs) * 0.7. --> 847 rgba = np.broadcast_to(mcolors.to_rgba_array(colors), (len(zs), 4)). 848 return np.column_stack([rgba[:, :3], rgba[:, 3] * sats]). 849 . ~/.local/lib/python3.7/site-packages/numpy/lib/stride_tricks.py in broadcast_to(array, shape, subok). 180 [1, 2, 3]]). 181 """""".",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:308,testability,Trace,Traceback,308,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:349,testability,Trace,Traceback,349,"ValueError comes out when plotting 3D UMAP projection; I tried `sc.pl.umap(adata,color='time',projection='3d',components=['1,2,3','1,2,4'])` to plot 3D UMAP. No matter how I changed the components, it always impressed me with a ValueError as follows. I have no idea of what the problem is. <details><summary>Traceback</summary>. ```pytb. ValueError Traceback (most recent call last). ~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj). 339 pass. 340 else:. --> 341 return printer(obj). 342 # Finally look for special method names. 343 method = get_real_method(obj, self.print_method). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in <lambda>(fig). 242 . 243 if 'png' in formats:. --> 244 png_formatter.for_type(Figure, lambda fig: print_figure(fig, 'png', **kwargs)). 245 if 'retina' in formats or 'png2x' in formats:. 246 png_formatter.for_type(Figure, lambda fig: retina_figure(fig, **kwargs)). ~/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/787:1977,usability,clear,cleared,1977,".7/site-packages/IPython/core/pylabtools.py in print_figure(fig, fmt, bbox_inches, **kwargs). 126 . 127 bytes_io = BytesIO(). --> 128 fig.canvas.print_figure(bytes_io, **kw). 129 data = bytes_io.getvalue(). 130 if fmt == 'svg':. ~/.local/lib/python3.7/site-packages/matplotlib/backend_bases.py in print_figure(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs). 2054 orientation=orientation,. 2055 dryrun=True,. -> 2056 **kwargs). 2057 renderer = self.figure._cachedRenderer. 2058 bbox_artists = kwargs.pop(""bbox_extra_artists"", None). ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in print_png(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs). 525 . 526 else:. --> 527 FigureCanvasAgg.draw(self). 528 renderer = self.get_renderer(). 529 with cbook._setattr_cm(renderer, dpi=self.figure.dpi), \. ~/.local/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py in draw(self). 386 self.renderer = self.get_renderer(cleared=True). 387 with RendererAgg.lock:. --> 388 self.figure.draw(self.renderer). 389 # A GUI class may be need to update a window using this draw, so. 390 # don't forget to call the superclass. ~/.local/lib/python3.7/site-packages/matplotlib/artist.py in draw_wrapper(artist, renderer, *args, **kwargs). 36 renderer.start_filter(). 37 . ---> 38 return draw(artist, renderer, *args, **kwargs). 39 finally:. 40 if artist.get_agg_filter() is not None:. ~/.local/lib/python3.7/site-packages/matplotlib/figure.py in draw(self, renderer). 1707 self.patch.draw(renderer). 1708 mimage._draw_list_compositing_images(. -> 1709 renderer, self, artists, self.suppressComposite). 1710 . 1711 renderer.close_group('figure'). ~/.local/lib/python3.7/site-packages/matplotlib/image.py in _draw_list_compositing_images(renderer, parent, artists, suppress_composite). 133 if not_composite or not has_images:. 134 for a in artists:. --> 135 a.draw(renderer). 136 else:. 137 # Composite any adjacent images together. ~/.lo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/787
https://github.com/scverse/scanpy/issues/788:24,deployability,api,api,24,"Remove usage of `scanpy.api` from docstrings; I was recently surprised to see `scanpy.api` used in docstrings, but a quick search of the code showed over a hundred results. We should fix that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/788
https://github.com/scverse/scanpy/issues/788:86,deployability,api,api,86,"Remove usage of `scanpy.api` from docstrings; I was recently surprised to see `scanpy.api` used in docstrings, but a quick search of the code showed over a hundred results. We should fix that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/788
https://github.com/scverse/scanpy/issues/788:24,integrability,api,api,24,"Remove usage of `scanpy.api` from docstrings; I was recently surprised to see `scanpy.api` used in docstrings, but a quick search of the code showed over a hundred results. We should fix that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/788
https://github.com/scverse/scanpy/issues/788:86,integrability,api,api,86,"Remove usage of `scanpy.api` from docstrings; I was recently surprised to see `scanpy.api` used in docstrings, but a quick search of the code showed over a hundred results. We should fix that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/788
https://github.com/scverse/scanpy/issues/788:24,interoperability,api,api,24,"Remove usage of `scanpy.api` from docstrings; I was recently surprised to see `scanpy.api` used in docstrings, but a quick search of the code showed over a hundred results. We should fix that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/788
https://github.com/scverse/scanpy/issues/788:86,interoperability,api,api,86,"Remove usage of `scanpy.api` from docstrings; I was recently surprised to see `scanpy.api` used in docstrings, but a quick search of the code showed over a hundred results. We should fix that.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/788
https://github.com/scverse/scanpy/pull/789:244,deployability,build,builds,244,"Fix paga when no argument is given; @flying-sheep . I've moved the argument handling from the functions to the constructor since `groups` is used later in the function, and I thought changing it's value midway seemed inelegant. This should get builds working again.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/789
https://github.com/scverse/scanpy/pull/790:35,deployability,Updat,Update,35,Fix scatter `use_raw`; Fixes #762. Update to #764. Just adding a test to make sure the `use_raw` argument does something. I'm opening this a new pull request since git didn't me using other PRs branch name.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/790
https://github.com/scverse/scanpy/pull/790:106,reliability,doe,does,106,Fix scatter `use_raw`; Fixes #762. Update to #764. Just adding a test to make sure the `use_raw` argument does something. I'm opening this a new pull request since git didn't me using other PRs branch name.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/790
https://github.com/scverse/scanpy/pull/790:35,safety,Updat,Update,35,Fix scatter `use_raw`; Fixes #762. Update to #764. Just adding a test to make sure the `use_raw` argument does something. I'm opening this a new pull request since git didn't me using other PRs branch name.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/790
https://github.com/scverse/scanpy/pull/790:65,safety,test,test,65,Fix scatter `use_raw`; Fixes #762. Update to #764. Just adding a test to make sure the `use_raw` argument does something. I'm opening this a new pull request since git didn't me using other PRs branch name.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/790
https://github.com/scverse/scanpy/pull/790:35,security,Updat,Update,35,Fix scatter `use_raw`; Fixes #762. Update to #764. Just adding a test to make sure the `use_raw` argument does something. I'm opening this a new pull request since git didn't me using other PRs branch name.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/790
https://github.com/scverse/scanpy/pull/790:65,testability,test,test,65,Fix scatter `use_raw`; Fixes #762. Update to #764. Just adding a test to make sure the `use_raw` argument does something. I'm opening this a new pull request since git didn't me using other PRs branch name.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/790
https://github.com/scverse/scanpy/pull/791:126,safety,compl,complicated,126,"Pass on copy and return_info to dca; The arguments were not passed in preprocessing/_dca.py, which. makes debugging much more complicated than needed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/791
https://github.com/scverse/scanpy/pull/791:126,security,compl,complicated,126,"Pass on copy and return_info to dca; The arguments were not passed in preprocessing/_dca.py, which. makes debugging much more complicated than needed.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/791
https://github.com/scverse/scanpy/issues/793:0,deployability,Updat,Update,0,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:11,deployability,version,version,11,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:102,deployability,updat,update,102,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:118,deployability,version,version,118,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:11,integrability,version,version,11,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:118,integrability,version,version,118,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:11,modifiability,version,version,11,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:118,modifiability,version,version,118,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:0,safety,Updat,Update,0,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:102,safety,updat,update,102,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:0,security,Updat,Update,0,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:58,security,access,accessible,58,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:102,security,updat,update,102,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:41,usability,learn,learning,41,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/issues/793:201,usability,learn,learning,201,Update dca version; In order to make the learning rate of accessible from scanpy it will be needed to update the dca. version once https://github.com/theislab/dca/pull/27 is merged. Indeed the default learning rate is very high and the training diverges a lot on the tabula muris dataset.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/793
https://github.com/scverse/scanpy/pull/794:308,availability,consist,consistent,308,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:234,deployability,API,API,234,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:234,integrability,API,API,234,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:234,interoperability,API,API,234,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:600,interoperability,format,format,600,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:941,interoperability,specif,specific,941,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:363,modifiability,paramet,parameter,363,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:776,modifiability,paramet,parameter,776,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:1036,modifiability,paramet,parameters,1036,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:1005,safety,test,tests,1005,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:1022,safety,test,tests,1022,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:1005,testability,test,tests,1005,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:1022,testability,test,tests,1022,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:95,usability,user,user-images,95,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:308,usability,consist,consistent,308,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:480,usability,indicat,indicate,480,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:570,usability,indicat,indicate,570,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:616,usability,indicat,indicate,616,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/pull/794:701,usability,user,user,701,"Embedding improvements; This PR allows the creation of embedding as follows:. ![image](https://user-images.githubusercontent.com/4964309/63443678-89757d80-c435-11e9-8218-5f8c82ea7629.png). sumary PR:. * Added `sc.pl.embedding` to the API. This makes possible to select any 'basis' for plotting. This name is consistent with other functions that accept 'basis' as parameter: `sc.pl.embedding_density` and `scvelo.velocity_embedding` (#762) . * vmax and vmin now could be a list to indicate individual vmin/vmax range when using multiple plots (#775). * vmax and vmin can indicate a quantile using the format `qNN` to indicate the NN quantile. Eg. `vmax=['q90', 'q80']` (#775). * vmax and vmin can be a user defined function. (#775). * added the ~~`add_contour`~~ `add_outline` parameter which produces figures like the one on top (I will be happy to know if there is a better name for this type of scatter plot as `contour` may refer to some specific type of plots). TODO:. - [x] Correct style. - [x] Pass tests. - [x] Add tests for new parameters. - [x] Find better name for 'contour'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/794
https://github.com/scverse/scanpy/issues/796:187,usability,help,help,187,New colour palette?; Hey! I was wondering if there's a point in adding another colour palettes after:. https://www.jstatsoft.org/article/view/v090c01. Just came across this. And it might help in light of discussions such as #740,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/796
https://github.com/scverse/scanpy/pull/797:5,security,hash,hashing,5,Cell hashing; Here is a stab at adding cell hashing demultiplexing to scanpy. This will address https://github.com/theislab/scanpy/issues/351.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797
https://github.com/scverse/scanpy/pull/797:44,security,hash,hashing,44,Cell hashing; Here is a stab at adding cell hashing demultiplexing to scanpy. This will address https://github.com/theislab/scanpy/issues/351.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/797
https://github.com/scverse/scanpy/pull/798:71,deployability,version,versioneer,71,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:161,deployability,log,logic,161,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:267,deployability,build,building,267,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:302,deployability,instal,installed,302,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:361,deployability,build,build-requires,361,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:389,deployability,build,building,389,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:482,deployability,version,version,482,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:577,deployability,version,version,577,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:646,deployability,instal,installed,646,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:805,deployability,version,version,805,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:873,deployability,instal,installed,873,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:602,energy efficiency,current,currently,602,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:71,integrability,version,versioneer,71,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:482,integrability,version,version,482,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:577,integrability,version,version,577,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:805,integrability,version,version,805,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:41,modifiability,pac,packaging,41,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:71,modifiability,version,versioneer,71,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:174,modifiability,Pac,Package,174,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:451,modifiability,pac,package,451,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:482,modifiability,version,version,482,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:577,modifiability,version,version,577,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:805,modifiability,version,version,805,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:818,modifiability,pac,package,818,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:883,modifiability,pac,packages,883,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:195,performance,content,content,195,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:858,reliability,doe,does,858,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:161,safety,log,logic,161,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:161,security,log,logic,161,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:123,testability,simpl,simpler,123,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:161,testability,log,logic,161,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:123,usability,simpl,simpler,123,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/pull/798:249,usability,statu,status,249,"Migrate to setuptools_scm and get rid of packaging cruft.; No vendored versioneer.py anymore, no setup.cfg! This is vastly simpler. Could you please check if my logic holds? Package metadata and content will be correctly derived from the git repos status. 1. before building, `setuptools_scm` will be installed through `setup_requires=['setuptools_scm']` or `[build-requires]`. 2. during building, due to `use_scm_version=True`, it will populate the package metadata with a proper version from git. (Looks like `scanpy-1.4.5.dev60+g0adf706`). `scanpy.__version__` will be the version of scanpy youre currently developing (if applicable) or the installed one:. 1. scanpy tries to use `setuptools_scm.get_version`. 2. if `setuptools_scm` cant be imported or were not in a git repo, it will just use the version from package metadata (like `check_versions` does for other installed packages)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/798
https://github.com/scverse/scanpy/issues/799:606,integrability,filter,filtering,606,"how to name each hdf5 file to find which original object any particular cell came from?; Hi everyone, . I'm having trouble to distinguish the original hdf5 file from the concatenated file. I used below codes to concatenate 4 hdf5 files into one file:. ----------------------------------------------------------------------------------------------------------. filenames = ['filename1','filename2','filename3','filename4']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:]). my concern is to use this code with original file after run filtering and other codes with the concatenated file, for example:. ----------------------------------------------------------------------------------------------------------. sc.pl.umap(adata, color=['filename1']). where filename1 is no longer same as the ""actual"" filename1, instead it is filtered and normalized. I searched and found that I can do it with Seurat in this way:. ----------------------------------------------------------------------------------------------------------. pbmc4k.data <- Read10X(data.dir = ""../data/pbmc4k/filtered_gene_bc_matrices/GRCh38/""). pbmc4k <- CreateSeuratObject(counts = pbmc4k.data, project = ""PBMC4K""). pbmc8k.data <- Read10X(data.dir = ""../data/pbmc8k/filtered_gene_bc_matrices/GRCh38/""). pbmc8k <- CreateSeuratObject(counts = pbmc8k.data, project = ""PBMC8K""). pbmc.combined <- merge(pbmc4k, y = pbmc8k, add.cell.ids = c(""4K"", ""8K""), project = ""PBMC12K""). Is there any code that I can use for scanpy such as ""add.cell.ids = c()"" so that I can easily tell which original object any particular cell came from and use the data? Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/799
https://github.com/scverse/scanpy/issues/799:897,integrability,filter,filtered,897,"how to name each hdf5 file to find which original object any particular cell came from?; Hi everyone, . I'm having trouble to distinguish the original hdf5 file from the concatenated file. I used below codes to concatenate 4 hdf5 files into one file:. ----------------------------------------------------------------------------------------------------------. filenames = ['filename1','filename2','filename3','filename4']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:]). my concern is to use this code with original file after run filtering and other codes with the concatenated file, for example:. ----------------------------------------------------------------------------------------------------------. sc.pl.umap(adata, color=['filename1']). where filename1 is no longer same as the ""actual"" filename1, instead it is filtered and normalized. I searched and found that I can do it with Seurat in this way:. ----------------------------------------------------------------------------------------------------------. pbmc4k.data <- Read10X(data.dir = ""../data/pbmc4k/filtered_gene_bc_matrices/GRCh38/""). pbmc4k <- CreateSeuratObject(counts = pbmc4k.data, project = ""PBMC4K""). pbmc8k.data <- Read10X(data.dir = ""../data/pbmc8k/filtered_gene_bc_matrices/GRCh38/""). pbmc8k <- CreateSeuratObject(counts = pbmc8k.data, project = ""PBMC8K""). pbmc.combined <- merge(pbmc4k, y = pbmc8k, add.cell.ids = c(""4K"", ""8K""), project = ""PBMC12K""). Is there any code that I can use for scanpy such as ""add.cell.ids = c()"" so that I can easily tell which original object any particular cell came from and use the data? Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/799
https://github.com/scverse/scanpy/issues/799:549,modifiability,concern,concern,549,"how to name each hdf5 file to find which original object any particular cell came from?; Hi everyone, . I'm having trouble to distinguish the original hdf5 file from the concatenated file. I used below codes to concatenate 4 hdf5 files into one file:. ----------------------------------------------------------------------------------------------------------. filenames = ['filename1','filename2','filename3','filename4']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:]). my concern is to use this code with original file after run filtering and other codes with the concatenated file, for example:. ----------------------------------------------------------------------------------------------------------. sc.pl.umap(adata, color=['filename1']). where filename1 is no longer same as the ""actual"" filename1, instead it is filtered and normalized. I searched and found that I can do it with Seurat in this way:. ----------------------------------------------------------------------------------------------------------. pbmc4k.data <- Read10X(data.dir = ""../data/pbmc4k/filtered_gene_bc_matrices/GRCh38/""). pbmc4k <- CreateSeuratObject(counts = pbmc4k.data, project = ""PBMC4K""). pbmc8k.data <- Read10X(data.dir = ""../data/pbmc8k/filtered_gene_bc_matrices/GRCh38/""). pbmc8k <- CreateSeuratObject(counts = pbmc8k.data, project = ""PBMC8K""). pbmc.combined <- merge(pbmc4k, y = pbmc8k, add.cell.ids = c(""4K"", ""8K""), project = ""PBMC12K""). Is there any code that I can use for scanpy such as ""add.cell.ids = c()"" so that I can easily tell which original object any particular cell came from and use the data? Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/799
https://github.com/scverse/scanpy/issues/799:549,testability,concern,concern,549,"how to name each hdf5 file to find which original object any particular cell came from?; Hi everyone, . I'm having trouble to distinguish the original hdf5 file from the concatenated file. I used below codes to concatenate 4 hdf5 files into one file:. ----------------------------------------------------------------------------------------------------------. filenames = ['filename1','filename2','filename3','filename4']. adatas = [sc.read_10x_h5(filename, gex_only = True) for filename in filenames]. adata = adatas[0].concatenate(adatas[1:]). my concern is to use this code with original file after run filtering and other codes with the concatenated file, for example:. ----------------------------------------------------------------------------------------------------------. sc.pl.umap(adata, color=['filename1']). where filename1 is no longer same as the ""actual"" filename1, instead it is filtered and normalized. I searched and found that I can do it with Seurat in this way:. ----------------------------------------------------------------------------------------------------------. pbmc4k.data <- Read10X(data.dir = ""../data/pbmc4k/filtered_gene_bc_matrices/GRCh38/""). pbmc4k <- CreateSeuratObject(counts = pbmc4k.data, project = ""PBMC4K""). pbmc8k.data <- Read10X(data.dir = ""../data/pbmc8k/filtered_gene_bc_matrices/GRCh38/""). pbmc8k <- CreateSeuratObject(counts = pbmc8k.data, project = ""PBMC8K""). pbmc.combined <- merge(pbmc4k, y = pbmc8k, add.cell.ids = c(""4K"", ""8K""), project = ""PBMC12K""). Is there any code that I can use for scanpy such as ""add.cell.ids = c()"" so that I can easily tell which original object any particular cell came from and use the data? Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/799
https://github.com/scverse/scanpy/issues/800:80,availability,error,error,80,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:247,availability,error,error,247,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:287,availability,error,error,287,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:333,deployability,log,logic,333,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:882,deployability,modul,module,882,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:3471,deployability,contain,contain,3471," q, axis, out, overwrite_input, interpolation, keepdims). 3714 . 3715 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims). 3831 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:3895,deployability,modul,module,3895,"pe + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:6485,deployability,contain,contain,6485," title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ValueError(""Percentiles must be in the range [0, 100]""). 3712 return _quantile_unchecked(. -> 3713 a, q, axis, out, overwrite_input, interpolation, keepdims). 3714 . 3715 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims). 3831 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:1369,integrability,compon,components,1369," vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:4383,integrability,compon,components,4383,"above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:1369,interoperability,compon,components,1369," vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:1914,interoperability,format,format,1914,"olor=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ValueError(""Percentiles must be in the range [0, 100]""). 3712 return _quantile_unchecked(. -> 3713 a, q, axis, out, overwrite_input, interpolation, keepdims). 3714 . 3715 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims). 3831 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:4383,interoperability,compon,components,4383,"above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:4928,interoperability,format,format,4928,"lor=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ValueError(""Percentiles must be in the range [0, 100]""). 3712 return _quantile_unchecked(. -> 3713 a, q, axis, out, overwrite_input, interpolation, keepdims). 3714 . 3715 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims). 3831 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:882,modifiability,modul,module,882,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:1369,modifiability,compon,components,1369," vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:1381,modifiability,layer,layer,1381,"numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ValueError",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:2253,modifiability,pac,packages,2253," basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ValueError(""Percentiles must be in the range [0, 100]""). 3712 return _quantile_unchecked(. -> 3713 a, q, axis, out, overwrite_input, interpolation, keepdims). 3714 . 3715 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims). 3831 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:2577,modifiability,pac,packages,2577,"title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ValueError(""Percentiles must be in the range [0, 100]""). 3712 return _quantile_unchecked(. -> 3713 a, q, axis, out, overwrite_input, interpolation, keepdims). 3714 . 3715 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims). 3831 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:2937,modifiability,pac,packages,2937,"391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ValueError(""Percentiles must be in the range [0, 100]""). 3712 return _quantile_unchecked(. -> 3713 a, q, axis, out, overwrite_input, interpolation, keepdims). 3714 . 3715 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims). 3831 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:3134,modifiability,pac,packages,3134,"min/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ValueError(""Percentiles must be in the range [0, 100]""). 3712 return _quantile_unchecked(. -> 3713 a, q, axis, out, overwrite_input, interpolation, keepdims). 3714 . 3715 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims). 3831 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:3895,modifiability,modul,module,3895,"pe + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:4383,modifiability,compon,components,4383,"above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:4395,modifiability,layer,layer,4395,"s=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ValueError",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:5267,modifiability,pac,packages,5267," basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ValueError(""Percentiles must be in the range [0, 100]""). 3712 return _quantile_unchecked(. -> 3713 a, q, axis, out, overwrite_input, interpolation, keepdims). 3714 . 3715 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims). 3831 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:5591,modifiability,pac,packages,5591," title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ValueError(""Percentiles must be in the range [0, 100]""). 3712 return _quantile_unchecked(. -> 3713 a, q, axis, out, overwrite_input, interpolation, keepdims). 3714 . 3715 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims). 3831 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:5951,modifiability,pac,packages,5951," title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ValueError(""Percentiles must be in the range [0, 100]""). 3712 return _quantile_unchecked(. -> 3713 a, q, axis, out, overwrite_input, interpolation, keepdims). 3714 . 3715 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims). 3831 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:6148,modifiability,pac,packages,6148," title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ValueError(""Percentiles must be in the range [0, 100]""). 3712 return _quantile_unchecked(. -> 3713 a, q, axis, out, overwrite_input, interpolation, keepdims). 3714 . 3715 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims). 3831 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:80,performance,error,error,80,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:247,performance,error,error,247,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:287,performance,error,error,287,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:80,safety,error,error,80,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:247,safety,error,error,247,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:287,safety,error,error,287,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:333,safety,log,logic,333,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:855,safety,input,input-,855,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:882,safety,modul,module,882,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:3868,safety,input,input-,3868,"3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_v",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:3895,safety,modul,module,3895,"pe + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:333,security,log,logic,333,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:3491,security,sign,signature,3491,"ite_input, interpolation, keepdims). 3714 . 3715 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims). 3831 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:6505,security,sign,signature,6505," title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'. --> 392 v_value = np.percentile(color_vector, q=float(v_value[1:])). 393 elif callable(v_value):. 394 # interpret vmin/vmax as function. <__array_function__ internals> in percentile(*args, **kwargs). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in percentile(a, q, axis, out, overwrite_input, interpolation, keepdims). 3711 raise ValueError(""Percentiles must be in the range [0, 100]""). 3712 return _quantile_unchecked(. -> 3713 a, q, axis, out, overwrite_input, interpolation, keepdims). 3714 . 3715 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_unchecked(a, q, axis, out, overwrite_input, interpolation, keepdims). 3831 r, k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:333,testability,log,logic,333,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:686,testability,Trace,Traceback,686,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:811,testability,Trace,Traceback,811,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:3699,testability,Trace,Traceback,3699,"k = _ureduce(a, func=_quantile_ureduce_func, q=q, axis=axis, out=out,. 3832 overwrite_input=overwrite_input,. -> 3833 interpolation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin']",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:3824,testability,Trace,Traceback,3824,"lation=interpolation). 3834 if keepdims:. 3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:80,usability,error,error,80,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:193,usability,minim,minimum,193,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:247,usability,error,error,247,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:287,usability,error,error,287,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:855,usability,input,input-,855,"Passing string or function to `vmax`, `vmin` when categorical in `color` throws error; @fidelram. Related to changes made in #794. . If a categorical column is passed in color, but maximum and minimum colors are passed as functions or strings, an error is thrown. It looks like the same error in each case. I think we just need some logic saying to only try and set vmin and vmax for numeric values. ## Example. Setup. ```python. import scanpy as sc. import numpy as np. from functools import partial. pbmc = sc.datasets.pbmc68k_reduced(). ```. Passing a function. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=partial(np.quantile, q=0.99)). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-13-83df06d6a2e8> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"",""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_vmin_vmax(vmin, vmax, index, color_vector). 390 f""correct format for percentiles.""). 391 # interpret value of vmin/vmax as quantile with the fol",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/issues/800:3868,usability,input,input-,3868,"3835 return r.reshape(q.shape + k). /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _ureduce(a, func, **kwargs). 3408 keepdim = (1,) * a.ndim. 3409 . -> 3410 r = func(a, **kwargs). 3411 return r, keepdim. 3412 . /usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py in _quantile_ureduce_func(a, q, axis, out, overwrite_input, interpolation, keepdims). 3946 n = np.isnan(ap[-1:, ...]). 3947 . -> 3948 x1 = take(ap, indices_below, axis=axis) * weights_below. 3949 x2 = take(ap, indices_above, axis=axis) * weights_above. 3950 . UFuncTypeError: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U32'), dtype('<U32')) -> dtype('<U32'). ```. </details>. Passing a string:. ```python. sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ```. <details>. <summary> Traceback </summary>. ```python. ---------------------------------------------------------------------------. UFuncTypeError Traceback (most recent call last). <ipython-input-19-810b38c71c8d> in <module>. ----> 1 sc.pl.umap(pbmc, color=[""bulk_labels"", ""HES4""], vmax=""p99""). ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in umap(adata, **kwargs). 436 If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. 437 """""". --> 438 return embedding(adata, 'umap', **kwargs). 439 . 440 . ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in embedding(adata, basis, color, gene_symbols, use_raw, sort_order, edges, edges_width, edges_color, arrows, arrows_kwds, groups, components, layer, projection, color_map, palette, size, frameon, legend_fontsize, legend_fontweight, legend_loc, legend_fontoutline, vmax, vmin, add_outline, outline_width, outline_color, ncols, hspace, wspace, title, show, save, ax, return_fig, **kwargs). 230 . 231 # check vmin and vmax options. --> 232 kwargs['vmin'], kwargs['vmax'] = _get_vmin_vmax(vmin, vmax, count, color_vector). 233 . 234 # make the scatter plot. ~/github/scanpy/scanpy/plotting/_tools/scatterplots.py in _get_v",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/800
https://github.com/scverse/scanpy/pull/801:113,interoperability,specif,specify,113,"De layers; Fixes #773. I'm thinking the default for `use_raw` should also change, since right now you'll have to specify `use_raw` to `False`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/801
https://github.com/scverse/scanpy/pull/801:3,modifiability,layer,layers,3,"De layers; Fixes #773. I'm thinking the default for `use_raw` should also change, since right now you'll have to specify `use_raw` to `False`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/801
https://github.com/scverse/scanpy/pull/802:204,availability,error,error,204,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:245,availability,error,error,245,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:4,deployability,build,builds,4,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:40,deployability,build,builds,40,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:427,deployability,api,api,427,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:427,integrability,api,api,427,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:114,interoperability,share,shared,114,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:427,interoperability,api,api,427,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:204,performance,error,error,204,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:245,performance,error,error,245,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:204,safety,error,error,204,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:245,safety,error,error,245,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:204,usability,error,error,204,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:245,usability,error,error,245,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/802:254,usability,User,Users,254,"Doc builds; This PR is meant to get doc builds working again. I've added `setuptools_scm` and fixed a typo in the shared plotting docs. Read the docs seems to be working again, but I'm still getting this error locally:. ```. Warning, treated as error:. /Users/isaac/github/scanpy/docs/external/scanpy.external.exporting.cellbrowser.rst:2:duplicate object description of scanpy.external.exporting.cellbrowser, other instance in api/scanpy.external.exporting.cellbrowser, use :noindex: for one of them. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/802
https://github.com/scverse/scanpy/pull/803:13,availability,error,error,13,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:410,availability,error,error,410,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:445,availability,error,error,445,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:118,integrability,compon,components,118,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:156,integrability,compon,components,156,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:222,integrability,compon,components,222,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:278,integrability,sub,subtracts,278,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:118,interoperability,compon,components,118,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:156,interoperability,compon,components,156,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:222,interoperability,compon,components,222,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:118,modifiability,compon,components,118,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:156,modifiability,compon,components,156,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:222,modifiability,compon,components,222,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:13,performance,error,error,13,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:410,performance,error,error,410,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:445,performance,error,error,445,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:13,safety,error,error,13,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:410,safety,error,error,410,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:445,safety,error,error,445,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:13,usability,error,error,13,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:410,usability,error,error,410,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/pull/803:445,usability,error,error,445,"Exit with an error if sc.pl.pca_loadings is called with indices < 1; Right now, when we do `sc.pl.pca_loadings(adata, components=range(5))` to plot first 5 components or even weirder things like `sc.pl.pca_loadings(adata, components=[-10, 0, 5])` the plotting function silently subtracts 1 and uses these as indices for `adata.varm['PCs']`. We can see in the plot title things like `PC-11`. It's confusing and error prone. This change throws an error if a PC index is invalid.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/803
https://github.com/scverse/scanpy/issues/909:155,availability,error,error,155,"ValueError: If `root` is a string, it needs to be one of [None] not 'neoblast 1'.; Hello. I need some help with this issue. when I run this line, I got an error. . ```py. sc.pl.paga(. adata,. threshold=0, . solid_edges='connectivities_tree',. dashed_edges='connectivities', . root='neoblast 1',. layout='rt_circular',. node_size_scale=0.5,. node_size_power=0.9,. max_edge_width=0.7,. fontsize=3.5,. ). ```. ![error](https://user-images.githubusercontent.com/48261734/63892645-beda1800-c9ad-11e9-8f04-f13a5b7a8812.jpg). thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/909
https://github.com/scverse/scanpy/issues/909:409,availability,error,error,409,"ValueError: If `root` is a string, it needs to be one of [None] not 'neoblast 1'.; Hello. I need some help with this issue. when I run this line, I got an error. . ```py. sc.pl.paga(. adata,. threshold=0, . solid_edges='connectivities_tree',. dashed_edges='connectivities', . root='neoblast 1',. layout='rt_circular',. node_size_scale=0.5,. node_size_power=0.9,. max_edge_width=0.7,. fontsize=3.5,. ). ```. ![error](https://user-images.githubusercontent.com/48261734/63892645-beda1800-c9ad-11e9-8f04-f13a5b7a8812.jpg). thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/909
https://github.com/scverse/scanpy/issues/909:155,performance,error,error,155,"ValueError: If `root` is a string, it needs to be one of [None] not 'neoblast 1'.; Hello. I need some help with this issue. when I run this line, I got an error. . ```py. sc.pl.paga(. adata,. threshold=0, . solid_edges='connectivities_tree',. dashed_edges='connectivities', . root='neoblast 1',. layout='rt_circular',. node_size_scale=0.5,. node_size_power=0.9,. max_edge_width=0.7,. fontsize=3.5,. ). ```. ![error](https://user-images.githubusercontent.com/48261734/63892645-beda1800-c9ad-11e9-8f04-f13a5b7a8812.jpg). thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/909
https://github.com/scverse/scanpy/issues/909:409,performance,error,error,409,"ValueError: If `root` is a string, it needs to be one of [None] not 'neoblast 1'.; Hello. I need some help with this issue. when I run this line, I got an error. . ```py. sc.pl.paga(. adata,. threshold=0, . solid_edges='connectivities_tree',. dashed_edges='connectivities', . root='neoblast 1',. layout='rt_circular',. node_size_scale=0.5,. node_size_power=0.9,. max_edge_width=0.7,. fontsize=3.5,. ). ```. ![error](https://user-images.githubusercontent.com/48261734/63892645-beda1800-c9ad-11e9-8f04-f13a5b7a8812.jpg). thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/909
https://github.com/scverse/scanpy/issues/909:155,safety,error,error,155,"ValueError: If `root` is a string, it needs to be one of [None] not 'neoblast 1'.; Hello. I need some help with this issue. when I run this line, I got an error. . ```py. sc.pl.paga(. adata,. threshold=0, . solid_edges='connectivities_tree',. dashed_edges='connectivities', . root='neoblast 1',. layout='rt_circular',. node_size_scale=0.5,. node_size_power=0.9,. max_edge_width=0.7,. fontsize=3.5,. ). ```. ![error](https://user-images.githubusercontent.com/48261734/63892645-beda1800-c9ad-11e9-8f04-f13a5b7a8812.jpg). thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/909
https://github.com/scverse/scanpy/issues/909:409,safety,error,error,409,"ValueError: If `root` is a string, it needs to be one of [None] not 'neoblast 1'.; Hello. I need some help with this issue. when I run this line, I got an error. . ```py. sc.pl.paga(. adata,. threshold=0, . solid_edges='connectivities_tree',. dashed_edges='connectivities', . root='neoblast 1',. layout='rt_circular',. node_size_scale=0.5,. node_size_power=0.9,. max_edge_width=0.7,. fontsize=3.5,. ). ```. ![error](https://user-images.githubusercontent.com/48261734/63892645-beda1800-c9ad-11e9-8f04-f13a5b7a8812.jpg). thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/909
https://github.com/scverse/scanpy/issues/909:102,usability,help,help,102,"ValueError: If `root` is a string, it needs to be one of [None] not 'neoblast 1'.; Hello. I need some help with this issue. when I run this line, I got an error. . ```py. sc.pl.paga(. adata,. threshold=0, . solid_edges='connectivities_tree',. dashed_edges='connectivities', . root='neoblast 1',. layout='rt_circular',. node_size_scale=0.5,. node_size_power=0.9,. max_edge_width=0.7,. fontsize=3.5,. ). ```. ![error](https://user-images.githubusercontent.com/48261734/63892645-beda1800-c9ad-11e9-8f04-f13a5b7a8812.jpg). thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/909
https://github.com/scverse/scanpy/issues/909:155,usability,error,error,155,"ValueError: If `root` is a string, it needs to be one of [None] not 'neoblast 1'.; Hello. I need some help with this issue. when I run this line, I got an error. . ```py. sc.pl.paga(. adata,. threshold=0, . solid_edges='connectivities_tree',. dashed_edges='connectivities', . root='neoblast 1',. layout='rt_circular',. node_size_scale=0.5,. node_size_power=0.9,. max_edge_width=0.7,. fontsize=3.5,. ). ```. ![error](https://user-images.githubusercontent.com/48261734/63892645-beda1800-c9ad-11e9-8f04-f13a5b7a8812.jpg). thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/909
https://github.com/scverse/scanpy/issues/909:409,usability,error,error,409,"ValueError: If `root` is a string, it needs to be one of [None] not 'neoblast 1'.; Hello. I need some help with this issue. when I run this line, I got an error. . ```py. sc.pl.paga(. adata,. threshold=0, . solid_edges='connectivities_tree',. dashed_edges='connectivities', . root='neoblast 1',. layout='rt_circular',. node_size_scale=0.5,. node_size_power=0.9,. max_edge_width=0.7,. fontsize=3.5,. ). ```. ![error](https://user-images.githubusercontent.com/48261734/63892645-beda1800-c9ad-11e9-8f04-f13a5b7a8812.jpg). thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/909
https://github.com/scverse/scanpy/issues/909:424,usability,user,user-images,424,"ValueError: If `root` is a string, it needs to be one of [None] not 'neoblast 1'.; Hello. I need some help with this issue. when I run this line, I got an error. . ```py. sc.pl.paga(. adata,. threshold=0, . solid_edges='connectivities_tree',. dashed_edges='connectivities', . root='neoblast 1',. layout='rt_circular',. node_size_scale=0.5,. node_size_power=0.9,. max_edge_width=0.7,. fontsize=3.5,. ). ```. ![error](https://user-images.githubusercontent.com/48261734/63892645-beda1800-c9ad-11e9-8f04-f13a5b7a8812.jpg). thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/909
https://github.com/scverse/scanpy/pull/804:95,availability,error,error,95,fix vmin/vmax for categorical data #800; * fixed vmin/vmax for categorical data #800 . * added error message when vmin is not valid to point out how to format it. * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:165,deployability,updat,updated,165,fix vmin/vmax for categorical data #800; * fixed vmin/vmax for categorical data #800 . * added error message when vmin is not valid to point out how to format it. * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:101,integrability,messag,message,101,fix vmin/vmax for categorical data #800; * fixed vmin/vmax for categorical data #800 . * added error message when vmin is not valid to point out how to format it. * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:101,interoperability,messag,message,101,fix vmin/vmax for categorical data #800; * fixed vmin/vmax for categorical data #800 . * added error message when vmin is not valid to point out how to format it. * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:152,interoperability,format,format,152,fix vmin/vmax for categorical data #800; * fixed vmin/vmax for categorical data #800 . * added error message when vmin is not valid to point out how to format it. * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:95,performance,error,error,95,fix vmin/vmax for categorical data #800; * fixed vmin/vmax for categorical data #800 . * added error message when vmin is not valid to point out how to format it. * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:95,safety,error,error,95,fix vmin/vmax for categorical data #800; * fixed vmin/vmax for categorical data #800 . * added error message when vmin is not valid to point out how to format it. * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:126,safety,valid,valid,126,fix vmin/vmax for categorical data #800; * fixed vmin/vmax for categorical data #800 . * added error message when vmin is not valid to point out how to format it. * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:165,safety,updat,updated,165,fix vmin/vmax for categorical data #800; * fixed vmin/vmax for categorical data #800 . * added error message when vmin is not valid to point out how to format it. * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:173,safety,test,test,173,fix vmin/vmax for categorical data #800; * fixed vmin/vmax for categorical data #800 . * added error message when vmin is not valid to point out how to format it. * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:165,security,updat,updated,165,fix vmin/vmax for categorical data #800; * fixed vmin/vmax for categorical data #800 . * added error message when vmin is not valid to point out how to format it. * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:173,testability,test,test,173,fix vmin/vmax for categorical data #800; * fixed vmin/vmax for categorical data #800 . * added error message when vmin is not valid to point out how to format it. * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/804:95,usability,error,error,95,fix vmin/vmax for categorical data #800; * fixed vmin/vmax for categorical data #800 . * added error message when vmin is not valid to point out how to format it. * updated test to cover categorical data,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/804
https://github.com/scverse/scanpy/pull/805:9,energy efficiency,load,loadings,9,"Show PCA loadings for genes with lowest loadings; Currently, we do not show the genes with the lowest loadings in `sc.pl.pca_loadings`. This PRs add an option for that:. ![image](https://user-images.githubusercontent.com/1140359/63976191-c6b5bd00-ca7e-11e9-9173-f04e2a473388.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805
https://github.com/scverse/scanpy/pull/805:40,energy efficiency,load,loadings,40,"Show PCA loadings for genes with lowest loadings; Currently, we do not show the genes with the lowest loadings in `sc.pl.pca_loadings`. This PRs add an option for that:. ![image](https://user-images.githubusercontent.com/1140359/63976191-c6b5bd00-ca7e-11e9-9173-f04e2a473388.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805
https://github.com/scverse/scanpy/pull/805:50,energy efficiency,Current,Currently,50,"Show PCA loadings for genes with lowest loadings; Currently, we do not show the genes with the lowest loadings in `sc.pl.pca_loadings`. This PRs add an option for that:. ![image](https://user-images.githubusercontent.com/1140359/63976191-c6b5bd00-ca7e-11e9-9173-f04e2a473388.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805
https://github.com/scverse/scanpy/pull/805:102,energy efficiency,load,loadings,102,"Show PCA loadings for genes with lowest loadings; Currently, we do not show the genes with the lowest loadings in `sc.pl.pca_loadings`. This PRs add an option for that:. ![image](https://user-images.githubusercontent.com/1140359/63976191-c6b5bd00-ca7e-11e9-9173-f04e2a473388.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805
https://github.com/scverse/scanpy/pull/805:9,performance,load,loadings,9,"Show PCA loadings for genes with lowest loadings; Currently, we do not show the genes with the lowest loadings in `sc.pl.pca_loadings`. This PRs add an option for that:. ![image](https://user-images.githubusercontent.com/1140359/63976191-c6b5bd00-ca7e-11e9-9173-f04e2a473388.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805
https://github.com/scverse/scanpy/pull/805:40,performance,load,loadings,40,"Show PCA loadings for genes with lowest loadings; Currently, we do not show the genes with the lowest loadings in `sc.pl.pca_loadings`. This PRs add an option for that:. ![image](https://user-images.githubusercontent.com/1140359/63976191-c6b5bd00-ca7e-11e9-9173-f04e2a473388.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805
https://github.com/scverse/scanpy/pull/805:102,performance,load,loadings,102,"Show PCA loadings for genes with lowest loadings; Currently, we do not show the genes with the lowest loadings in `sc.pl.pca_loadings`. This PRs add an option for that:. ![image](https://user-images.githubusercontent.com/1140359/63976191-c6b5bd00-ca7e-11e9-9173-f04e2a473388.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805
https://github.com/scverse/scanpy/pull/805:187,usability,user,user-images,187,"Show PCA loadings for genes with lowest loadings; Currently, we do not show the genes with the lowest loadings in `sc.pl.pca_loadings`. This PRs add an option for that:. ![image](https://user-images.githubusercontent.com/1140359/63976191-c6b5bd00-ca7e-11e9-9173-f04e2a473388.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/805
https://github.com/scverse/scanpy/pull/806:66,reliability,doe,does,66,"Minor PAGA plotting improvements related to legend fonts; This PR does a few things:. - Adds `fontoutline` argument to sc.pl.paga as a convenient shortcut. Similar to https://github.com/theislab/scanpy/pull/640. This can also be achieved via text_kwds but it's super verbose and less intuitive for beginners. - Removes the usage of empty dictionaries as default arguments, because it might lead to [erroneous results in successive calls](https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments). . - `sc.pl.paga_compare` now adds `legend_font{size,weight,outline}` arguments to paga_graph_params by default which results in better agreement between the embedding view and paga view.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/806
https://github.com/scverse/scanpy/pull/806:146,usability,shortcut,shortcut,146,"Minor PAGA plotting improvements related to legend fonts; This PR does a few things:. - Adds `fontoutline` argument to sc.pl.paga as a convenient shortcut. Similar to https://github.com/theislab/scanpy/pull/640. This can also be achieved via text_kwds but it's super verbose and less intuitive for beginners. - Removes the usage of empty dictionaries as default arguments, because it might lead to [erroneous results in successive calls](https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments). . - `sc.pl.paga_compare` now adds `legend_font{size,weight,outline}` arguments to paga_graph_params by default which results in better agreement between the embedding view and paga view.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/806
https://github.com/scverse/scanpy/pull/806:284,usability,intuit,intuitive,284,"Minor PAGA plotting improvements related to legend fonts; This PR does a few things:. - Adds `fontoutline` argument to sc.pl.paga as a convenient shortcut. Similar to https://github.com/theislab/scanpy/pull/640. This can also be achieved via text_kwds but it's super verbose and less intuitive for beginners. - Removes the usage of empty dictionaries as default arguments, because it might lead to [erroneous results in successive calls](https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments). . - `sc.pl.paga_compare` now adds `legend_font{size,weight,outline}` arguments to paga_graph_params by default which results in better agreement between the embedding view and paga view.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/806
https://github.com/scverse/scanpy/pull/806:458,usability,guid,guide,458,"Minor PAGA plotting improvements related to legend fonts; This PR does a few things:. - Adds `fontoutline` argument to sc.pl.paga as a convenient shortcut. Similar to https://github.com/theislab/scanpy/pull/640. This can also be achieved via text_kwds but it's super verbose and less intuitive for beginners. - Removes the usage of empty dictionaries as default arguments, because it might lead to [erroneous results in successive calls](https://docs.python-guide.org/writing/gotchas/#mutable-default-arguments). . - `sc.pl.paga_compare` now adds `legend_font{size,weight,outline}` arguments to paga_graph_params by default which results in better agreement between the embedding view and paga view.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/806
https://github.com/scverse/scanpy/issues/807:81,availability,error,error,81,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:390,deployability,modul,module,390,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:686,deployability,log,logg,686,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1281,deployability,modul,module,1281,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1477,deployability,upgrad,upgrade,1477,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1638,deployability,upgrad,upgrade,1638,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:167,modifiability,pac,package,167,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:390,modifiability,modul,module,390,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:458,modifiability,pac,packages,458,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:914,modifiability,pac,packages,914,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1249,modifiability,pac,packages,1249,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1281,modifiability,modul,module,1281,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1477,modifiability,upgrad,upgrade,1477,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1638,modifiability,upgrad,upgrade,1638,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:81,performance,error,error,81,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:81,safety,error,error,81,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:362,safety,input,input-,362,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:390,safety,modul,module,390,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:686,safety,log,logg,686,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1281,safety,modul,module,1281,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1354,safety,avoid,avoid,1354,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1528,safety,avoid,avoid,1528,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:686,security,log,logg,686,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:318,testability,Trace,Traceback,318,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:686,testability,log,logg,686,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:81,usability,error,error,81,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:362,usability,input,input-,362,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:474,usability,tool,tools,474,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1411,usability,visual,visualization,1411,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/807:1579,usability,visual,visualization,1579,"It seems that igraph project changed its name; While I run 'sc.tl.louvain', this error raised and told me I should call 'jgraph'. Is it necessary for me to change the package name of 'igraph'to 'jgraph' manually? ```pytb. ---------------------------------------------------------------------------. DeprecationWarning Traceback (most recent call last). <ipython-input-191-71b705e00011> in <module>. ----> 1 sc.tl.louvain(ad_2). /usr/local/lib/python3.6/dist-packages/scanpy/tools/_louvain.py in louvain(adata, resolution, random_state, restrict_to, key_added, adjacency, flavor, directed, use_weights, partition_type, partition_kwargs, copy). 112 directed = False. 113 if not directed: logg.debug(' using the undirected graph'). --> 114 g = utils.get_igraph_from_adjacency(adjacency, directed=directed). 115 if use_weights:. 116 weights = np.array(g.es[""weight""]).astype(np.float64). /usr/local/lib/python3.6/dist-packages/scanpy/utils.py in get_igraph_from_adjacency(adjacency, directed). 379 def get_igraph_from_adjacency(adjacency, directed=None):. 380 """"""Get igraph graph from adjacency matrix."""""". --> 381 import igraph as ig. 382 sources, targets = adjacency.nonzero(). 383 weights = adjacency[sources, targets]. /usr/local/lib/python3.6/dist-packages/igraph/__init__.py in <module>. 6 __license__ = ""MIT"". 7 . ----> 8 raise DeprecationWarning(""To avoid name collision with the igraph project, "". 9 ""this visualization library has been renamed to "". 10 ""'jgraph'. Please upgrade when convenient.""). DeprecationWarning: To avoid name collision with the igraph project, this visualization library has been renamed to 'jgraph'. Please upgrade when convenient. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/807
https://github.com/scverse/scanpy/issues/808:119,deployability,fail,failed,119,"create scanpy.object by ""CreateSeuratObject(pbmc.matrix, meta.data = pbmc.MetaData)""; sorry to start this issue, but i failed in all the methods i found to transport a seurat.object into scanpy, including loom file, SingleCellExperiment . hence, i wonder if scanpy have the function like seurat that can create an object by `CreateSeuratObject(pbmc.matrix, meta.data = pbmc.MetaData)`. because i can successfully export the matrix and meta.data from seurat.object, and the meta data is the key information i want to import into scanpy for further analysis.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/808
https://github.com/scverse/scanpy/issues/808:119,reliability,fail,failed,119,"create scanpy.object by ""CreateSeuratObject(pbmc.matrix, meta.data = pbmc.MetaData)""; sorry to start this issue, but i failed in all the methods i found to transport a seurat.object into scanpy, including loom file, SingleCellExperiment . hence, i wonder if scanpy have the function like seurat that can create an object by `CreateSeuratObject(pbmc.matrix, meta.data = pbmc.MetaData)`. because i can successfully export the matrix and meta.data from seurat.object, and the meta data is the key information i want to import into scanpy for further analysis.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/808
https://github.com/scverse/scanpy/pull/809:4,availability,cluster,clustermap,4,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:15,availability,error,error,15,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:60,availability,cluster,clustermap,60,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:86,availability,error,error,86,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:166,availability,cluster,clustermap,166,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:453,availability,cluster,clustermap,453,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:539,availability,cluster,clustermap,539,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:699,availability,cluster,clustermap,699,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:769,availability,cluster,clustermap,769,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:829,availability,cluster,clustermap,829,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:993,availability,mask,mask,993,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:1124,availability,mask,mask,1124,"adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:1129,availability,mask,mask,1129," = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, met",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:1325,availability,mask,mask,1325,"ck (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:1945,availability,down,downcast,1945,"nkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, l",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:1990,availability,down,downcast,1990,"ask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(sel",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:1999,availability,down,downcast,1999,"args). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value,",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2158,availability,down,downcast,2158,".plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=li",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2263,availability,down,downcast,2263,"z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2272,availability,down,downcast,2272,"standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2497,availability,down,downcast,2497,"._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2964,availability,down,downcast,2964,"43 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the colo",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3031,availability,down,downcast,3031,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3705,availability,mask,mask,3705,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:4,deployability,cluster,clustermap,4,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:60,deployability,cluster,clustermap,60,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:72,deployability,fail,fails,72,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:166,deployability,cluster,clustermap,166,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:390,deployability,modul,module,390,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:453,deployability,cluster,clustermap,453,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:539,deployability,cluster,clustermap,539,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:699,deployability,cluster,clustermap,699,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:769,deployability,cluster,clustermap,769,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:829,deployability,cluster,clustermap,829,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2358,deployability,manag,managers,2358,"f.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_valu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2585,deployability,manag,managers,2585,"rocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:1877,energy efficiency,core,core,1877,"e, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2089,energy efficiency,core,core,2089,"d_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else s",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2343,energy efficiency,core,core,2343,"f.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_nam",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2358,energy efficiency,manag,managers,2358,"f.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_valu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2570,energy efficiency,core,core,2570,"atrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2585,energy efficiency,manag,managers,2585,"rocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2900,energy efficiency,core,core,2900,"self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical var",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3475,energy efficiency,core,core,3475,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2621,integrability,filter,filter,2621,"xis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.catego",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3286,integrability,wrap,wrapper,3286,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3420,integrability,wrap,wrapper,3420,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3286,interoperability,wrapper,wrapper,3286,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3420,interoperability,wrapper,wrapper,3420,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:390,modifiability,modul,module,390,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:1861,modifiability,pac,packages,1861,"metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2073,modifiability,pac,packages,2073,"score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2327,modifiability,pac,packages,2327," 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwar",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2554,modifiability,pac,packages,2554,"born/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hash",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2884,modifiability,pac,packages,2884,".py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3247,modifiability,pac,packages,3247,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3459,modifiability,pac,packages,3459,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3899,modifiability,variab,variable,3899,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3982,modifiability,variab,variable,3982,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:15,performance,error,error,15,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:86,performance,error,error,86,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2693,performance,reindex,reindex,2693,"these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:72,reliability,fail,fails,72,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:15,safety,error,error,15,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:86,safety,error,error,86,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:364,safety,input,input-,364,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:390,safety,modul,module,390,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2358,safety,manag,managers,2358,"f.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_valu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:2585,safety,manag,managers,2585,"rocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=downcast,. -> 4345 **kwargs. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:4016,safety,test,tests,4016,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:320,testability,Trace,Traceback,320,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3943,testability,simpl,simply,3943,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:4016,testability,test,tests,4016,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:15,usability,error,error,15,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:86,usability,error,error,86,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:364,usability,input,input-,364,"Fix clustermap error due to fillna call in seaborn.; `sc.pl.clustermap` fails with an error:. ```python. import scanpy as sc. adata = sc.datasets.krumsiek11(). sc.pl.clustermap(adata, obs_keys='cell_type'). ```. Output:. ```python. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-2-15436b6f0954> in <module>. 3 . 4 adata = sc.datasets.krumsiek11(). ----> 5 sc.pl.clustermap(adata, obs_keys='cell_type'). ~/Code/scanpy/scanpy/plotting/_anndata.py in clustermap(adata, obs_keys, use_raw, show, save, **kwds). 777 adata.uns[obs_keys + '_colors'])). 778 row_colors = adata.obs[obs_keys].map(lut). --> 779 g = sns.clustermap(df, row_colors=row_colors, **kwds). 780 else:. 781 g = sns.clustermap(df, **kwds). ~/Code/seaborn/seaborn/matrix.py in clustermap(data, pivot_kws, method, metric, z_score, standard_scale, figsize, cbar_kws, row_cluster, col_cluster, row_linkage, col_linkage, row_colors, col_colors, mask, **kwargs). 1316 row_colors=row_colors, col_colors=col_colors,. 1317 z_score=z_score, standard_scale=standard_scale,. -> 1318 mask=mask). 1319 . 1320 return plotter.plot(metric=metric, method=method,. ~/Code/seaborn/seaborn/matrix.py in __init__(self, data, pivot_kws, z_score, standard_scale, figsize, row_colors, col_colors, mask). 772 . 773 self.row_colors, self.row_color_labels = \. --> 774 self._preprocess_colors(data, row_colors, axis=0). 775 self.col_colors, self.col_color_labels = \. 776 self._preprocess_colors(data, col_colors, axis=1). ~/Code/seaborn/seaborn/matrix.py in _preprocess_colors(self, data, colors, axis). 827 # Replace na's with background color. 828 # TODO We should set these to transparent instead. --> 829 colors = colors.fillna('white'). 830 . 831 # Extract color values and labels from frame/series. /usr/local/lib/python3.7/site-packages/pandas/core/series.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs). 4343 limit=limit,. 4344 downcast=d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/pull/809:3943,usability,simpl,simply,3943,"args. 4346 ). 4347 . /usr/local/lib/python3.7/site-packages/pandas/core/generic.py in fillna(self, value, method, axis, inplace, limit, downcast). 6256 . 6257 new_data = self._data.fillna(. -> 6258 value=value, limit=limit, inplace=inplace, downcast=downcast. 6259 ). 6260 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in fillna(self, **kwargs). 573 . 574 def fillna(self, **kwargs):. --> 575 return self.apply(""fillna"", **kwargs). 576 . 577 def downcast(self, **kwargs):. /usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs). 436 kwargs[k] = obj.reindex(b_items, axis=axis, copy=align_copy). 437 . --> 438 applied = getattr(b, f)(**kwargs). 439 result_blocks = _extend_blocks(applied, result_blocks). 440 . /usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py in fillna(self, value, limit, inplace, downcast). 1934 def fillna(self, value, limit=None, inplace=False, downcast=None):. 1935 values = self.values if inplace else self.values.copy(). -> 1936 values = values.fillna(value=value, limit=limit). 1937 return [. 1938 self.make_block_same_class(. /usr/local/lib/python3.7/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs). 206 else:. 207 kwargs[new_arg_name] = new_arg_value. --> 208 return func(*args, **kwargs). 209 . 210 return wrapper. /usr/local/lib/python3.7/site-packages/pandas/core/arrays/categorical.py in fillna(self, value, method, limit). 1871 elif is_hashable(value):. 1872 if not isna(value) and value not in self.categories:. -> 1873 raise ValueError(""fill value must be in categories""). 1874 . 1875 mask = codes == -1. ValueError: fill value must be in categories. ```. That's because `colors = colors.fillna('white')` line in the seaborn code is trying to add a new category to a categorical variable, which is not allowed in pandas. I simply converted the color categorical variable to numpy array and added tests.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/809
https://github.com/scverse/scanpy/issues/810:101,performance,time,time,101,"compute connectivity runtime; Hi, do you have any analysis / approximate figures or examples for run time of pp.neighbors split into the time taking to find the neighbors (compute_neighbors_umap) and compute the connectivities (compute_connectivites_umap). . Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/810
https://github.com/scverse/scanpy/issues/810:137,performance,time,time,137,"compute connectivity runtime; Hi, do you have any analysis / approximate figures or examples for run time of pp.neighbors split into the time taking to find the neighbors (compute_neighbors_umap) and compute the connectivities (compute_connectivites_umap). . Thanks",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/810
https://github.com/scverse/scanpy/issues/811:141,availability,cluster,cluster,141,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:211,availability,error,error,211,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:660,availability,error,error,660,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:141,deployability,cluster,cluster,141,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:166,deployability,releas,released,166,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:394,integrability,filter,filtered,394,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:511,interoperability,standard,standard-,511,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:276,modifiability,Variab,Variable,276,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:0,performance,Memor,Memory,0,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:204,performance,memor,memory,204,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:211,performance,error,error,211,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:500,performance,memor,memory,500,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:660,performance,error,error,660,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:211,safety,error,error,211,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:427,safety,detect,detectedin,427,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:660,safety,error,error,660,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:427,security,detect,detectedin,427,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:600,testability,regress,regression,600,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:0,usability,Memor,Memory,0,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:204,usability,memor,memory,204,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:211,usability,error,error,211,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:500,usability,memor,memory,500,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/issues/811:660,usability,error,error,660,"Memory issue running zheng17 on ~1M cells; Running https://github.com/theislab/scanpy_usage/blob/master/170522_visualizing_one_million_cells/cluster.py on the latest released Scanpy (1.4.4.post1) gives a memory error:. ```. reading 1M_neurons_filtered_gene_bc_matrices_h5.h5. Variable names are not unique. To make them unique, call `.var_names_make_unique`. (0:01:39). running recipe zheng17. filtered out 3983 genes that are detectedin less than 1 counts. Killed. ```. This is running with 60GB of memory (n1-standard-16), but also occurs with 104GB (n1-highmem-16). It looks like there has been a regression somewhere since this used to run OK. I think the error may be happening in anndata.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/811
https://github.com/scverse/scanpy/pull/812:463,integrability,filter,filter,463,"The method for annotating genes with cell types; With this PR we propose our new annotation method to Scannpy:. Annotator marks the data with cell type annotations based on marker genes. Over-expressed genes are selected with the Mann-Whitney U tests and cell. types are assigned with the hypergeometric test. This function first selects. genes from gene expression data with the Mann Whitney U test, then annotate. them with the hypergeometric test, and finally filter out cell types that. have zero scores for all cells. The results are scores that tell how. probable each cell type is for each cell. Hope you like the method and merge it to Scampy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812
https://github.com/scverse/scanpy/pull/812:245,safety,test,tests,245,"The method for annotating genes with cell types; With this PR we propose our new annotation method to Scannpy:. Annotator marks the data with cell type annotations based on marker genes. Over-expressed genes are selected with the Mann-Whitney U tests and cell. types are assigned with the hypergeometric test. This function first selects. genes from gene expression data with the Mann Whitney U test, then annotate. them with the hypergeometric test, and finally filter out cell types that. have zero scores for all cells. The results are scores that tell how. probable each cell type is for each cell. Hope you like the method and merge it to Scampy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812
https://github.com/scverse/scanpy/pull/812:304,safety,test,test,304,"The method for annotating genes with cell types; With this PR we propose our new annotation method to Scannpy:. Annotator marks the data with cell type annotations based on marker genes. Over-expressed genes are selected with the Mann-Whitney U tests and cell. types are assigned with the hypergeometric test. This function first selects. genes from gene expression data with the Mann Whitney U test, then annotate. them with the hypergeometric test, and finally filter out cell types that. have zero scores for all cells. The results are scores that tell how. probable each cell type is for each cell. Hope you like the method and merge it to Scampy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812
https://github.com/scverse/scanpy/pull/812:395,safety,test,test,395,"The method for annotating genes with cell types; With this PR we propose our new annotation method to Scannpy:. Annotator marks the data with cell type annotations based on marker genes. Over-expressed genes are selected with the Mann-Whitney U tests and cell. types are assigned with the hypergeometric test. This function first selects. genes from gene expression data with the Mann Whitney U test, then annotate. them with the hypergeometric test, and finally filter out cell types that. have zero scores for all cells. The results are scores that tell how. probable each cell type is for each cell. Hope you like the method and merge it to Scampy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812
https://github.com/scverse/scanpy/pull/812:445,safety,test,test,445,"The method for annotating genes with cell types; With this PR we propose our new annotation method to Scannpy:. Annotator marks the data with cell type annotations based on marker genes. Over-expressed genes are selected with the Mann-Whitney U tests and cell. types are assigned with the hypergeometric test. This function first selects. genes from gene expression data with the Mann Whitney U test, then annotate. them with the hypergeometric test, and finally filter out cell types that. have zero scores for all cells. The results are scores that tell how. probable each cell type is for each cell. Hope you like the method and merge it to Scampy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812
https://github.com/scverse/scanpy/pull/812:245,testability,test,tests,245,"The method for annotating genes with cell types; With this PR we propose our new annotation method to Scannpy:. Annotator marks the data with cell type annotations based on marker genes. Over-expressed genes are selected with the Mann-Whitney U tests and cell. types are assigned with the hypergeometric test. This function first selects. genes from gene expression data with the Mann Whitney U test, then annotate. them with the hypergeometric test, and finally filter out cell types that. have zero scores for all cells. The results are scores that tell how. probable each cell type is for each cell. Hope you like the method and merge it to Scampy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812
https://github.com/scverse/scanpy/pull/812:304,testability,test,test,304,"The method for annotating genes with cell types; With this PR we propose our new annotation method to Scannpy:. Annotator marks the data with cell type annotations based on marker genes. Over-expressed genes are selected with the Mann-Whitney U tests and cell. types are assigned with the hypergeometric test. This function first selects. genes from gene expression data with the Mann Whitney U test, then annotate. them with the hypergeometric test, and finally filter out cell types that. have zero scores for all cells. The results are scores that tell how. probable each cell type is for each cell. Hope you like the method and merge it to Scampy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812
https://github.com/scverse/scanpy/pull/812:395,testability,test,test,395,"The method for annotating genes with cell types; With this PR we propose our new annotation method to Scannpy:. Annotator marks the data with cell type annotations based on marker genes. Over-expressed genes are selected with the Mann-Whitney U tests and cell. types are assigned with the hypergeometric test. This function first selects. genes from gene expression data with the Mann Whitney U test, then annotate. them with the hypergeometric test, and finally filter out cell types that. have zero scores for all cells. The results are scores that tell how. probable each cell type is for each cell. Hope you like the method and merge it to Scampy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812
https://github.com/scverse/scanpy/pull/812:445,testability,test,test,445,"The method for annotating genes with cell types; With this PR we propose our new annotation method to Scannpy:. Annotator marks the data with cell type annotations based on marker genes. Over-expressed genes are selected with the Mann-Whitney U tests and cell. types are assigned with the hypergeometric test. This function first selects. genes from gene expression data with the Mann Whitney U test, then annotate. them with the hypergeometric test, and finally filter out cell types that. have zero scores for all cells. The results are scores that tell how. probable each cell type is for each cell. Hope you like the method and merge it to Scampy.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/812
https://github.com/scverse/scanpy/pull/813:59,energy efficiency,current,currently,59,"Make the dca learning rate accessible from scanpy.; scanpy currently does not allow to select the dca learning rate. This can create convergence issues (for example dca diverges 60% of the time on Tabula Muris when trained on CPU, but oddly enough converges when trained on GPU)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/813
https://github.com/scverse/scanpy/pull/813:226,energy efficiency,CPU,CPU,226,"Make the dca learning rate accessible from scanpy.; scanpy currently does not allow to select the dca learning rate. This can create convergence issues (for example dca diverges 60% of the time on Tabula Muris when trained on CPU, but oddly enough converges when trained on GPU)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/813
https://github.com/scverse/scanpy/pull/813:274,energy efficiency,GPU,GPU,274,"Make the dca learning rate accessible from scanpy.; scanpy currently does not allow to select the dca learning rate. This can create convergence issues (for example dca diverges 60% of the time on Tabula Muris when trained on CPU, but oddly enough converges when trained on GPU)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/813
https://github.com/scverse/scanpy/pull/813:189,performance,time,time,189,"Make the dca learning rate accessible from scanpy.; scanpy currently does not allow to select the dca learning rate. This can create convergence issues (for example dca diverges 60% of the time on Tabula Muris when trained on CPU, but oddly enough converges when trained on GPU)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/813
https://github.com/scverse/scanpy/pull/813:226,performance,CPU,CPU,226,"Make the dca learning rate accessible from scanpy.; scanpy currently does not allow to select the dca learning rate. This can create convergence issues (for example dca diverges 60% of the time on Tabula Muris when trained on CPU, but oddly enough converges when trained on GPU)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/813
https://github.com/scverse/scanpy/pull/813:274,performance,GPU,GPU,274,"Make the dca learning rate accessible from scanpy.; scanpy currently does not allow to select the dca learning rate. This can create convergence issues (for example dca diverges 60% of the time on Tabula Muris when trained on CPU, but oddly enough converges when trained on GPU)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/813
https://github.com/scverse/scanpy/pull/813:69,reliability,doe,does,69,"Make the dca learning rate accessible from scanpy.; scanpy currently does not allow to select the dca learning rate. This can create convergence issues (for example dca diverges 60% of the time on Tabula Muris when trained on CPU, but oddly enough converges when trained on GPU)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/813
https://github.com/scverse/scanpy/pull/813:27,security,access,accessible,27,"Make the dca learning rate accessible from scanpy.; scanpy currently does not allow to select the dca learning rate. This can create convergence issues (for example dca diverges 60% of the time on Tabula Muris when trained on CPU, but oddly enough converges when trained on GPU)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/813
https://github.com/scverse/scanpy/pull/813:13,usability,learn,learning,13,"Make the dca learning rate accessible from scanpy.; scanpy currently does not allow to select the dca learning rate. This can create convergence issues (for example dca diverges 60% of the time on Tabula Muris when trained on CPU, but oddly enough converges when trained on GPU)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/813
https://github.com/scverse/scanpy/pull/813:102,usability,learn,learning,102,"Make the dca learning rate accessible from scanpy.; scanpy currently does not allow to select the dca learning rate. This can create convergence issues (for example dca diverges 60% of the time on Tabula Muris when trained on CPU, but oddly enough converges when trained on GPU)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/813
https://github.com/scverse/scanpy/pull/814:94,deployability,upgrad,upgrade,94,Moved external tools to scanpy.external.tl; This also highlights a problem: pypairs had a big upgrade in the meantime and we didnt see it. Its docs are now much better than ours were (and still a bit more complete after this PR) IDK how to prevent this,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/814
https://github.com/scverse/scanpy/pull/814:94,modifiability,upgrad,upgrade,94,Moved external tools to scanpy.external.tl; This also highlights a problem: pypairs had a big upgrade in the meantime and we didnt see it. Its docs are now much better than ours were (and still a bit more complete after this PR) IDK how to prevent this,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/814
https://github.com/scverse/scanpy/pull/814:206,safety,compl,complete,206,Moved external tools to scanpy.external.tl; This also highlights a problem: pypairs had a big upgrade in the meantime and we didnt see it. Its docs are now much better than ours were (and still a bit more complete after this PR) IDK how to prevent this,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/814
https://github.com/scverse/scanpy/pull/814:241,safety,prevent,prevent,241,Moved external tools to scanpy.external.tl; This also highlights a problem: pypairs had a big upgrade in the meantime and we didnt see it. Its docs are now much better than ours were (and still a bit more complete after this PR) IDK how to prevent this,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/814
https://github.com/scverse/scanpy/pull/814:206,security,compl,complete,206,Moved external tools to scanpy.external.tl; This also highlights a problem: pypairs had a big upgrade in the meantime and we didnt see it. Its docs are now much better than ours were (and still a bit more complete after this PR) IDK how to prevent this,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/814
https://github.com/scverse/scanpy/pull/814:241,security,preven,prevent,241,Moved external tools to scanpy.external.tl; This also highlights a problem: pypairs had a big upgrade in the meantime and we didnt see it. Its docs are now much better than ours were (and still a bit more complete after this PR) IDK how to prevent this,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/814
https://github.com/scverse/scanpy/pull/814:15,usability,tool,tools,15,Moved external tools to scanpy.external.tl; This also highlights a problem: pypairs had a big upgrade in the meantime and we didnt see it. Its docs are now much better than ours were (and still a bit more complete after this PR) IDK how to prevent this,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/814
https://github.com/scverse/scanpy/issues/815:773,deployability,Version,Versions,773,"Show scatterplots with colorbar side-by-side; Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. . I tried it using `subplots`, but this is what happens: . ```python. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```. ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. ### Full repex. ```python. import scanpy as sc. import numpy as np. from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1). adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) . p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815
https://github.com/scverse/scanpy/issues/815:170,integrability,sub,subplots,170,"Show scatterplots with colorbar side-by-side; Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. . I tried it using `subplots`, but this is what happens: . ```python. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```. ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. ### Full repex. ```python. import scanpy as sc. import numpy as np. from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1). adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) . p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815
https://github.com/scverse/scanpy/issues/815:242,integrability,sub,subplots,242,"Show scatterplots with colorbar side-by-side; Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. . I tried it using `subplots`, but this is what happens: . ```python. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```. ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. ### Full repex. ```python. import scanpy as sc. import numpy as np. from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1). adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) . p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815
https://github.com/scverse/scanpy/issues/815:703,integrability,sub,subplots,703,"Show scatterplots with colorbar side-by-side; Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. . I tried it using `subplots`, but this is what happens: . ```python. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```. ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. ### Full repex. ```python. import scanpy as sc. import numpy as np. from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1). adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) . p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815
https://github.com/scverse/scanpy/issues/815:773,integrability,Version,Versions,773,"Show scatterplots with colorbar side-by-side; Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. . I tried it using `subplots`, but this is what happens: . ```python. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```. ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. ### Full repex. ```python. import scanpy as sc. import numpy as np. from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1). adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) . p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815
https://github.com/scverse/scanpy/issues/815:1528,integrability,sub,subplots,1528,"Show scatterplots with colorbar side-by-side; Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. . I tried it using `subplots`, but this is what happens: . ```python. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```. ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. ### Full repex. ```python. import scanpy as sc. import numpy as np. from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1). adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) . p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815
https://github.com/scverse/scanpy/issues/815:1241,interoperability,standard,standard,1241,"Show scatterplots with colorbar side-by-side; Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. . I tried it using `subplots`, but this is what happens: . ```python. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```. ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. ### Full repex. ```python. import scanpy as sc. import numpy as np. from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1). adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) . p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815
https://github.com/scverse/scanpy/issues/815:773,modifiability,Version,Versions,773,"Show scatterplots with colorbar side-by-side; Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. . I tried it using `subplots`, but this is what happens: . ```python. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```. ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. ### Full repex. ```python. import scanpy as sc. import numpy as np. from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1). adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) . p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815
https://github.com/scverse/scanpy/issues/815:713,reliability,doe,doesn,713,"Show scatterplots with colorbar side-by-side; Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. . I tried it using `subplots`, but this is what happens: . ```python. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```. ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. ### Full repex. ```python. import scanpy as sc. import numpy as np. from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1). adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) . p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815
https://github.com/scverse/scanpy/issues/815:87,security,control,control,87,"Show scatterplots with colorbar side-by-side; Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. . I tried it using `subplots`, but this is what happens: . ```python. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```. ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. ### Full repex. ```python. import scanpy as sc. import numpy as np. from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1). adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) . p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815
https://github.com/scverse/scanpy/issues/815:87,testability,control,control,87,"Show scatterplots with colorbar side-by-side; Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. . I tried it using `subplots`, but this is what happens: . ```python. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```. ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. ### Full repex. ```python. import scanpy as sc. import numpy as np. from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1). adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) . p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815
https://github.com/scverse/scanpy/issues/815:532,usability,user,user-images,532,"Show scatterplots with colorbar side-by-side; Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. . I tried it using `subplots`, but this is what happens: . ```python. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```. ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. ### Full repex. ```python. import scanpy as sc. import numpy as np. from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1). adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) . p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815
https://github.com/scverse/scanpy/issues/815:886,usability,learn,learn,886,"Show scatterplots with colorbar side-by-side; Hi, . I created scatterplots for quality control and want to show them side-by-side to save some space. . I tried it using `subplots`, but this is what happens: . ```python. fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```. ![2019-09-05_16:57:05_997x433](https://user-images.githubusercontent.com/7051479/64353530-33403700-cffe-11e9-8527-5b8aaac1a9b1.png). The problem seems to be that `pl.scatter` adds an additional `axes` of that `subplots` doesn't know anything. Is there an easy fix for that? . ### Versions. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.1 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. ### Full repex. ```python. import scanpy as sc. import numpy as np. from matplotlib import pyplot as plt. adata = sc.read_h5ad(""data/pbmc3k_raw.h5ad""). adata.obs['n_counts'] = adata.X.sum(axis=1). adata.obs['n_genes'] = (adata.X != 0).sum(axis=1). # show plots below each other (standard, works) . p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False). # try to show plots side-by-side (looks weird) . fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5)). p1 = sc.pl.scatter(adata, x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax1). p2 = sc.pl.scatter(adata[adata.obs['n_counts'] < 10000], x='n_counts', y='n_genes', color='n_genes', show=False, ax=ax2). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/815
https://github.com/scverse/scanpy/pull/816:70,performance,memor,memory,70,fix zheng17; This fixes #811 and should fix #511. The analysis of the memory requirments is in the second issue.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/816
https://github.com/scverse/scanpy/pull/816:70,usability,memor,memory,70,fix zheng17; This fixes #811 and should fix #511. The analysis of the memory requirments is in the second issue.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/816
https://github.com/scverse/scanpy/issues/817:47,availability,sli,sliced,47,"rank_genes_groups wrong gene name; Hi,. I have sliced some candidate genes (according to my pre-knowledge) from adata, and do sc.tl.rank_genes_groups for adata to check those genes are enriched in which group of cells. But the output rank gene names is wrong, many of the ouptput genes names are not the in adata.var.index after my selection, which should be already excluded by my candidate genes selection. Bellow is my code, seems still use the genes before selection? Can you help me? Thanks,. Jphe. ```py. adata_raw = adata.copy(). df = pd.read_table('/public/home/jphe/omicsdata/genome/mm10/scTE/atac/candidates.txt', header=None). genes = list(df[0]). genes = [k for k in genes if k in adata.var.index ]. adata = adata[:, genes] # only have ~1000 genes in adata after selection. sc.tl.rank_genes_groups(adata, 'leiden_r0.5', n_genes=20). sc.pl.rank_genes_groups(adata, n_genes=20, show=True). adata. ```. ```. AnnData object with n_obs  n_vars = 53165  1080 . obs: 'batch', 'n_counts', 'n_genes', 'time', 'log_counts', 'mt_frac', 'size_factors', 'leiden_r1', 'leiden_r0.5', 'leiden_r0.1'. var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. uns: 'leiden', 'neighbors', 'pca', 'time_colors', 'leiden_r0.5_colors', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap'. varm: 'PCs'. layers: 'counts'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/817
https://github.com/scverse/scanpy/issues/817:558,integrability,pub,public,558,"rank_genes_groups wrong gene name; Hi,. I have sliced some candidate genes (according to my pre-knowledge) from adata, and do sc.tl.rank_genes_groups for adata to check those genes are enriched in which group of cells. But the output rank gene names is wrong, many of the ouptput genes names are not the in adata.var.index after my selection, which should be already excluded by my candidate genes selection. Bellow is my code, seems still use the genes before selection? Can you help me? Thanks,. Jphe. ```py. adata_raw = adata.copy(). df = pd.read_table('/public/home/jphe/omicsdata/genome/mm10/scTE/atac/candidates.txt', header=None). genes = list(df[0]). genes = [k for k in genes if k in adata.var.index ]. adata = adata[:, genes] # only have ~1000 genes in adata after selection. sc.tl.rank_genes_groups(adata, 'leiden_r0.5', n_genes=20). sc.pl.rank_genes_groups(adata, n_genes=20, show=True). adata. ```. ```. AnnData object with n_obs  n_vars = 53165  1080 . obs: 'batch', 'n_counts', 'n_genes', 'time', 'log_counts', 'mt_frac', 'size_factors', 'leiden_r1', 'leiden_r0.5', 'leiden_r0.1'. var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. uns: 'leiden', 'neighbors', 'pca', 'time_colors', 'leiden_r0.5_colors', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap'. varm: 'PCs'. layers: 'counts'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/817
https://github.com/scverse/scanpy/issues/817:975,integrability,batch,batch,975,"rank_genes_groups wrong gene name; Hi,. I have sliced some candidate genes (according to my pre-knowledge) from adata, and do sc.tl.rank_genes_groups for adata to check those genes are enriched in which group of cells. But the output rank gene names is wrong, many of the ouptput genes names are not the in adata.var.index after my selection, which should be already excluded by my candidate genes selection. Bellow is my code, seems still use the genes before selection? Can you help me? Thanks,. Jphe. ```py. adata_raw = adata.copy(). df = pd.read_table('/public/home/jphe/omicsdata/genome/mm10/scTE/atac/candidates.txt', header=None). genes = list(df[0]). genes = [k for k in genes if k in adata.var.index ]. adata = adata[:, genes] # only have ~1000 genes in adata after selection. sc.tl.rank_genes_groups(adata, 'leiden_r0.5', n_genes=20). sc.pl.rank_genes_groups(adata, n_genes=20, show=True). adata. ```. ```. AnnData object with n_obs  n_vars = 53165  1080 . obs: 'batch', 'n_counts', 'n_genes', 'time', 'log_counts', 'mt_frac', 'size_factors', 'leiden_r1', 'leiden_r0.5', 'leiden_r0.1'. var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. uns: 'leiden', 'neighbors', 'pca', 'time_colors', 'leiden_r0.5_colors', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap'. varm: 'PCs'. layers: 'counts'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/817
https://github.com/scverse/scanpy/issues/817:1318,modifiability,layer,layers,1318,"rank_genes_groups wrong gene name; Hi,. I have sliced some candidate genes (according to my pre-knowledge) from adata, and do sc.tl.rank_genes_groups for adata to check those genes are enriched in which group of cells. But the output rank gene names is wrong, many of the ouptput genes names are not the in adata.var.index after my selection, which should be already excluded by my candidate genes selection. Bellow is my code, seems still use the genes before selection? Can you help me? Thanks,. Jphe. ```py. adata_raw = adata.copy(). df = pd.read_table('/public/home/jphe/omicsdata/genome/mm10/scTE/atac/candidates.txt', header=None). genes = list(df[0]). genes = [k for k in genes if k in adata.var.index ]. adata = adata[:, genes] # only have ~1000 genes in adata after selection. sc.tl.rank_genes_groups(adata, 'leiden_r0.5', n_genes=20). sc.pl.rank_genes_groups(adata, n_genes=20, show=True). adata. ```. ```. AnnData object with n_obs  n_vars = 53165  1080 . obs: 'batch', 'n_counts', 'n_genes', 'time', 'log_counts', 'mt_frac', 'size_factors', 'leiden_r1', 'leiden_r0.5', 'leiden_r0.1'. var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. uns: 'leiden', 'neighbors', 'pca', 'time_colors', 'leiden_r0.5_colors', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap'. varm: 'PCs'. layers: 'counts'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/817
https://github.com/scverse/scanpy/issues/817:975,performance,batch,batch,975,"rank_genes_groups wrong gene name; Hi,. I have sliced some candidate genes (according to my pre-knowledge) from adata, and do sc.tl.rank_genes_groups for adata to check those genes are enriched in which group of cells. But the output rank gene names is wrong, many of the ouptput genes names are not the in adata.var.index after my selection, which should be already excluded by my candidate genes selection. Bellow is my code, seems still use the genes before selection? Can you help me? Thanks,. Jphe. ```py. adata_raw = adata.copy(). df = pd.read_table('/public/home/jphe/omicsdata/genome/mm10/scTE/atac/candidates.txt', header=None). genes = list(df[0]). genes = [k for k in genes if k in adata.var.index ]. adata = adata[:, genes] # only have ~1000 genes in adata after selection. sc.tl.rank_genes_groups(adata, 'leiden_r0.5', n_genes=20). sc.pl.rank_genes_groups(adata, n_genes=20, show=True). adata. ```. ```. AnnData object with n_obs  n_vars = 53165  1080 . obs: 'batch', 'n_counts', 'n_genes', 'time', 'log_counts', 'mt_frac', 'size_factors', 'leiden_r1', 'leiden_r0.5', 'leiden_r0.1'. var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. uns: 'leiden', 'neighbors', 'pca', 'time_colors', 'leiden_r0.5_colors', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap'. varm: 'PCs'. layers: 'counts'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/817
https://github.com/scverse/scanpy/issues/817:1007,performance,time,time,1007,"rank_genes_groups wrong gene name; Hi,. I have sliced some candidate genes (according to my pre-knowledge) from adata, and do sc.tl.rank_genes_groups for adata to check those genes are enriched in which group of cells. But the output rank gene names is wrong, many of the ouptput genes names are not the in adata.var.index after my selection, which should be already excluded by my candidate genes selection. Bellow is my code, seems still use the genes before selection? Can you help me? Thanks,. Jphe. ```py. adata_raw = adata.copy(). df = pd.read_table('/public/home/jphe/omicsdata/genome/mm10/scTE/atac/candidates.txt', header=None). genes = list(df[0]). genes = [k for k in genes if k in adata.var.index ]. adata = adata[:, genes] # only have ~1000 genes in adata after selection. sc.tl.rank_genes_groups(adata, 'leiden_r0.5', n_genes=20). sc.pl.rank_genes_groups(adata, n_genes=20, show=True). adata. ```. ```. AnnData object with n_obs  n_vars = 53165  1080 . obs: 'batch', 'n_counts', 'n_genes', 'time', 'log_counts', 'mt_frac', 'size_factors', 'leiden_r1', 'leiden_r0.5', 'leiden_r0.1'. var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. uns: 'leiden', 'neighbors', 'pca', 'time_colors', 'leiden_r0.5_colors', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap'. varm: 'PCs'. layers: 'counts'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/817
https://github.com/scverse/scanpy/issues/817:47,reliability,sli,sliced,47,"rank_genes_groups wrong gene name; Hi,. I have sliced some candidate genes (according to my pre-knowledge) from adata, and do sc.tl.rank_genes_groups for adata to check those genes are enriched in which group of cells. But the output rank gene names is wrong, many of the ouptput genes names are not the in adata.var.index after my selection, which should be already excluded by my candidate genes selection. Bellow is my code, seems still use the genes before selection? Can you help me? Thanks,. Jphe. ```py. adata_raw = adata.copy(). df = pd.read_table('/public/home/jphe/omicsdata/genome/mm10/scTE/atac/candidates.txt', header=None). genes = list(df[0]). genes = [k for k in genes if k in adata.var.index ]. adata = adata[:, genes] # only have ~1000 genes in adata after selection. sc.tl.rank_genes_groups(adata, 'leiden_r0.5', n_genes=20). sc.pl.rank_genes_groups(adata, n_genes=20, show=True). adata. ```. ```. AnnData object with n_obs  n_vars = 53165  1080 . obs: 'batch', 'n_counts', 'n_genes', 'time', 'log_counts', 'mt_frac', 'size_factors', 'leiden_r1', 'leiden_r0.5', 'leiden_r0.1'. var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. uns: 'leiden', 'neighbors', 'pca', 'time_colors', 'leiden_r0.5_colors', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap'. varm: 'PCs'. layers: 'counts'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/817
https://github.com/scverse/scanpy/issues/817:480,usability,help,help,480,"rank_genes_groups wrong gene name; Hi,. I have sliced some candidate genes (according to my pre-knowledge) from adata, and do sc.tl.rank_genes_groups for adata to check those genes are enriched in which group of cells. But the output rank gene names is wrong, many of the ouptput genes names are not the in adata.var.index after my selection, which should be already excluded by my candidate genes selection. Bellow is my code, seems still use the genes before selection? Can you help me? Thanks,. Jphe. ```py. adata_raw = adata.copy(). df = pd.read_table('/public/home/jphe/omicsdata/genome/mm10/scTE/atac/candidates.txt', header=None). genes = list(df[0]). genes = [k for k in genes if k in adata.var.index ]. adata = adata[:, genes] # only have ~1000 genes in adata after selection. sc.tl.rank_genes_groups(adata, 'leiden_r0.5', n_genes=20). sc.pl.rank_genes_groups(adata, n_genes=20, show=True). adata. ```. ```. AnnData object with n_obs  n_vars = 53165  1080 . obs: 'batch', 'n_counts', 'n_genes', 'time', 'log_counts', 'mt_frac', 'size_factors', 'leiden_r1', 'leiden_r0.5', 'leiden_r0.1'. var: 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'. uns: 'leiden', 'neighbors', 'pca', 'time_colors', 'leiden_r0.5_colors', 'rank_genes_groups'. obsm: 'X_pca', 'X_tsne', 'X_umap'. varm: 'PCs'. layers: 'counts'. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/817
https://github.com/scverse/scanpy/issues/818:0,usability,interact,interactive,0,"interactive visualization; Dear,. Could scanpy provide some interactive visulaization tools ? So that non-bioinformaticans can also look up the results outuputed by scanpy, just like the following link:. https://scDissector.org/martin.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/818
https://github.com/scverse/scanpy/issues/818:12,usability,visual,visualization,12,"interactive visualization; Dear,. Could scanpy provide some interactive visulaization tools ? So that non-bioinformaticans can also look up the results outuputed by scanpy, just like the following link:. https://scDissector.org/martin.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/818
https://github.com/scverse/scanpy/issues/818:60,usability,interact,interactive,60,"interactive visualization; Dear,. Could scanpy provide some interactive visulaization tools ? So that non-bioinformaticans can also look up the results outuputed by scanpy, just like the following link:. https://scDissector.org/martin.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/818
https://github.com/scverse/scanpy/issues/818:86,usability,tool,tools,86,"interactive visualization; Dear,. Could scanpy provide some interactive visulaization tools ? So that non-bioinformaticans can also look up the results outuputed by scanpy, just like the following link:. https://scDissector.org/martin.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/818
https://github.com/scverse/scanpy/pull/819:26,availability,avail,available,26,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:15,deployability,modul,modularity,15,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:65,deployability,modul,modularity,65,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:15,integrability,modular,modularity,15,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:65,integrability,modular,modularity,65,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:626,integrability,messag,messages,626,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:626,interoperability,messag,messages,626,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:15,modifiability,modul,modularity,15,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:65,modifiability,modul,modularity,65,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:200,modifiability,paramet,parameters,200,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:26,reliability,availab,available,26,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:15,safety,modul,modularity,15,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:26,safety,avail,available,26,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:65,safety,modul,modularity,65,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:26,security,availab,available,26,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:15,testability,modula,modularity,15,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:65,testability,modula,modularity,65,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/819:498,usability,user,user-images,498,"Make partition modularity available in adata.uns; This PR stores modularity metric into adata.uns. I also added `key_added` as a suffix to uns key ""louvain"", so that we can run louvain with different parameters and store their quality metrics separately. ```python. import scanpy as sc. sc.settings.verbosity = 3. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata, svd_solver='arpack'). sc.pp.neighbors(adata). sc.tl.louvain(adata, resolution=2.0, key_added='newkey'). adata.uns. ```. ![image](https://user-images.githubusercontent.com/1140359/64479798-2a7c6c00-d18a-11e9-9c83-179ddfc54a8a.png). I'm really bad at writing warning messages and naming things, so feel free to edit these :).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/819
https://github.com/scverse/scanpy/pull/820:368,availability,avail,available,368,"Make write_knn_indices option of Neighbors.compute_neighbors() public; `compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python. import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)). lm.rows = adata.uns['neighbors']['knn_indices']. lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']). lm = lm.tocsr(). lm.setdiag(0). lm.eliminate_zeros(). lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm). adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/pull/820:913,availability,mask,mask,913,"Make write_knn_indices option of Neighbors.compute_neighbors() public; `compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python. import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)). lm.rows = adata.uns['neighbors']['knn_indices']. lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']). lm = lm.tocsr(). lm.setdiag(0). lm.eliminate_zeros(). lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm). adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/pull/820:590,deployability,build,building,590,"Make write_knn_indices option of Neighbors.compute_neighbors() public; `compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python. import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)). lm.rows = adata.uns['neighbors']['knn_indices']. lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']). lm = lm.tocsr(). lm.setdiag(0). lm.eliminate_zeros(). lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm). adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/pull/820:903,deployability,build,build,903,"Make write_knn_indices option of Neighbors.compute_neighbors() public; `compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python. import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)). lm.rows = adata.uns['neighbors']['knn_indices']. lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']). lm = lm.tocsr(). lm.setdiag(0). lm.eliminate_zeros(). lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm). adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/pull/820:573,energy efficiency,cool,cool,573,"Make write_knn_indices option of Neighbors.compute_neighbors() public; `compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python. import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)). lm.rows = adata.uns['neighbors']['knn_indices']. lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']). lm = lm.tocsr(). lm.setdiag(0). lm.eliminate_zeros(). lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm). adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/pull/820:63,integrability,pub,public,63,"Make write_knn_indices option of Neighbors.compute_neighbors() public; `compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python. import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)). lm.rows = adata.uns['neighbors']['knn_indices']. lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']). lm = lm.tocsr(). lm.setdiag(0). lm.eliminate_zeros(). lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm). adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/pull/820:368,reliability,availab,available,368,"Make write_knn_indices option of Neighbors.compute_neighbors() public; `compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python. import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)). lm.rows = adata.uns['neighbors']['knn_indices']. lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']). lm = lm.tocsr(). lm.setdiag(0). lm.eliminate_zeros(). lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm). adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/pull/820:368,safety,avail,available,368,"Make write_knn_indices option of Neighbors.compute_neighbors() public; `compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python. import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)). lm.rows = adata.uns['neighbors']['knn_indices']. lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']). lm = lm.tocsr(). lm.setdiag(0). lm.eliminate_zeros(). lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm). adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/pull/820:257,security,access,accessible,257,"Make write_knn_indices option of Neighbors.compute_neighbors() public; `compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python. import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)). lm.rows = adata.uns['neighbors']['knn_indices']. lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']). lm = lm.tocsr(). lm.setdiag(0). lm.eliminate_zeros(). lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm). adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/pull/820:368,security,availab,available,368,"Make write_knn_indices option of Neighbors.compute_neighbors() public; `compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python. import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)). lm.rows = adata.uns['neighbors']['knn_indices']. lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']). lm = lm.tocsr(). lm.setdiag(0). lm.eliminate_zeros(). lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm). adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/pull/820:407,security,access,access,407,"Make write_knn_indices option of Neighbors.compute_neighbors() public; `compute_neighbors` method of the Neighbors class has a nice option called `write_knn_indices` which saves kNN indices into the Neighbors object. But this object and its members are not accessible from sc.pp.neighbors since we don't save the reference to it. . Making the write_knn_indices option available from `sc.pp.neighbors` gives access to knn indices which provide additional information on top of connectivities and distances since these matrices are symmetrized. . With knn_indices one can do cool things like building a mutual kNN graph, e.g.:. ```python. import scipy.sparse as sp. lm = sp.lil_matrix((adata.n_obs, adata.n_obs)). lm.rows = adata.uns['neighbors']['knn_indices']. lm.data = np.ones_like(adata.uns['neighbors']['knn_indices']). lm = lm.tocsr(). lm.setdiag(0). lm.eliminate_zeros(). lm = lm.multiply(lm.T) # build mnn mask. adata.uns['neighbors']['distances'] = adata.uns['neighbors']['distances'].multiply(lm). adata.uns['neighbors']['connectivities'] = adata.uns['neighbors']['connectivities'].multiply(lm). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/820
https://github.com/scverse/scanpy/issues/821:97,deployability,pipelin,pipeline,97,"Stimulated vs Control analysisi; Hello, scanpy developers, . Firstly, thank you so much for your pipeline scanpy. It is very useful to me. Secondly, I want to ask a question. Are you going to support Stimulated vs Control analysis? Thank you. Sincerely yours,. Shangyu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821
https://github.com/scverse/scanpy/issues/821:97,integrability,pipelin,pipeline,97,"Stimulated vs Control analysisi; Hello, scanpy developers, . Firstly, thank you so much for your pipeline scanpy. It is very useful to me. Secondly, I want to ask a question. Are you going to support Stimulated vs Control analysis? Thank you. Sincerely yours,. Shangyu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821
https://github.com/scverse/scanpy/issues/821:14,security,Control,Control,14,"Stimulated vs Control analysisi; Hello, scanpy developers, . Firstly, thank you so much for your pipeline scanpy. It is very useful to me. Secondly, I want to ask a question. Are you going to support Stimulated vs Control analysis? Thank you. Sincerely yours,. Shangyu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821
https://github.com/scverse/scanpy/issues/821:214,security,Control,Control,214,"Stimulated vs Control analysisi; Hello, scanpy developers, . Firstly, thank you so much for your pipeline scanpy. It is very useful to me. Secondly, I want to ask a question. Are you going to support Stimulated vs Control analysis? Thank you. Sincerely yours,. Shangyu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821
https://github.com/scverse/scanpy/issues/821:14,testability,Control,Control,14,"Stimulated vs Control analysisi; Hello, scanpy developers, . Firstly, thank you so much for your pipeline scanpy. It is very useful to me. Secondly, I want to ask a question. Are you going to support Stimulated vs Control analysis? Thank you. Sincerely yours,. Shangyu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821
https://github.com/scverse/scanpy/issues/821:214,testability,Control,Control,214,"Stimulated vs Control analysisi; Hello, scanpy developers, . Firstly, thank you so much for your pipeline scanpy. It is very useful to me. Secondly, I want to ask a question. Are you going to support Stimulated vs Control analysis? Thank you. Sincerely yours,. Shangyu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821
https://github.com/scverse/scanpy/issues/821:192,usability,support,support,192,"Stimulated vs Control analysisi; Hello, scanpy developers, . Firstly, thank you so much for your pipeline scanpy. It is very useful to me. Secondly, I want to ask a question. Are you going to support Stimulated vs Control analysis? Thank you. Sincerely yours,. Shangyu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/821
https://github.com/scverse/scanpy/pull/822:43,deployability,build,build,43,Don't allow h5py 2.10.0; Trying to fix the build,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/822
https://github.com/scverse/scanpy/pull/824:218,deployability,fail,fail,218,"highly_variable_gene batch_key variant fix; This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/824:96,integrability,filter,filtering,96,"highly_variable_gene batch_key variant fix; This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/824:260,integrability,batch,batch,260,"highly_variable_gene batch_key variant fix; This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/824:329,integrability,filter,filtered,329,"highly_variable_gene batch_key variant fix; This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/824:369,interoperability,standard,standard,369,"highly_variable_gene batch_key variant fix; This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/824:260,performance,batch,batch,260,"highly_variable_gene batch_key variant fix; This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/824:209,reliability,doe,does,209,"highly_variable_gene batch_key variant fix; This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/824:218,reliability,fail,fail,218,"highly_variable_gene batch_key variant fix; This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/824:475,safety,test,tested,475,"highly_variable_gene batch_key variant fix; This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/824:525,safety,Test,Testing,525,"highly_variable_gene batch_key variant fix; This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/824:475,testability,test,tested,475,"highly_variable_gene batch_key variant fix; This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/824:525,testability,Test,Testing,525,"highly_variable_gene batch_key variant fix; This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/824:288,usability,feedback,feedback,288,"highly_variable_gene batch_key variant fix; This PR addresses part of #758. I just added a gene filtering addition to the `batch_key` variant of `sc.pp.highly_variable_genes()`. This ensures that the function does not fail because a gene is not expressed in a batch. I would welcome some feedback on setting dispersions to 0 for filtered out genes. I think this is the standard set by Seurat, and also what is implemented by @gokceneraslan in the original function. . I have tested on my own data for correct implementation. Testing for result is more difficult in this I guess...",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/824
https://github.com/scverse/scanpy/pull/825:0,security,Rotat,Rotate,0,"Rotate only xticks in sc.pl.violin; `rotation` argument is supposed to rotate only xticks, not both x and y according to the documentation:. ![image](https://user-images.githubusercontent.com/1140359/64564216-043c0500-d31f-11e9-8248-c24a93d037f3.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/825
https://github.com/scverse/scanpy/pull/825:37,security,rotat,rotation,37,"Rotate only xticks in sc.pl.violin; `rotation` argument is supposed to rotate only xticks, not both x and y according to the documentation:. ![image](https://user-images.githubusercontent.com/1140359/64564216-043c0500-d31f-11e9-8248-c24a93d037f3.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/825
https://github.com/scverse/scanpy/pull/825:71,security,rotat,rotate,71,"Rotate only xticks in sc.pl.violin; `rotation` argument is supposed to rotate only xticks, not both x and y according to the documentation:. ![image](https://user-images.githubusercontent.com/1140359/64564216-043c0500-d31f-11e9-8248-c24a93d037f3.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/825
https://github.com/scverse/scanpy/pull/825:125,usability,document,documentation,125,"Rotate only xticks in sc.pl.violin; `rotation` argument is supposed to rotate only xticks, not both x and y according to the documentation:. ![image](https://user-images.githubusercontent.com/1140359/64564216-043c0500-d31f-11e9-8248-c24a93d037f3.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/825
https://github.com/scverse/scanpy/pull/825:158,usability,user,user-images,158,"Rotate only xticks in sc.pl.violin; `rotation` argument is supposed to rotate only xticks, not both x and y according to the documentation:. ![image](https://user-images.githubusercontent.com/1140359/64564216-043c0500-d31f-11e9-8248-c24a93d037f3.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/825
https://github.com/scverse/scanpy/issues/826:0,integrability,sub,subsetting,0,"subsetting / subclustering, use raw; when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use. ```. tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]. ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var). ```. to ""reset"" the `.X` matrix (maybe there's a better way?). or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:13,integrability,sub,subclustering,13,"subsetting / subclustering, use raw; when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use. ```. tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]. ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var). ```. to ""reset"" the `.X` matrix (maybe there's a better way?). or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:53,integrability,sub,subset,53,"subsetting / subclustering, use raw; when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use. ```. tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]. ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var). ```. to ""reset"" the `.X` matrix (maybe there's a better way?). or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:167,integrability,rout,routines,167,"subsetting / subclustering, use raw; when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use. ```. tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]. ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var). ```. to ""reset"" the `.X` matrix (maybe there's a better way?). or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:300,integrability,sub,subcluster,300,"subsetting / subclustering, use raw; when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use. ```. tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]. ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var). ```. to ""reset"" the `.X` matrix (maybe there's a better way?). or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:344,integrability,sub,substructure,344,"subsetting / subclustering, use raw; when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use. ```. tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]. ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var). ```. to ""reset"" the `.X` matrix (maybe there's a better way?). or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:498,integrability,sub,subsetting,498,"subsetting / subclustering, use raw; when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use. ```. tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]. ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var). ```. to ""reset"" the `.X` matrix (maybe there's a better way?). or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:217,modifiability,variab,variable,217,"subsetting / subclustering, use raw; when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use. ```. tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]. ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var). ```. to ""reset"" the `.X` matrix (maybe there's a better way?). or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:275,modifiability,variab,variable,275,"subsetting / subclustering, use raw; when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use. ```. tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]. ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var). ```. to ""reset"" the `.X` matrix (maybe there's a better way?). or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:464,modifiability,variab,variable,464,"subsetting / subclustering, use raw; when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use. ```. tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]. ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var). ```. to ""reset"" the `.X` matrix (maybe there's a better way?). or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:378,reliability,doe,doesn,378,"subsetting / subclustering, use raw; when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use. ```. tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]. ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var). ```. to ""reset"" the `.X` matrix (maybe there's a better way?). or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/826:883,usability,learn,learn,883,"subsetting / subclustering, use raw; when I select a subset of cells using `ad_sub=ad[ad.obs['louvain']=='subcluster_of_interest',:]`, and then re-apply preprocessing routines, this will use only the genes of `ad.X` (variable over the entire dataset), but not those that are variable only within the subcluster and might be informative for its substructure even if the variance doesn't pass the cutoff when evaluated over the entire dataset. basically, the set of variable genes can only shrink by subsetting.. I'd propose to either use. ```. tmp=ad[ad.obs['louvain']=='subcluster_of_interest',:]. ad_sub=sc.AnnData(tmp.raw.X,obs=tmp.obs,var=tmp.raw.var). ```. to ""reset"" the `.X` matrix (maybe there's a better way?). or to make `sc.pp.highly_variable_genes` work on `ad.raw.X`. ```. scanpy==1.4.4 anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.2.1 pandas==0.25.1 scikit-learn==0.20.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/826
https://github.com/scverse/scanpy/issues/827:249,availability,stead,steady-state,249,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:7,deployability,api,api,7,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:108,deployability,api,api,108,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:165,deployability,api,api,165,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:176,deployability,api,api,176,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:203,deployability,api,api,203,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:7,integrability,api,api,7,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:108,integrability,api,api,108,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:165,integrability,api,api,165,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:176,integrability,api,api,176,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:203,integrability,api,api,203,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:256,integrability,state,state,256,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:7,interoperability,api,api,7,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:108,interoperability,api,api,108,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:165,interoperability,api,api,165,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:176,interoperability,api,api,176,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:203,interoperability,api,api,203,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:608,interoperability,coordinat,coordinates,608,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:22,usability,document,documentation,22,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/827:82,usability,document,documentation,82,"scanpy.api.tl.diffmap documentation; Please consider mentioning explicitly in the documentation of [`scanpy.api.tl.diffmap`](https://scanpy.readthedocs.io/en/stable/api/scanpy.api.tl.diffmap.html#scanpy.api.tl.diffmap) that _""the 0-th column is the steady-state solution, which is non-informative in diffusion maps"",_ as mentioned here: https://github.com/theislab/single-cell-tutorial/blob/master/latest_notebook/Case-study_Mouse-intestinal-epithelium_1906.ipynb. Also that it's not plotted by default, cf. https://github.com/theislab/scanpy/issues/675. It is very important for those who want to use these coordinates (`adata.obsm[""X_diffmap""]`).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/827
https://github.com/scverse/scanpy/issues/828:103,availability,consist,consistent,103,"Allow selection of layer/ raw for methods that use X; I think this could use a consolidated effort for consistent behavior. Especially since testing whether it works will probably have some common patterns. In some cases `Raw` will need to be an option. I like the convention of having the arguments `use_raw`, `layers`, and (when appropriate) `obsm_key`/ `varm_key`. With these at most one of the values can be not None, and if all are None (the default) `X` is used. An alternative convention is `use_rep: Optional[str]`. Im less a fan of this due to potential key collisions. Some relevant issues/ prs: #826 #801 #730",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828
https://github.com/scverse/scanpy/issues/828:19,modifiability,layer,layer,19,"Allow selection of layer/ raw for methods that use X; I think this could use a consolidated effort for consistent behavior. Especially since testing whether it works will probably have some common patterns. In some cases `Raw` will need to be an option. I like the convention of having the arguments `use_raw`, `layers`, and (when appropriate) `obsm_key`/ `varm_key`. With these at most one of the values can be not None, and if all are None (the default) `X` is used. An alternative convention is `use_rep: Optional[str]`. Im less a fan of this due to potential key collisions. Some relevant issues/ prs: #826 #801 #730",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828
https://github.com/scverse/scanpy/issues/828:312,modifiability,layer,layers,312,"Allow selection of layer/ raw for methods that use X; I think this could use a consolidated effort for consistent behavior. Especially since testing whether it works will probably have some common patterns. In some cases `Raw` will need to be an option. I like the convention of having the arguments `use_raw`, `layers`, and (when appropriate) `obsm_key`/ `varm_key`. With these at most one of the values can be not None, and if all are None (the default) `X` is used. An alternative convention is `use_rep: Optional[str]`. Im less a fan of this due to potential key collisions. Some relevant issues/ prs: #826 #801 #730",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828
https://github.com/scverse/scanpy/issues/828:141,safety,test,testing,141,"Allow selection of layer/ raw for methods that use X; I think this could use a consolidated effort for consistent behavior. Especially since testing whether it works will probably have some common patterns. In some cases `Raw` will need to be an option. I like the convention of having the arguments `use_raw`, `layers`, and (when appropriate) `obsm_key`/ `varm_key`. With these at most one of the values can be not None, and if all are None (the default) `X` is used. An alternative convention is `use_rep: Optional[str]`. Im less a fan of this due to potential key collisions. Some relevant issues/ prs: #826 #801 #730",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828
https://github.com/scverse/scanpy/issues/828:141,testability,test,testing,141,"Allow selection of layer/ raw for methods that use X; I think this could use a consolidated effort for consistent behavior. Especially since testing whether it works will probably have some common patterns. In some cases `Raw` will need to be an option. I like the convention of having the arguments `use_raw`, `layers`, and (when appropriate) `obsm_key`/ `varm_key`. With these at most one of the values can be not None, and if all are None (the default) `X` is used. An alternative convention is `use_rep: Optional[str]`. Im less a fan of this due to potential key collisions. Some relevant issues/ prs: #826 #801 #730",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828
https://github.com/scverse/scanpy/issues/828:103,usability,consist,consistent,103,"Allow selection of layer/ raw for methods that use X; I think this could use a consolidated effort for consistent behavior. Especially since testing whether it works will probably have some common patterns. In some cases `Raw` will need to be an option. I like the convention of having the arguments `use_raw`, `layers`, and (when appropriate) `obsm_key`/ `varm_key`. With these at most one of the values can be not None, and if all are None (the default) `X` is used. An alternative convention is `use_rep: Optional[str]`. Im less a fan of this due to potential key collisions. Some relevant issues/ prs: #826 #801 #730",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828
https://github.com/scverse/scanpy/issues/828:114,usability,behavi,behavior,114,"Allow selection of layer/ raw for methods that use X; I think this could use a consolidated effort for consistent behavior. Especially since testing whether it works will probably have some common patterns. In some cases `Raw` will need to be an option. I like the convention of having the arguments `use_raw`, `layers`, and (when appropriate) `obsm_key`/ `varm_key`. With these at most one of the values can be not None, and if all are None (the default) `X` is used. An alternative convention is `use_rep: Optional[str]`. Im less a fan of this due to potential key collisions. Some relevant issues/ prs: #826 #801 #730",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/828
https://github.com/scverse/scanpy/issues/829:78,availability,error,error,78,"ValueError when using projection = '3d'; Dear Theis lab,. I get the following error:. `ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (3255,4)`. when running `sc.pl.diffmap(adata, color='leiden', projection='3d', save='_diff_3d.pdf')`. I hope you guys can help me with this. regards",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/829
https://github.com/scverse/scanpy/issues/829:78,performance,error,error,78,"ValueError when using projection = '3d'; Dear Theis lab,. I get the following error:. `ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (3255,4)`. when running `sc.pl.diffmap(adata, color='leiden', projection='3d', save='_diff_3d.pdf')`. I hope you guys can help me with this. regards",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/829
https://github.com/scverse/scanpy/issues/829:78,safety,error,error,78,"ValueError when using projection = '3d'; Dear Theis lab,. I get the following error:. `ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (3255,4)`. when running `sc.pl.diffmap(adata, color='leiden', projection='3d', save='_diff_3d.pdf')`. I hope you guys can help me with this. regards",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/829
https://github.com/scverse/scanpy/issues/829:78,usability,error,error,78,"ValueError when using projection = '3d'; Dear Theis lab,. I get the following error:. `ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (3255,4)`. when running `sc.pl.diffmap(adata, color='leiden', projection='3d', save='_diff_3d.pdf')`. I hope you guys can help me with this. regards",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/829
https://github.com/scverse/scanpy/issues/829:331,usability,help,help,331,"ValueError when using projection = '3d'; Dear Theis lab,. I get the following error:. `ValueError: operands could not be broadcast together with remapped shapes [original->remapped]: (0,4) and requested shape (3255,4)`. when running `sc.pl.diffmap(adata, color='leiden', projection='3d', save='_diff_3d.pdf')`. I hope you guys can help me with this. regards",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/829
https://github.com/scverse/scanpy/pull/830:1306,availability,cluster,clusters,1306,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1853,availability,avail,available,1853,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1306,deployability,cluster,clusters,1306,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1817,deployability,version,version,1817,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1845,deployability,version,version,1845,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:60,energy efficiency,GPU,GPU,60,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:169,energy efficiency,GPU,GPU,169,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:263,energy efficiency,gpu,gpu,263,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:466,energy efficiency,CPU,CPU,466,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:481,energy efficiency,GPU,GPU,481,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1877,energy efficiency,Cloud,Cloud,1877,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1905,energy efficiency,cloud,cloud,1905,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1509,integrability,transform,transform,1509,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1817,integrability,version,version,1817,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1845,integrability,version,version,1845,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1509,interoperability,transform,transform,1509,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1128,modifiability,paramet,parameters,1128,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1443,modifiability,reu,reuse,1443,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1817,modifiability,version,version,1817,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1845,modifiability,version,version,1845,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:60,performance,GPU,GPU,60,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:169,performance,GPU,GPU,169,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:263,performance,gpu,gpu,263,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:466,performance,CPU,CPU,466,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:470,performance,time,time,470,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:481,performance,GPU,GPU,481,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:485,performance,time,time,485,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1853,reliability,availab,available,1853,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:986,safety,detect,detection,986,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1853,safety,avail,available,1853,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:986,security,detect,detection,986,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1493,security,expos,exposes,1493,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1581,security,expos,exposes,1581,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1712,security,control,controlled,1712,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1767,security,ident,identical,1767,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1853,security,availab,available,1853,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1625,testability,understand,understand,1625,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1712,testability,control,controlled,1712,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/830:1927,usability,learn,learning-vm,1927,"Add ability to run nearest neighbors, umap and louvain on a GPU using RAPIDS; This adds RAPIDS as an (experimental) option for running some of the graph algorithms on a GPU. Example usage is demonstrated here: https://github.com/tomwhite/scanpy_usage/tree/rapids-gpu/1909_rapids_gpu. In summary:. ```python. sc.pp.neighbors(adata, method='rapids'). sc.tl.louvain(adata, flavor='rapids'). sc.tl.umap(adata, method='rapids'). ```. Timings (on 130K samples):. | Step | CPU time (s) | GPU time (s) | Speedup |. | --------- | ------------ | ------------ | ------- |. | Neighbors | 47 | 15 | 3x |. | Louvain | 70 | 1 | 70x |. | UMAP | 186 | 15 | 12x |. Comments:. * In general the output figures are quite different for each of the algorithms compared to regular Scanpy. (See the figures in the link above.). * RAPIDS uses exact nearest neighbors (using https://github.com/facebookresearch/faiss I think), whereas the default NNDescent algorithm in Scanpy is approximate. * Louvain community detection results in a different number of communities (28 vs 45). I'm not sure if it is possible to change the termination criteria or other parameters that would make the two outputs the same, or at least more similar. * UMAP is different too, and in particular it is bunched in one corner. There are a few very small clusters in the opposite corner, which looks like it might be a bug. * UMAP computes the nearest neighbors again from scratch - it cant reuse the ones computed earlier since RAPIDS only exposes the fit/transform methods, and not `simplicial_set_embedding` that regular UMAP exposes. * In general it would be useful to understand what random number generators each of the alternatives use, how they can be controlled, and what prospects there are for achieving identical output (if any). * This is using RAPIDS version 0.7 since it is the version available on the Google Cloud image I used (https://cloud.google.com/deep-learning-vm/docs/images).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/830
https://github.com/scverse/scanpy/pull/831:68,reliability,doe,doesn,68,added cache_compression kwarg to read(); Since theislab/anndata#123 doesnt seem to gain traction ,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/831
https://github.com/scverse/scanpy/issues/832:575,availability,error,error,575,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:3726,availability,error,error,3726,"backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. Any idea what is going on or what I can do to make it past this error? It only started happening after I updated my operating system to Ubuntu 18 and my Python to 3.7 and reinstalled scanpy from conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:3778,availability,operat,operating,3778,"backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. Any idea what is going on or what I can do to make it past this error? It only started happening after I updated my operating system to Ubuntu 18 and my Python to 3.7 and reinstalled scanpy from conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:120,deployability,version,versions,120,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:683,deployability,modul,module,683,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:795,deployability,log,log,795,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:800,deployability,scale,scale,800,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:914,deployability,log,log,914,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:918,deployability,log,logstatus,918,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1701,deployability,log,logg,1701,"ile = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:3767,deployability,updat,updated,3767,"backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. Any idea what is going on or what I can do to make it past this error? It only started happening after I updated my operating system to Ubuntu 18 and my Python to 3.7 and reinstalled scanpy from conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:317,energy efficiency,load,load,317,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:800,energy efficiency,scale,scale,800,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1901,energy efficiency,load,load,1901,"vgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:120,integrability,version,versions,120,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:581,integrability,messag,message,581,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:581,interoperability,messag,message,581,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:817,interoperability,format,format,817,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:3446,interoperability,format,format,3446,"backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. Any idea what is going on or what I can do to make it past this error? It only started happening after I updated my operating system to Ubuntu 18 and my Python to 3.7 and reinstalled scanpy from conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:120,modifiability,version,versions,120,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:683,modifiability,modul,module,683,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:800,modifiability,scal,scale,800,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1038,modifiability,pac,packages,1038,"aving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_a",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1428,modifiability,pac,packages,1428,"y. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packag",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1805,modifiability,pac,packages,1805,"T.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_fr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:2113,modifiability,pac,packages,2113,"irst_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:2426,modifiability,pac,packages,2426,"ckages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:2753,modifiability,pac,packages,2753,"). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. Any idea what is going on or what I can do to make it past this error? It only started happenin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:3080,modifiability,pac,packages,3080,"backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. Any idea what is going on or what I can do to make it past this error? It only started happening after I updated my operating system to Ubuntu 18 and my Python to 3.7 and reinstalled scanpy from conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:3343,modifiability,pac,packages,3343,"backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. Any idea what is going on or what I can do to make it past this error? It only started happening after I updated my operating system to Ubuntu 18 and my Python to 3.7 and reinstalled scanpy from conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:317,performance,load,load,317,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:575,performance,error,error,575,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:800,performance,scale,scale,800,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1148,performance,cach,cache,1148,"=1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1312,performance,cach,cache,1312,"o load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # back",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1318,performance,cach,cache,1318," it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1539,performance,cach,cache,1539,"hen I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1901,performance,load,load,1901,"vgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1922,performance,memor,memory,1922,"us)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[ke",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:3726,performance,error,error,3726,"backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. Any idea what is going on or what I can do to make it past this error? It only started happening after I updated my operating system to Ubuntu 18 and my Python to 3.7 and reinstalled scanpy from conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:575,safety,error,error,575,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:655,safety,input,input-,655,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:683,safety,modul,module,683,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:795,safety,log,log,795,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:806,safety,TEST,TEST,806,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:914,safety,log,log,914,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:918,safety,log,logstatus,918,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1701,safety,log,logg,1701,"ile = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:3726,safety,error,error,3726,"backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. Any idea what is going on or what I can do to make it past this error? It only started happening after I updated my operating system to Ubuntu 18 and my Python to 3.7 and reinstalled scanpy from conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:3767,safety,updat,updated,3767,"backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. Any idea what is going on or what I can do to make it past this error? It only started happening after I updated my operating system to Ubuntu 18 and my Python to 3.7 and reinstalled scanpy from conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:795,security,log,log,795,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:914,security,log,log,914,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:918,security,log,logstatus,918,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1701,security,log,logg,1701,"ile = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:3767,security,updat,updated,3767,"backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. Any idea what is going on or what I can do to make it past this error? It only started happening after I updated my operating system to Ubuntu 18 and my Python to 3.7 and reinstalled scanpy from conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:611,testability,Trace,Traceback,611,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:795,testability,log,log,795,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:806,testability,TEST,TEST,806,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:914,testability,log,log,914,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:918,testability,log,logstatus,918,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1701,testability,log,logg,1701,"ile = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:418,usability,command,command,418,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:575,usability,error,error,575,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:655,usability,input,input-,655,"""invalid shape in fixed-type tuple"" when saving adata after ranking genes; I'm using Scanpy with the following software versions:. python==3.7. scanpy==1.4.4. numpy==1.17.2. anndata==0.6.22.post1. on Ubuntu 18.04. I am able to save my AnnData object just fine with . ```py. sc.write(results_file, adata). ```. and to load it again with . ```py. adata = sc.read(results_file). ```. however if I save it after I run the command . ```py. sc.tl.rank_genes_groups(adata, 'louvain12_lab', method='wilcoxon'). ```. the AnnData object will save but when I try to reload it, I get an error message:. ```pytb. ValueError Traceback (most recent call last). <ipython-input-141-159082f1696f> in <module>. 1 results_file = os.path.join(adir, '{project}.count_{count}.gene_{gene}.mito_{mito}.HVGs_{nhvgs}.TPT.{log}.scale.TEST.h5ad'.format(project=project_name, count=count_thresh, gene=gene_thresh, mito=mitothresh, nhvgs=nhvgs, log=logstatus)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chu",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:1922,usability,memor,memory,1922,"us)). 2 print(results_file). ----> 3 adata = sc.read(results_file). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, **kwargs). 95 filename, backed=backed, sheet=sheet, ext=ext,. 96 delimiter=delimiter, first_column_names=first_column_names,. ---> 97 backup_url=backup_url, cache=cache, **kwargs,. 98 ). 99 # generate filename and read to dict. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/scanpy/readwrite.py in _read(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, suppress_cache_warning, **kwargs). 497 if ext in {'h5', 'h5ad'}:. 498 if sheet is None:. --> 499 return read_h5ad(filename, backed=backed). 500 else:. 501 logg.debug(f'reading sheet {sheet} from file {filename}'). /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in read_h5ad(filename, backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[ke",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/832:3726,usability,error,error,3726,"backed, chunk_size). 445 else:. 446 # load everything into memory. --> 447 constructor_args = _read_args_from_h5ad(filename=filename, chunk_size=chunk_size). 448 X = constructor_args[0]. 449 dtype = None. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_args_from_h5ad(adata, filename, mode, chunk_size). 484 d[key] = None. 485 else:. --> 486 _read_key_value_from_h5(f, d, key, chunk_size=chunk_size). 487 # backwards compat: save X with the correct name. 488 if 'X' not in d:. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 508 d[key_write] = OrderedDict() if key == 'uns' else {}. 509 for k in f[key].keys():. --> 510 _read_key_value_from_h5(f, d[key_write], key + '/' + k, k, chunk_size). 511 return. 512 . /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in _read_key_value_from_h5(f, d, key, key_write, chunk_size). 542 return key, value. 543 . --> 544 key, value = postprocess_reading(key, value). 545 d[key_write] = value. 546 return. /opt/miniconda3/envs/py37/lib/python3.7/site-packages/anndata/readwrite/read.py in postprocess_reading(key, value). 539 new_dtype = [((dt[0], 'U{}'.format(int(int(dt[1][2:])/4))). 540 if dt[1][1] == 'S' else dt) for dt in value.dtype.descr]. --> 541 value = value.astype(new_dtype). 542 return key, value. 543 . ValueError: invalid shape in fixed-type tuple. ```. Any idea what is going on or what I can do to make it past this error? It only started happening after I updated my operating system to Ubuntu 18 and my Python to 3.7 and reinstalled scanpy from conda.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/832
https://github.com/scverse/scanpy/issues/833:12,availability,error,error,12,"subgrouping error?; I'm trying to take subgroups from the AnnData object as such:. `adata_1 = adata[adata.obs['louvain'] == '0']`. which works but if I want to get the inverse. `adata_1 = adata[adata.obs['louvain'] != '0']`. it sometimes throws this error only on the second line. 'IndexError: index 8 is out of bounds for axis 0 with size 8'. After some debugging, it seems like this occurs when you try run louvain clustering at different resolutions (ie doing it more than once). . Is there something real elementary that I am missing here? Or is there a better way of doing what I need? Thanks, Kevin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833
https://github.com/scverse/scanpy/issues/833:250,availability,error,error,250,"subgrouping error?; I'm trying to take subgroups from the AnnData object as such:. `adata_1 = adata[adata.obs['louvain'] == '0']`. which works but if I want to get the inverse. `adata_1 = adata[adata.obs['louvain'] != '0']`. it sometimes throws this error only on the second line. 'IndexError: index 8 is out of bounds for axis 0 with size 8'. After some debugging, it seems like this occurs when you try run louvain clustering at different resolutions (ie doing it more than once). . Is there something real elementary that I am missing here? Or is there a better way of doing what I need? Thanks, Kevin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833
https://github.com/scverse/scanpy/issues/833:417,availability,cluster,clustering,417,"subgrouping error?; I'm trying to take subgroups from the AnnData object as such:. `adata_1 = adata[adata.obs['louvain'] == '0']`. which works but if I want to get the inverse. `adata_1 = adata[adata.obs['louvain'] != '0']`. it sometimes throws this error only on the second line. 'IndexError: index 8 is out of bounds for axis 0 with size 8'. After some debugging, it seems like this occurs when you try run louvain clustering at different resolutions (ie doing it more than once). . Is there something real elementary that I am missing here? Or is there a better way of doing what I need? Thanks, Kevin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833
https://github.com/scverse/scanpy/issues/833:417,deployability,cluster,clustering,417,"subgrouping error?; I'm trying to take subgroups from the AnnData object as such:. `adata_1 = adata[adata.obs['louvain'] == '0']`. which works but if I want to get the inverse. `adata_1 = adata[adata.obs['louvain'] != '0']`. it sometimes throws this error only on the second line. 'IndexError: index 8 is out of bounds for axis 0 with size 8'. After some debugging, it seems like this occurs when you try run louvain clustering at different resolutions (ie doing it more than once). . Is there something real elementary that I am missing here? Or is there a better way of doing what I need? Thanks, Kevin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833
https://github.com/scverse/scanpy/issues/833:0,integrability,sub,subgrouping,0,"subgrouping error?; I'm trying to take subgroups from the AnnData object as such:. `adata_1 = adata[adata.obs['louvain'] == '0']`. which works but if I want to get the inverse. `adata_1 = adata[adata.obs['louvain'] != '0']`. it sometimes throws this error only on the second line. 'IndexError: index 8 is out of bounds for axis 0 with size 8'. After some debugging, it seems like this occurs when you try run louvain clustering at different resolutions (ie doing it more than once). . Is there something real elementary that I am missing here? Or is there a better way of doing what I need? Thanks, Kevin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833
https://github.com/scverse/scanpy/issues/833:39,integrability,sub,subgroups,39,"subgrouping error?; I'm trying to take subgroups from the AnnData object as such:. `adata_1 = adata[adata.obs['louvain'] == '0']`. which works but if I want to get the inverse. `adata_1 = adata[adata.obs['louvain'] != '0']`. it sometimes throws this error only on the second line. 'IndexError: index 8 is out of bounds for axis 0 with size 8'. After some debugging, it seems like this occurs when you try run louvain clustering at different resolutions (ie doing it more than once). . Is there something real elementary that I am missing here? Or is there a better way of doing what I need? Thanks, Kevin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833
https://github.com/scverse/scanpy/issues/833:12,performance,error,error,12,"subgrouping error?; I'm trying to take subgroups from the AnnData object as such:. `adata_1 = adata[adata.obs['louvain'] == '0']`. which works but if I want to get the inverse. `adata_1 = adata[adata.obs['louvain'] != '0']`. it sometimes throws this error only on the second line. 'IndexError: index 8 is out of bounds for axis 0 with size 8'. After some debugging, it seems like this occurs when you try run louvain clustering at different resolutions (ie doing it more than once). . Is there something real elementary that I am missing here? Or is there a better way of doing what I need? Thanks, Kevin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833
https://github.com/scverse/scanpy/issues/833:250,performance,error,error,250,"subgrouping error?; I'm trying to take subgroups from the AnnData object as such:. `adata_1 = adata[adata.obs['louvain'] == '0']`. which works but if I want to get the inverse. `adata_1 = adata[adata.obs['louvain'] != '0']`. it sometimes throws this error only on the second line. 'IndexError: index 8 is out of bounds for axis 0 with size 8'. After some debugging, it seems like this occurs when you try run louvain clustering at different resolutions (ie doing it more than once). . Is there something real elementary that I am missing here? Or is there a better way of doing what I need? Thanks, Kevin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833
https://github.com/scverse/scanpy/issues/833:12,safety,error,error,12,"subgrouping error?; I'm trying to take subgroups from the AnnData object as such:. `adata_1 = adata[adata.obs['louvain'] == '0']`. which works but if I want to get the inverse. `adata_1 = adata[adata.obs['louvain'] != '0']`. it sometimes throws this error only on the second line. 'IndexError: index 8 is out of bounds for axis 0 with size 8'. After some debugging, it seems like this occurs when you try run louvain clustering at different resolutions (ie doing it more than once). . Is there something real elementary that I am missing here? Or is there a better way of doing what I need? Thanks, Kevin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833
https://github.com/scverse/scanpy/issues/833:250,safety,error,error,250,"subgrouping error?; I'm trying to take subgroups from the AnnData object as such:. `adata_1 = adata[adata.obs['louvain'] == '0']`. which works but if I want to get the inverse. `adata_1 = adata[adata.obs['louvain'] != '0']`. it sometimes throws this error only on the second line. 'IndexError: index 8 is out of bounds for axis 0 with size 8'. After some debugging, it seems like this occurs when you try run louvain clustering at different resolutions (ie doing it more than once). . Is there something real elementary that I am missing here? Or is there a better way of doing what I need? Thanks, Kevin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833
https://github.com/scverse/scanpy/issues/833:12,usability,error,error,12,"subgrouping error?; I'm trying to take subgroups from the AnnData object as such:. `adata_1 = adata[adata.obs['louvain'] == '0']`. which works but if I want to get the inverse. `adata_1 = adata[adata.obs['louvain'] != '0']`. it sometimes throws this error only on the second line. 'IndexError: index 8 is out of bounds for axis 0 with size 8'. After some debugging, it seems like this occurs when you try run louvain clustering at different resolutions (ie doing it more than once). . Is there something real elementary that I am missing here? Or is there a better way of doing what I need? Thanks, Kevin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833
https://github.com/scverse/scanpy/issues/833:250,usability,error,error,250,"subgrouping error?; I'm trying to take subgroups from the AnnData object as such:. `adata_1 = adata[adata.obs['louvain'] == '0']`. which works but if I want to get the inverse. `adata_1 = adata[adata.obs['louvain'] != '0']`. it sometimes throws this error only on the second line. 'IndexError: index 8 is out of bounds for axis 0 with size 8'. After some debugging, it seems like this occurs when you try run louvain clustering at different resolutions (ie doing it more than once). . Is there something real elementary that I am missing here? Or is there a better way of doing what I need? Thanks, Kevin",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/833
https://github.com/scverse/scanpy/issues/834:169,availability,error,error,169,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:857,availability,sli,slightly,857,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:866,availability,slo,slower,866,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:1013,deployability,log,logg,1013,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:1160,deployability,version,versions,1160,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:388,integrability,sub,subset,388,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:1160,integrability,version,versions,1160,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:231,modifiability,pac,packages,231,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:549,modifiability,pac,packages,549,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:1160,modifiability,version,versions,1160,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:169,performance,error,error,169,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:857,reliability,sli,slightly,857,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:866,reliability,slo,slower,866,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:169,safety,error,error,169,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:1013,safety,log,logg,1013,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:1013,security,log,logg,1013,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:1013,testability,log,logg,1013,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:169,usability,error,error,169,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/issues/834:1289,usability,learn,learn,1289,"highly_variable_genes n_top_genes option; Hi, . I tried to run `sc.tl.highly_variable_genes` with `flavor=CellRanger` and `n_top_genes = 2000`. I obtained the following error:. ```pytb. /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in highly_variable_genes(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor, subset, inplace, batch_key). 280 n_top_genes=n_top_genes,. 281 n_bins=n_bins,. --> 282 flavor=flavor,. 283 ). 284 . /home/miniconda3/envs/dev/lib/python3.7/site-packages/scanpy/preprocessing/_highly_variable_genes.py in _highly_variable_genes_single_batch(adata, min_disp, max_disp, min_mean, max_mean, n_top_genes, n_bins, flavor). 141 dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]. 142 dispersion_norm[::-1].sort() # interestingly, np.argpartition is slightly slower. --> 143 disp_cut_off = dispersion_norm[n_top_genes-1]. 144 gene_subset = np.nan_to_num(df['dispersions_norm'].values) >= disp_cut_off. 145 logg.debug(. IndexError: index 1999 is out of bounds for axis 0 with size 1898. ```. I run scanpy in Python 3.7 (linux machine) with the following versions:. ```. scanpy==1.4.5.dev114+gd69832a anndata==0.6.22.post1 umap==0.3.9 numpy==1.17.0 scipy==1.3.0 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. I suggest to check if `n_top_genes` is larger than `len(dispersion_norm)`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/834
https://github.com/scverse/scanpy/pull/835:6,availability,error,error,6,Dont error if n_top_genes > n_var; TODO: test. Fixes #834,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/835
https://github.com/scverse/scanpy/pull/835:6,performance,error,error,6,Dont error if n_top_genes > n_var; TODO: test. Fixes #834,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/835
https://github.com/scverse/scanpy/pull/835:6,safety,error,error,6,Dont error if n_top_genes > n_var; TODO: test. Fixes #834,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/835
https://github.com/scverse/scanpy/pull/835:42,safety,test,test,42,Dont error if n_top_genes > n_var; TODO: test. Fixes #834,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/835
https://github.com/scverse/scanpy/pull/835:42,testability,test,test,42,Dont error if n_top_genes > n_var; TODO: test. Fixes #834,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/835
https://github.com/scverse/scanpy/pull/835:6,usability,error,error,6,Dont error if n_top_genes > n_var; TODO: test. Fixes #834,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/835
https://github.com/scverse/scanpy/issues/836:46,usability,user,user-images,46,"export the gene list of PCA; ![image](https://user-images.githubusercontent.com/49429496/64786892-18dbff80-d5a2-11e9-9685-0295e31385f9.png). Hi, how could I export those genes of PCA into csv? I need the gene list of pc1 to pc9. it would be better if I can keep the ranking information.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/836
https://github.com/scverse/scanpy/issues/837:127,availability,error,error,127,"sc.pp.neighbors fails when only 1 feature present; I've tried running `sc.pp.neighbors` with only 1 feature, got the following error:. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-6-e652dbfa9fae> in <module>. 3 . 4 adata = sc.datasets.paul15(). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:16,deployability,fail,fails,16,"sc.pp.neighbors fails when only 1 feature present; I've tried running `sc.pp.neighbors` with only 1 feature, got the following error:. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-6-e652dbfa9fae> in <module>. 3 . 4 adata = sc.datasets.paul15(). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:302,deployability,modul,module,302,"sc.pp.neighbors fails when only 1 feature present; I've tried running `sc.pp.neighbors` with only 1 feature, got the following error:. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-6-e652dbfa9fae> in <module>. 3 . 4 adata = sc.datasets.paul15(). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:2991,deployability,contain,contains,2991,"vent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ```. To reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata[:, 0]). ```. Not using `.copy()` doesn't seem to be the cause of the problem, since I've tried that and it still crashes. My versions are:. ```python. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.8 numpy==1.16.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. When running `sc.pp.neighbors(adata[:, :2])`, it works as expected.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:3316,deployability,contain,contains,3316,"vent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ```. To reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata[:, 0]). ```. Not using `.copy()` doesn't seem to be the cause of the problem, since I've tried that and it still crashes. My versions are:. ```python. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.8 numpy==1.16.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. When running `sc.pp.neighbors(adata[:, :2])`, it works as expected.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:3571,deployability,version,versions,3571,"vent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ```. To reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata[:, 0]). ```. Not using `.copy()` doesn't seem to be the cause of the problem, since I've tried that and it still crashes. My versions are:. ```python. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.8 numpy==1.16.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. When running `sc.pp.neighbors(adata[:, :2])`, it works as expected.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:2486,energy efficiency,estimat,estimator,2486,"587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ```. To reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata[:, 0]). ```. Not using `.copy()` doesn't seem",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:2496,energy efficiency,estimat,estimator,2496,"588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ```. To reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata[:, 0]). ```. Not using `.copy()` doesn't seem to be the",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:2838,energy efficiency,estimat,estimator,2838,"vent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ```. To reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata[:, 0]). ```. Not using `.copy()` doesn't seem to be the cause of the problem, since I've tried that and it still crashes. My versions are:. ```python. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.8 numpy==1.16.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. When running `sc.pp.neighbors(adata[:, :2])`, it works as expected.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:3571,integrability,version,versions,3571,"vent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ```. To reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata[:, 0]). ```. Not using `.copy()` doesn't seem to be the cause of the problem, since I've tried that and it still crashes. My versions are:. ```python. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.8 numpy==1.16.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. When running `sc.pp.neighbors(adata[:, :2])`, it works as expected.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:3018,interoperability,format,format,3018,"vent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ```. To reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata[:, 0]). ```. Not using `.copy()` doesn't seem to be the cause of the problem, since I've tried that and it still crashes. My versions are:. ```python. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.8 numpy==1.16.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. When running `sc.pp.neighbors(adata[:, :2])`, it works as expected.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:302,modifiability,modul,module,302,"sc.pp.neighbors fails when only 1 feature present; I've tried running `sc.pp.neighbors` with only 1 feature, got the following error:. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-6-e652dbfa9fae> in <module>. 3 . 4 adata = sc.datasets.paul15(). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:436,modifiability,pac,packages,436,"sc.pp.neighbors fails when only 1 feature present; I've tried running `sc.pp.neighbors` with only 1 feature, got the following error:. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-6-e652dbfa9fae> in <module>. 3 . 4 adata = sc.datasets.paul15(). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:830,modifiability,pac,packages,830,"sc.pp.neighbors fails when only 1 feature present; I've tried running `sc.pp.neighbors` with only 1 feature, got the following error:. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-6-e652dbfa9fae> in <module>. 3 . 4 adata = sc.datasets.paul15(). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:1340,modifiability,pac,packages,1340,"). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:1625,modifiability,pac,packages,1625,"se_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:1928,modifiability,pac,packages,1928,"hod, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:2298,modifiability,pac,packages,2298,"conda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:2628,modifiability,pac,packages,2628,"s/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ```. To reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata[:, 0]). ```. Not using `.copy()` doesn't seem to be the cause of the problem, since I've tried that and it still crashes. My versions are:. ```python. scanpy==1.4.4.post1 anndata==0.6.22",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:3571,modifiability,version,versions,3571,"vent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ```. To reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata[:, 0]). ```. Not using `.copy()` doesn't seem to be the cause of the problem, since I've tried that and it still crashes. My versions are:. ```python. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.8 numpy==1.16.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. When running `sc.pp.neighbors(adata[:, :2])`, it works as expected.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:127,performance,error,error,127,"sc.pp.neighbors fails when only 1 feature present; I've tried running `sc.pp.neighbors` with only 1 feature, got the following error:. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-6-e652dbfa9fae> in <module>. 3 . 4 adata = sc.datasets.paul15(). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:1867,performance,overhead,overhead,1867,"compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your dat",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:16,reliability,fail,fails,16,"sc.pp.neighbors fails when only 1 feature present; I've tried running `sc.pp.neighbors` with only 1 feature, got the following error:. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-6-e652dbfa9fae> in <module>. 3 . 4 adata = sc.datasets.paul15(). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:3479,reliability,doe,doesn,3479,"vent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ```. To reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata[:, 0]). ```. Not using `.copy()` doesn't seem to be the cause of the problem, since I've tried that and it still crashes. My versions are:. ```python. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.8 numpy==1.16.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. When running `sc.pp.neighbors(adata[:, :2])`, it works as expected.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:127,safety,error,error,127,"sc.pp.neighbors fails when only 1 feature present; I've tried running `sc.pp.neighbors` with only 1 feature, got the following error:. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-6-e652dbfa9fae> in <module>. 3 . 4 adata = sc.datasets.paul15(). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:276,safety,input,input-,276,"sc.pp.neighbors fails when only 1 feature present; I've tried running `sc.pp.neighbors` with only 1 feature, got the following error:. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-6-e652dbfa9fae> in <module>. 3 . 4 adata = sc.datasets.paul15(). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:302,safety,modul,module,302,"sc.pp.neighbors fails when only 1 feature present; I've tried running `sc.pp.neighbors` with only 1 feature, got the following error:. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-6-e652dbfa9fae> in <module>. 3 . 4 adata = sc.datasets.paul15(). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:1840,safety,prevent,prevent,1840,"y/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estima",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:2651,safety,valid,validation,2651,"se.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ```. To reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata[:, 0]). ```. Not using `.copy()` doesn't seem to be the cause of the problem, since I've tried that and it still crashes. My versions are:. ```python. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.8 numpy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:1840,security,preven,prevent,1840,"y/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estima",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:2651,security,validat,validation,2651,"se.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ```. To reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata[:, 0]). ```. Not using `.copy()` doesn't seem to be the cause of the problem, since I've tried that and it still crashes. My versions are:. ```python. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.8 numpy",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:232,testability,Trace,Traceback,232,"sc.pp.neighbors fails when only 1 feature present; I've tried running `sc.pp.neighbors` with only 1 feature, got the following error:. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-6-e652dbfa9fae> in <module>. 3 . 4 adata = sc.datasets.paul15(). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:127,usability,error,error,127,"sc.pp.neighbors fails when only 1 feature present; I've tried running `sc.pp.neighbors` with only 1 feature, got the following error:. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-6-e652dbfa9fae> in <module>. 3 . 4 adata = sc.datasets.paul15(). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:276,usability,input,input-,276,"sc.pp.neighbors fails when only 1 feature present; I've tried running `sc.pp.neighbors` with only 1 feature, got the following error:. ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-6-e652dbfa9fae> in <module>. 3 . 4 adata = sc.datasets.paul15(). ----> 5 sc.pp.neighbors(adata[:, 0]). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in neighbors(adata, n_neighbors, n_pcs, use_rep, knn, random_state, method, metric, metric_kwds, copy). 93 n_neighbors=n_neighbors, knn=knn, n_pcs=n_pcs, use_rep=use_rep,. 94 method=method, metric=metric, metric_kwds=metric_kwds,. ---> 95 random_state=random_state,. 96 ). 97 adata.uns['neighbors'] = {}. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/scanpy/neighbors/__init__.py in compute_neighbors(self, n_neighbors, knn, n_pcs, use_rep, method, random_state, write_knn_indices, metric, metric_kwds). 653 use_dense_distances = (metric == 'euclidean' and X.shape[0] < 8192) or knn == False. 654 if use_dense_distances:. --> 655 _distances = pairwise_distances(X, metric=metric, **metric_kwds). 656 knn_indices, knn_distances = get_indices_distances_from_dense_matrix(. 657 _distances, n_neighbors). ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in pairwise_distances(X, Y, metric, n_jobs, **kwds). 1586 func = partial(distance.cdist, metric=metric, **kwds). 1587 . -> 1588 return _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1589 . 1590 . ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in _parallel_pairwise(X, Y, func, n_jobs, **kwds). 1204 . 1205 if effective_n_jobs(n_jobs) == 1:. -> 1206 return func(X, Y, **kwds). 1207 . 1208 # enforce a threading backend to prevent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/837:3700,usability,learn,learn,3700,"vent data communication overhead. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in euclidean_distances(X, Y, Y_norm_squared, squared, X_norm_squared). 230 paired_distances : distances betweens pairs of elements of X and Y. 231 """""". --> 232 X, Y = check_pairwise_arrays(X, Y). 233 . 234 # If norms are passed as float32, they are unused. If arrays are passed as. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/metrics/pairwise.py in check_pairwise_arrays(X, Y, precomputed, dtype). 107 if Y is X or Y is None:. 108 X = Y = check_array(X, accept_sparse='csr', dtype=dtype,. --> 109 estimator=estimator). 110 else:. 111 X = check_array(X, accept_sparse='csr', dtype=dtype,. ~/.local/miniconda3/envs/reprog/lib/python3.6/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator). 519 ""Reshape your data either using array.reshape(-1, 1) if "". 520 ""your data has a single feature or array.reshape(1, -1) "". --> 521 ""if it contains a single sample."".format(array)). 522 . 523 # in the future np.flexible dtypes will be handled like object dtypes. ValueError: Expected 2D array, got 1D array instead:. array=[0. 0. 1. ... 0. 3. 0.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample. ```. To reproduce:. ```python. import scanpy as sc. adata = sc.datasets.paul15(). sc.pp.neighbors(adata[:, 0]). ```. Not using `.copy()` doesn't seem to be the cause of the problem, since I've tried that and it still crashes. My versions are:. ```python. scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.8 numpy==1.16.4 scipy==1.3.1 pandas==0.25.1 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```. When running `sc.pp.neighbors(adata[:, :2])`, it works as expected.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/837
https://github.com/scverse/scanpy/issues/838:35,availability,cluster,cluster,35,"sc.tl.rank_genes_groups() for same cluster but different batch; Hi, if I had 3 batch (sample01, sample02, and sample03) and 18 clusters (louvain), how could I find the differentially expressed gene between [A] and [B]. [A] refers to [cluster 1, 2, 3 in **sample01** and **sample02**] . [B] refers to [cluster 2 and 3 in **sample03**].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838
https://github.com/scverse/scanpy/issues/838:127,availability,cluster,clusters,127,"sc.tl.rank_genes_groups() for same cluster but different batch; Hi, if I had 3 batch (sample01, sample02, and sample03) and 18 clusters (louvain), how could I find the differentially expressed gene between [A] and [B]. [A] refers to [cluster 1, 2, 3 in **sample01** and **sample02**] . [B] refers to [cluster 2 and 3 in **sample03**].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838
https://github.com/scverse/scanpy/issues/838:234,availability,cluster,cluster,234,"sc.tl.rank_genes_groups() for same cluster but different batch; Hi, if I had 3 batch (sample01, sample02, and sample03) and 18 clusters (louvain), how could I find the differentially expressed gene between [A] and [B]. [A] refers to [cluster 1, 2, 3 in **sample01** and **sample02**] . [B] refers to [cluster 2 and 3 in **sample03**].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838
https://github.com/scverse/scanpy/issues/838:301,availability,cluster,cluster,301,"sc.tl.rank_genes_groups() for same cluster but different batch; Hi, if I had 3 batch (sample01, sample02, and sample03) and 18 clusters (louvain), how could I find the differentially expressed gene between [A] and [B]. [A] refers to [cluster 1, 2, 3 in **sample01** and **sample02**] . [B] refers to [cluster 2 and 3 in **sample03**].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838
https://github.com/scverse/scanpy/issues/838:35,deployability,cluster,cluster,35,"sc.tl.rank_genes_groups() for same cluster but different batch; Hi, if I had 3 batch (sample01, sample02, and sample03) and 18 clusters (louvain), how could I find the differentially expressed gene between [A] and [B]. [A] refers to [cluster 1, 2, 3 in **sample01** and **sample02**] . [B] refers to [cluster 2 and 3 in **sample03**].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838
https://github.com/scverse/scanpy/issues/838:127,deployability,cluster,clusters,127,"sc.tl.rank_genes_groups() for same cluster but different batch; Hi, if I had 3 batch (sample01, sample02, and sample03) and 18 clusters (louvain), how could I find the differentially expressed gene between [A] and [B]. [A] refers to [cluster 1, 2, 3 in **sample01** and **sample02**] . [B] refers to [cluster 2 and 3 in **sample03**].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838
https://github.com/scverse/scanpy/issues/838:234,deployability,cluster,cluster,234,"sc.tl.rank_genes_groups() for same cluster but different batch; Hi, if I had 3 batch (sample01, sample02, and sample03) and 18 clusters (louvain), how could I find the differentially expressed gene between [A] and [B]. [A] refers to [cluster 1, 2, 3 in **sample01** and **sample02**] . [B] refers to [cluster 2 and 3 in **sample03**].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838
https://github.com/scverse/scanpy/issues/838:301,deployability,cluster,cluster,301,"sc.tl.rank_genes_groups() for same cluster but different batch; Hi, if I had 3 batch (sample01, sample02, and sample03) and 18 clusters (louvain), how could I find the differentially expressed gene between [A] and [B]. [A] refers to [cluster 1, 2, 3 in **sample01** and **sample02**] . [B] refers to [cluster 2 and 3 in **sample03**].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838
https://github.com/scverse/scanpy/issues/838:57,integrability,batch,batch,57,"sc.tl.rank_genes_groups() for same cluster but different batch; Hi, if I had 3 batch (sample01, sample02, and sample03) and 18 clusters (louvain), how could I find the differentially expressed gene between [A] and [B]. [A] refers to [cluster 1, 2, 3 in **sample01** and **sample02**] . [B] refers to [cluster 2 and 3 in **sample03**].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838
https://github.com/scverse/scanpy/issues/838:79,integrability,batch,batch,79,"sc.tl.rank_genes_groups() for same cluster but different batch; Hi, if I had 3 batch (sample01, sample02, and sample03) and 18 clusters (louvain), how could I find the differentially expressed gene between [A] and [B]. [A] refers to [cluster 1, 2, 3 in **sample01** and **sample02**] . [B] refers to [cluster 2 and 3 in **sample03**].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838
https://github.com/scverse/scanpy/issues/838:57,performance,batch,batch,57,"sc.tl.rank_genes_groups() for same cluster but different batch; Hi, if I had 3 batch (sample01, sample02, and sample03) and 18 clusters (louvain), how could I find the differentially expressed gene between [A] and [B]. [A] refers to [cluster 1, 2, 3 in **sample01** and **sample02**] . [B] refers to [cluster 2 and 3 in **sample03**].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838
https://github.com/scverse/scanpy/issues/838:79,performance,batch,batch,79,"sc.tl.rank_genes_groups() for same cluster but different batch; Hi, if I had 3 batch (sample01, sample02, and sample03) and 18 clusters (louvain), how could I find the differentially expressed gene between [A] and [B]. [A] refers to [cluster 1, 2, 3 in **sample01** and **sample02**] . [B] refers to [cluster 2 and 3 in **sample03**].",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/838
https://github.com/scverse/scanpy/pull/839:166,usability,help,helper,166,"Unify all isinstance calls; This should take care of everything: All `isinstance` calls are with concrete types or ABCs, never `typing` types. We should maybe make a helper for this common case:. ```py. T = TypeVar(""T"", Any). def _ensure_sequence(obj: Union[T, Sequence[T]]) -> Sequence[T]:. if isinstance(obj, str) or not isinstance(obj, cabc.Sequence):. obj = [obj]. return obj. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/839
https://github.com/scverse/scanpy/issues/840:339,availability,error,errors,339,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:64,deployability,version,version,64,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:389,deployability,log,logging,389,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:32,integrability,pub,public,32,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:64,integrability,version,version,64,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:553,integrability,pub,public,553,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:650,integrability,pub,public,650,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:800,integrability,pub,public,800,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:885,integrability,pub,public,885,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1035,integrability,pub,public,1035,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1124,integrability,pub,public,1124,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1274,integrability,pub,public,1274,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1371,integrability,pub,public,1371,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1521,integrability,pub,public,1521,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1621,integrability,pub,public,1621,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:64,modifiability,version,version,64,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:438,modifiability,pac,packages,438,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:490,modifiability,pac,packages,490,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:685,modifiability,pac,packages,685,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:737,modifiability,pac,packages,737,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:920,modifiability,pac,packages,920,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:972,modifiability,pac,packages,972,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1159,modifiability,pac,packages,1159,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1211,modifiability,pac,packages,1211,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1406,modifiability,pac,packages,1406,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1458,modifiability,pac,packages,1458,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:339,performance,error,errors,339,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1058,performance,time,timeseries,1058,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:339,safety,error,errors,339,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:389,safety,log,logging,389,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:389,security,log,logging,389,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:389,testability,log,logging,389,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:0,usability,User,UserWarning,0,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:339,usability,error,errors,339,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:375,usability,hint,hints,375,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:521,usability,User,UserWarning,521,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:768,usability,User,UserWarning,768,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1003,usability,User,UserWarning,1003,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1242,usability,User,UserWarning,1242,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1489,usability,User,UserWarning,1489,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/issues/840:1767,usability,learn,learn,1767,"UserWarning: Found an util with public name; Hi,. I'm using the version from the main branch ```scanpy==1.4.5.dev104+g1a5defb```, and got some warnings at start up:. ```py. import numpy as np. import pandas as pd. import matplotlib.pyplot as pl. from matplotlib import rcParams. import scanpy as sc. sc.settings.verbosity = 3 # verbosity: errors (0), warnings (1), info (2), hints (3). sc.logging.print_versions(). ```. ```. /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function annotate_doc_types at 0x2ac365a5f200>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function matrix at 0x2ac36c882950>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries at 0x2ac36c897830>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_subplot at 0x2ac36c8977a0>. warnings.warn(f""Found an util with public name: {obj}""). /home/gzhang/packages/anaconda3/envs/monocle3/lib/python3.7/site-packages/scanpy/_utils.py:132: UserWarning: Found an util with public name: <function timeseries_as_heatmap at 0x2ac36c8978c0>. warnings.warn(f""Found an util with public name: {obj}""). ```. ```. scanpy==1.4.5.dev104+g1a5defb anndata==0.6.22.post1 umap==0.3.10 numpy==1.16.4 scipy==1.3.1 pandas==0.25.0 scikit-learn==0.21.2 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/840
https://github.com/scverse/scanpy/pull/841:63,safety,test,tests,63,blacken everything without too many line changes; TODO:. - [x] tests. - [x] go over changes if theyre good or the code should be changed,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/841
https://github.com/scverse/scanpy/pull/841:63,testability,test,tests,63,blacken everything without too many line changes; TODO:. - [x] tests. - [x] go over changes if theyre good or the code should be changed,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/841
https://github.com/scverse/scanpy/issues/842:295,integrability,sub,subset,295,"Inconsistent results with groups parameter in rank_genes_groups; I get inconsistent results when I use the parameter groups in `rank_genes_groups`. What I want to achieve is to focus on some groups for the DE genes computation. However, the results I get using the parameter groups, to select a subset of groups, are the same, in terms of DE genes, with respect to using the full set of groups. For example, l get different results if I run the following two snippets of code (which should provide instead the same set of DE genes). ``` python. # Subsetting should be done by method. sc.tl.rank_genes_groups(. adata=adata,. groupby='leiden',. use_raw=False,. method='t-test',. groups=['1', '2'],. ). ```. ``` python. # Explicit subsetting. adata_f = adata[adata.obs['leiden'].isin(['1','2'])].copy(). sc.tl.rank_genes_groups(. adata=adata_f,. groupby='leiden',. use_raw=False,. method='t-test',. ). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842
https://github.com/scverse/scanpy/issues/842:547,integrability,Sub,Subsetting,547,"Inconsistent results with groups parameter in rank_genes_groups; I get inconsistent results when I use the parameter groups in `rank_genes_groups`. What I want to achieve is to focus on some groups for the DE genes computation. However, the results I get using the parameter groups, to select a subset of groups, are the same, in terms of DE genes, with respect to using the full set of groups. For example, l get different results if I run the following two snippets of code (which should provide instead the same set of DE genes). ``` python. # Subsetting should be done by method. sc.tl.rank_genes_groups(. adata=adata,. groupby='leiden',. use_raw=False,. method='t-test',. groups=['1', '2'],. ). ```. ``` python. # Explicit subsetting. adata_f = adata[adata.obs['leiden'].isin(['1','2'])].copy(). sc.tl.rank_genes_groups(. adata=adata_f,. groupby='leiden',. use_raw=False,. method='t-test',. ). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842
https://github.com/scverse/scanpy/issues/842:728,integrability,sub,subsetting,728,"Inconsistent results with groups parameter in rank_genes_groups; I get inconsistent results when I use the parameter groups in `rank_genes_groups`. What I want to achieve is to focus on some groups for the DE genes computation. However, the results I get using the parameter groups, to select a subset of groups, are the same, in terms of DE genes, with respect to using the full set of groups. For example, l get different results if I run the following two snippets of code (which should provide instead the same set of DE genes). ``` python. # Subsetting should be done by method. sc.tl.rank_genes_groups(. adata=adata,. groupby='leiden',. use_raw=False,. method='t-test',. groups=['1', '2'],. ). ```. ``` python. # Explicit subsetting. adata_f = adata[adata.obs['leiden'].isin(['1','2'])].copy(). sc.tl.rank_genes_groups(. adata=adata_f,. groupby='leiden',. use_raw=False,. method='t-test',. ). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842
https://github.com/scverse/scanpy/issues/842:33,modifiability,paramet,parameter,33,"Inconsistent results with groups parameter in rank_genes_groups; I get inconsistent results when I use the parameter groups in `rank_genes_groups`. What I want to achieve is to focus on some groups for the DE genes computation. However, the results I get using the parameter groups, to select a subset of groups, are the same, in terms of DE genes, with respect to using the full set of groups. For example, l get different results if I run the following two snippets of code (which should provide instead the same set of DE genes). ``` python. # Subsetting should be done by method. sc.tl.rank_genes_groups(. adata=adata,. groupby='leiden',. use_raw=False,. method='t-test',. groups=['1', '2'],. ). ```. ``` python. # Explicit subsetting. adata_f = adata[adata.obs['leiden'].isin(['1','2'])].copy(). sc.tl.rank_genes_groups(. adata=adata_f,. groupby='leiden',. use_raw=False,. method='t-test',. ). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842
https://github.com/scverse/scanpy/issues/842:107,modifiability,paramet,parameter,107,"Inconsistent results with groups parameter in rank_genes_groups; I get inconsistent results when I use the parameter groups in `rank_genes_groups`. What I want to achieve is to focus on some groups for the DE genes computation. However, the results I get using the parameter groups, to select a subset of groups, are the same, in terms of DE genes, with respect to using the full set of groups. For example, l get different results if I run the following two snippets of code (which should provide instead the same set of DE genes). ``` python. # Subsetting should be done by method. sc.tl.rank_genes_groups(. adata=adata,. groupby='leiden',. use_raw=False,. method='t-test',. groups=['1', '2'],. ). ```. ``` python. # Explicit subsetting. adata_f = adata[adata.obs['leiden'].isin(['1','2'])].copy(). sc.tl.rank_genes_groups(. adata=adata_f,. groupby='leiden',. use_raw=False,. method='t-test',. ). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842
https://github.com/scverse/scanpy/issues/842:265,modifiability,paramet,parameter,265,"Inconsistent results with groups parameter in rank_genes_groups; I get inconsistent results when I use the parameter groups in `rank_genes_groups`. What I want to achieve is to focus on some groups for the DE genes computation. However, the results I get using the parameter groups, to select a subset of groups, are the same, in terms of DE genes, with respect to using the full set of groups. For example, l get different results if I run the following two snippets of code (which should provide instead the same set of DE genes). ``` python. # Subsetting should be done by method. sc.tl.rank_genes_groups(. adata=adata,. groupby='leiden',. use_raw=False,. method='t-test',. groups=['1', '2'],. ). ```. ``` python. # Explicit subsetting. adata_f = adata[adata.obs['leiden'].isin(['1','2'])].copy(). sc.tl.rank_genes_groups(. adata=adata_f,. groupby='leiden',. use_raw=False,. method='t-test',. ). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842
https://github.com/scverse/scanpy/issues/842:669,safety,test,test,669,"Inconsistent results with groups parameter in rank_genes_groups; I get inconsistent results when I use the parameter groups in `rank_genes_groups`. What I want to achieve is to focus on some groups for the DE genes computation. However, the results I get using the parameter groups, to select a subset of groups, are the same, in terms of DE genes, with respect to using the full set of groups. For example, l get different results if I run the following two snippets of code (which should provide instead the same set of DE genes). ``` python. # Subsetting should be done by method. sc.tl.rank_genes_groups(. adata=adata,. groupby='leiden',. use_raw=False,. method='t-test',. groups=['1', '2'],. ). ```. ``` python. # Explicit subsetting. adata_f = adata[adata.obs['leiden'].isin(['1','2'])].copy(). sc.tl.rank_genes_groups(. adata=adata_f,. groupby='leiden',. use_raw=False,. method='t-test',. ). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842
https://github.com/scverse/scanpy/issues/842:888,safety,test,test,888,"Inconsistent results with groups parameter in rank_genes_groups; I get inconsistent results when I use the parameter groups in `rank_genes_groups`. What I want to achieve is to focus on some groups for the DE genes computation. However, the results I get using the parameter groups, to select a subset of groups, are the same, in terms of DE genes, with respect to using the full set of groups. For example, l get different results if I run the following two snippets of code (which should provide instead the same set of DE genes). ``` python. # Subsetting should be done by method. sc.tl.rank_genes_groups(. adata=adata,. groupby='leiden',. use_raw=False,. method='t-test',. groups=['1', '2'],. ). ```. ``` python. # Explicit subsetting. adata_f = adata[adata.obs['leiden'].isin(['1','2'])].copy(). sc.tl.rank_genes_groups(. adata=adata_f,. groupby='leiden',. use_raw=False,. method='t-test',. ). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842
https://github.com/scverse/scanpy/issues/842:669,testability,test,test,669,"Inconsistent results with groups parameter in rank_genes_groups; I get inconsistent results when I use the parameter groups in `rank_genes_groups`. What I want to achieve is to focus on some groups for the DE genes computation. However, the results I get using the parameter groups, to select a subset of groups, are the same, in terms of DE genes, with respect to using the full set of groups. For example, l get different results if I run the following two snippets of code (which should provide instead the same set of DE genes). ``` python. # Subsetting should be done by method. sc.tl.rank_genes_groups(. adata=adata,. groupby='leiden',. use_raw=False,. method='t-test',. groups=['1', '2'],. ). ```. ``` python. # Explicit subsetting. adata_f = adata[adata.obs['leiden'].isin(['1','2'])].copy(). sc.tl.rank_genes_groups(. adata=adata_f,. groupby='leiden',. use_raw=False,. method='t-test',. ). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842
https://github.com/scverse/scanpy/issues/842:888,testability,test,test,888,"Inconsistent results with groups parameter in rank_genes_groups; I get inconsistent results when I use the parameter groups in `rank_genes_groups`. What I want to achieve is to focus on some groups for the DE genes computation. However, the results I get using the parameter groups, to select a subset of groups, are the same, in terms of DE genes, with respect to using the full set of groups. For example, l get different results if I run the following two snippets of code (which should provide instead the same set of DE genes). ``` python. # Subsetting should be done by method. sc.tl.rank_genes_groups(. adata=adata,. groupby='leiden',. use_raw=False,. method='t-test',. groups=['1', '2'],. ). ```. ``` python. # Explicit subsetting. adata_f = adata[adata.obs['leiden'].isin(['1','2'])].copy(). sc.tl.rank_genes_groups(. adata=adata_f,. groupby='leiden',. use_raw=False,. method='t-test',. ). ```",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/842
https://github.com/scverse/scanpy/issues/843:6,availability,error,error,6,"Numba error in calculate_qc_metrics on a windows machine; Numba issue: https://github.com/numba/numba/issues/4529. The issue seems to come from `np.array(..., dtype=np.int)` and `sparse.csr_matrix(...).indptr.shape - 1` not having equivalent dtypes. I'm unable to reproduce on my machine, so I'm guessing this is coming from @fkoegel being on a windows machine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/843
https://github.com/scverse/scanpy/issues/843:6,performance,error,error,6,"Numba error in calculate_qc_metrics on a windows machine; Numba issue: https://github.com/numba/numba/issues/4529. The issue seems to come from `np.array(..., dtype=np.int)` and `sparse.csr_matrix(...).indptr.shape - 1` not having equivalent dtypes. I'm unable to reproduce on my machine, so I'm guessing this is coming from @fkoegel being on a windows machine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/843
https://github.com/scverse/scanpy/issues/843:6,safety,error,error,6,"Numba error in calculate_qc_metrics on a windows machine; Numba issue: https://github.com/numba/numba/issues/4529. The issue seems to come from `np.array(..., dtype=np.int)` and `sparse.csr_matrix(...).indptr.shape - 1` not having equivalent dtypes. I'm unable to reproduce on my machine, so I'm guessing this is coming from @fkoegel being on a windows machine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/843
https://github.com/scverse/scanpy/issues/843:6,usability,error,error,6,"Numba error in calculate_qc_metrics on a windows machine; Numba issue: https://github.com/numba/numba/issues/4529. The issue seems to come from `np.array(..., dtype=np.int)` and `sparse.csr_matrix(...).indptr.shape - 1` not having equivalent dtypes. I'm unable to reproduce on my machine, so I'm guessing this is coming from @fkoegel being on a windows machine.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/843
https://github.com/scverse/scanpy/pull/844:167,availability,down,downside,167,"Use cacheing and parallelism for qc metrics; This simplifies `top_segment_proportions_sparse_csr` by using improvements in numba which allow cacheing parallel code. A downside of this is it takes a really long time to compile on first run, which might be off-putting. Side note: I accidentally ran formatting before committing, so some other lines got changed too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844
https://github.com/scverse/scanpy/pull/844:298,interoperability,format,formatting,298,"Use cacheing and parallelism for qc metrics; This simplifies `top_segment_proportions_sparse_csr` by using improvements in numba which allow cacheing parallel code. A downside of this is it takes a really long time to compile on first run, which might be off-putting. Side note: I accidentally ran formatting before committing, so some other lines got changed too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844
https://github.com/scverse/scanpy/pull/844:4,performance,cach,cacheing,4,"Use cacheing and parallelism for qc metrics; This simplifies `top_segment_proportions_sparse_csr` by using improvements in numba which allow cacheing parallel code. A downside of this is it takes a really long time to compile on first run, which might be off-putting. Side note: I accidentally ran formatting before committing, so some other lines got changed too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844
https://github.com/scverse/scanpy/pull/844:17,performance,parallel,parallelism,17,"Use cacheing and parallelism for qc metrics; This simplifies `top_segment_proportions_sparse_csr` by using improvements in numba which allow cacheing parallel code. A downside of this is it takes a really long time to compile on first run, which might be off-putting. Side note: I accidentally ran formatting before committing, so some other lines got changed too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844
https://github.com/scverse/scanpy/pull/844:141,performance,cach,cacheing,141,"Use cacheing and parallelism for qc metrics; This simplifies `top_segment_proportions_sparse_csr` by using improvements in numba which allow cacheing parallel code. A downside of this is it takes a really long time to compile on first run, which might be off-putting. Side note: I accidentally ran formatting before committing, so some other lines got changed too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844
https://github.com/scverse/scanpy/pull/844:150,performance,parallel,parallel,150,"Use cacheing and parallelism for qc metrics; This simplifies `top_segment_proportions_sparse_csr` by using improvements in numba which allow cacheing parallel code. A downside of this is it takes a really long time to compile on first run, which might be off-putting. Side note: I accidentally ran formatting before committing, so some other lines got changed too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844
https://github.com/scverse/scanpy/pull/844:210,performance,time,time,210,"Use cacheing and parallelism for qc metrics; This simplifies `top_segment_proportions_sparse_csr` by using improvements in numba which allow cacheing parallel code. A downside of this is it takes a really long time to compile on first run, which might be off-putting. Side note: I accidentally ran formatting before committing, so some other lines got changed too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844
https://github.com/scverse/scanpy/pull/844:281,safety,accid,accidentally,281,"Use cacheing and parallelism for qc metrics; This simplifies `top_segment_proportions_sparse_csr` by using improvements in numba which allow cacheing parallel code. A downside of this is it takes a really long time to compile on first run, which might be off-putting. Side note: I accidentally ran formatting before committing, so some other lines got changed too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844
https://github.com/scverse/scanpy/pull/844:50,testability,simpl,simplifies,50,"Use cacheing and parallelism for qc metrics; This simplifies `top_segment_proportions_sparse_csr` by using improvements in numba which allow cacheing parallel code. A downside of this is it takes a really long time to compile on first run, which might be off-putting. Side note: I accidentally ran formatting before committing, so some other lines got changed too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844
https://github.com/scverse/scanpy/pull/844:50,usability,simpl,simplifies,50,"Use cacheing and parallelism for qc metrics; This simplifies `top_segment_proportions_sparse_csr` by using improvements in numba which allow cacheing parallel code. A downside of this is it takes a really long time to compile on first run, which might be off-putting. Side note: I accidentally ran formatting before committing, so some other lines got changed too.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/844
https://github.com/scverse/scanpy/pull/845:4,deployability,releas,release,4,Add release note for queries;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/845
https://github.com/scverse/scanpy/pull/846:136,availability,down,down,136,"Run static analysis concurrently; Runs static analysis concurrently with tests, while currently static analysis is run first. This cuts down on total test time, and will always test both correctness and style.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846
https://github.com/scverse/scanpy/pull/846:86,energy efficiency,current,currently,86,"Run static analysis concurrently; Runs static analysis concurrently with tests, while currently static analysis is run first. This cuts down on total test time, and will always test both correctness and style.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846
https://github.com/scverse/scanpy/pull/846:20,performance,concurren,concurrently,20,"Run static analysis concurrently; Runs static analysis concurrently with tests, while currently static analysis is run first. This cuts down on total test time, and will always test both correctness and style.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846
https://github.com/scverse/scanpy/pull/846:55,performance,concurren,concurrently,55,"Run static analysis concurrently; Runs static analysis concurrently with tests, while currently static analysis is run first. This cuts down on total test time, and will always test both correctness and style.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846
https://github.com/scverse/scanpy/pull/846:155,performance,time,time,155,"Run static analysis concurrently; Runs static analysis concurrently with tests, while currently static analysis is run first. This cuts down on total test time, and will always test both correctness and style.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846
https://github.com/scverse/scanpy/pull/846:73,safety,test,tests,73,"Run static analysis concurrently; Runs static analysis concurrently with tests, while currently static analysis is run first. This cuts down on total test time, and will always test both correctness and style.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846
https://github.com/scverse/scanpy/pull/846:150,safety,test,test,150,"Run static analysis concurrently; Runs static analysis concurrently with tests, while currently static analysis is run first. This cuts down on total test time, and will always test both correctness and style.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846
https://github.com/scverse/scanpy/pull/846:177,safety,test,test,177,"Run static analysis concurrently; Runs static analysis concurrently with tests, while currently static analysis is run first. This cuts down on total test time, and will always test both correctness and style.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846
https://github.com/scverse/scanpy/pull/846:73,testability,test,tests,73,"Run static analysis concurrently; Runs static analysis concurrently with tests, while currently static analysis is run first. This cuts down on total test time, and will always test both correctness and style.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846
https://github.com/scverse/scanpy/pull/846:150,testability,test,test,150,"Run static analysis concurrently; Runs static analysis concurrently with tests, while currently static analysis is run first. This cuts down on total test time, and will always test both correctness and style.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846
https://github.com/scverse/scanpy/pull/846:177,testability,test,test,177,"Run static analysis concurrently; Runs static analysis concurrently with tests, while currently static analysis is run first. This cuts down on total test time, and will always test both correctness and style.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/846
https://github.com/scverse/scanpy/pull/847:102,availability,down,down,102,"Add cache_compression setting; Default: lzf. see theislab/anndata#123. To review without being bogged down with whitespace changes, check: https://github.com/theislab/scanpy/pull/847/files?w=1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/847
https://github.com/scverse/scanpy/pull/847:74,safety,review,review,74,"Add cache_compression setting; Default: lzf. see theislab/anndata#123. To review without being bogged down with whitespace changes, check: https://github.com/theislab/scanpy/pull/847/files?w=1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/847
https://github.com/scverse/scanpy/pull/847:74,testability,review,review,74,"Add cache_compression setting; Default: lzf. see theislab/anndata#123. To review without being bogged down with whitespace changes, check: https://github.com/theislab/scanpy/pull/847/files?w=1",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/847
https://github.com/scverse/scanpy/issues/849:17,deployability,version,version,17,"Unpin matplotlib version once possible; As @fidelram said in https://github.com/theislab/scanpy/pull/661#issuecomment-496144015, we have to wait until matplotlib/matplotlib#14298 is fixed. It seems to be in matplotlibs 3.1.2 milestone, so maybe we can just set the dependency to matplotlib == 3.0.0 or matplotlib >= 3.1.2. This originally came up in #663, and then later in e.g. #787",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849
https://github.com/scverse/scanpy/issues/849:266,deployability,depend,dependency,266,"Unpin matplotlib version once possible; As @fidelram said in https://github.com/theislab/scanpy/pull/661#issuecomment-496144015, we have to wait until matplotlib/matplotlib#14298 is fixed. It seems to be in matplotlibs 3.1.2 milestone, so maybe we can just set the dependency to matplotlib == 3.0.0 or matplotlib >= 3.1.2. This originally came up in #663, and then later in e.g. #787",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849
https://github.com/scverse/scanpy/issues/849:17,integrability,version,version,17,"Unpin matplotlib version once possible; As @fidelram said in https://github.com/theislab/scanpy/pull/661#issuecomment-496144015, we have to wait until matplotlib/matplotlib#14298 is fixed. It seems to be in matplotlibs 3.1.2 milestone, so maybe we can just set the dependency to matplotlib == 3.0.0 or matplotlib >= 3.1.2. This originally came up in #663, and then later in e.g. #787",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849
https://github.com/scverse/scanpy/issues/849:266,integrability,depend,dependency,266,"Unpin matplotlib version once possible; As @fidelram said in https://github.com/theislab/scanpy/pull/661#issuecomment-496144015, we have to wait until matplotlib/matplotlib#14298 is fixed. It seems to be in matplotlibs 3.1.2 milestone, so maybe we can just set the dependency to matplotlib == 3.0.0 or matplotlib >= 3.1.2. This originally came up in #663, and then later in e.g. #787",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849
https://github.com/scverse/scanpy/issues/849:17,modifiability,version,version,17,"Unpin matplotlib version once possible; As @fidelram said in https://github.com/theislab/scanpy/pull/661#issuecomment-496144015, we have to wait until matplotlib/matplotlib#14298 is fixed. It seems to be in matplotlibs 3.1.2 milestone, so maybe we can just set the dependency to matplotlib == 3.0.0 or matplotlib >= 3.1.2. This originally came up in #663, and then later in e.g. #787",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849
https://github.com/scverse/scanpy/issues/849:266,modifiability,depend,dependency,266,"Unpin matplotlib version once possible; As @fidelram said in https://github.com/theislab/scanpy/pull/661#issuecomment-496144015, we have to wait until matplotlib/matplotlib#14298 is fixed. It seems to be in matplotlibs 3.1.2 milestone, so maybe we can just set the dependency to matplotlib == 3.0.0 or matplotlib >= 3.1.2. This originally came up in #663, and then later in e.g. #787",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849
https://github.com/scverse/scanpy/issues/849:266,safety,depend,dependency,266,"Unpin matplotlib version once possible; As @fidelram said in https://github.com/theislab/scanpy/pull/661#issuecomment-496144015, we have to wait until matplotlib/matplotlib#14298 is fixed. It seems to be in matplotlibs 3.1.2 milestone, so maybe we can just set the dependency to matplotlib == 3.0.0 or matplotlib >= 3.1.2. This originally came up in #663, and then later in e.g. #787",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849
https://github.com/scverse/scanpy/issues/849:266,testability,depend,dependency,266,"Unpin matplotlib version once possible; As @fidelram said in https://github.com/theislab/scanpy/pull/661#issuecomment-496144015, we have to wait until matplotlib/matplotlib#14298 is fixed. It seems to be in matplotlibs 3.1.2 milestone, so maybe we can just set the dependency to matplotlib == 3.0.0 or matplotlib >= 3.1.2. This originally came up in #663, and then later in e.g. #787",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/849
https://github.com/scverse/scanpy/issues/850:1122,integrability,pub,public-supool,1122,"sc.pl.correlation_matrix `save` parameter broken; I followed the Plot correlation of [tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html) and I cannot save the figure. ps: the previous code in that tutorial worked perfectly. ### My code:. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from matplotlib import rcParams. import matplotlib.pyplot as plt. fig = plt.figure(). sc.tl.dendrogram(adata, 'louvain', n_pcs=30). sc.pl.correlation_matrix(adata, 'louvain', save='Correlation--57.png'). ```. ```console. [<matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efc98390>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efa84dd8>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3f0157320>]. ```. ```python. ax = sc.pl.correlation_matrix(adata, 'louvain', save='Correlation--57.png'). ax. ```. ```console. [<matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efc98390>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efa84dd8>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3f0157320>]. ```. ```python. fig.savefig('/public-supool/home/wuhaoda/Scanpy/figures/57.png'). ```. ### And I can only get ""57.png"" rather than ""Correlation--57.png"". More inportantly, the ""57.png"" is purely blank as attached. ![57](https://user-images.githubusercontent.com/49429496/65104442-029bcc80-da04-11e9-96e3-0563c4145af1.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/850
https://github.com/scverse/scanpy/issues/850:32,modifiability,paramet,parameter,32,"sc.pl.correlation_matrix `save` parameter broken; I followed the Plot correlation of [tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html) and I cannot save the figure. ps: the previous code in that tutorial worked perfectly. ### My code:. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from matplotlib import rcParams. import matplotlib.pyplot as plt. fig = plt.figure(). sc.tl.dendrogram(adata, 'louvain', n_pcs=30). sc.pl.correlation_matrix(adata, 'louvain', save='Correlation--57.png'). ```. ```console. [<matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efc98390>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efa84dd8>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3f0157320>]. ```. ```python. ax = sc.pl.correlation_matrix(adata, 'louvain', save='Correlation--57.png'). ax. ```. ```console. [<matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efc98390>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efa84dd8>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3f0157320>]. ```. ```python. fig.savefig('/public-supool/home/wuhaoda/Scanpy/figures/57.png'). ```. ### And I can only get ""57.png"" rather than ""Correlation--57.png"". More inportantly, the ""57.png"" is purely blank as attached. ![57](https://user-images.githubusercontent.com/49429496/65104442-029bcc80-da04-11e9-96e3-0563c4145af1.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/850
https://github.com/scverse/scanpy/issues/850:148,usability,visual,visualizing-marker-genes,148,"sc.pl.correlation_matrix `save` parameter broken; I followed the Plot correlation of [tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html) and I cannot save the figure. ps: the previous code in that tutorial worked perfectly. ### My code:. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from matplotlib import rcParams. import matplotlib.pyplot as plt. fig = plt.figure(). sc.tl.dendrogram(adata, 'louvain', n_pcs=30). sc.pl.correlation_matrix(adata, 'louvain', save='Correlation--57.png'). ```. ```console. [<matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efc98390>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efa84dd8>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3f0157320>]. ```. ```python. ax = sc.pl.correlation_matrix(adata, 'louvain', save='Correlation--57.png'). ax. ```. ```console. [<matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efc98390>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efa84dd8>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3f0157320>]. ```. ```python. fig.savefig('/public-supool/home/wuhaoda/Scanpy/figures/57.png'). ```. ### And I can only get ""57.png"" rather than ""Correlation--57.png"". More inportantly, the ""57.png"" is purely blank as attached. ![57](https://user-images.githubusercontent.com/49429496/65104442-029bcc80-da04-11e9-96e3-0563c4145af1.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/850
https://github.com/scverse/scanpy/issues/850:1320,usability,user,user-images,1320,"sc.pl.correlation_matrix `save` parameter broken; I followed the Plot correlation of [tutorial](https://scanpy-tutorials.readthedocs.io/en/latest/visualizing-marker-genes.html) and I cannot save the figure. ps: the previous code in that tutorial worked perfectly. ### My code:. ```python. import numpy as np. import pandas as pd. import scanpy as sc. from matplotlib import rcParams. import matplotlib.pyplot as plt. fig = plt.figure(). sc.tl.dendrogram(adata, 'louvain', n_pcs=30). sc.pl.correlation_matrix(adata, 'louvain', save='Correlation--57.png'). ```. ```console. [<matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efc98390>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efa84dd8>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3f0157320>]. ```. ```python. ax = sc.pl.correlation_matrix(adata, 'louvain', save='Correlation--57.png'). ax. ```. ```console. [<matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efc98390>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3efa84dd8>,. <matplotlib.axes._subplots.AxesSubplot object at 0x7fd3f0157320>]. ```. ```python. fig.savefig('/public-supool/home/wuhaoda/Scanpy/figures/57.png'). ```. ### And I can only get ""57.png"" rather than ""Correlation--57.png"". More inportantly, the ""57.png"" is purely blank as attached. ![57](https://user-images.githubusercontent.com/49429496/65104442-029bcc80-da04-11e9-96e3-0563c4145af1.png)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/850
https://github.com/scverse/scanpy/issues/851:430,availability,error,errors,430,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:0,deployability,modul,module,0,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:67,deployability,version,version,67,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:527,deployability,modul,module,527,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:567,deployability,Modul,ModuleNotFoundError,567,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:591,deployability,modul,module,591,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:631,deployability,API,API,631,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:67,integrability,version,version,67,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:631,integrability,API,API,631,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:631,interoperability,API,API,631,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:0,modifiability,modul,module,0,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:67,modifiability,version,version,67,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:527,modifiability,modul,module,527,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:567,modifiability,Modul,ModuleNotFoundError,567,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:591,modifiability,modul,module,591,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:430,performance,error,errors,430,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:0,safety,modul,module,0,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:430,safety,error,errors,430,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:505,safety,test,test,505,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:527,safety,modul,module,527,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:567,safety,Modul,ModuleNotFoundError,567,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:591,safety,modul,module,591,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:505,testability,test,test,505,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/851:430,usability,error,errors,430,"module 'scanpy' has no attribute 'get'; Hello, . I am using scanpy version 1.4 from bioconda. I am running one of the examples from the code to query datasets - . ```. import scanpy as sc. pbmc = sc.datasets.pbmc3k(). print(pbmc). plotdf = sc.get.obs_df(. pbmc,. keys=[""CD8B"", ""n_genes""],. obsm_keys=[(""X_umap"", 0), (""X_umap"", 1)]. ). plotdf.plot.scatter(""X_umap0"", ""X_umap1"", c=""CD8B""). ```. the library can read the dataset but errors out saying `scanpy` has no attribute `get`. I even tried . ```File ""test.py"", line 2, in <module>. from scanpy.get import obs_df. ModuleNotFoundError: No module named 'scanpy.get'. ```. Did the API change for any of this ? Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/851
https://github.com/scverse/scanpy/issues/852:30,availability,error,error,30,"RuntimeError: libpng signaled error; Hello Everyone ... . I tried to run the code in this link. . https://github.com/theislab/paga/blob/master/planaria/planaria.ipynb. I didn't change anything in the code. but still, I have an issue. Can please anyone help me to fix this? . ![image](https://user-images.githubusercontent.com/48261734/65398863-361d8480-dd7f-11e9-9011-cc3ea1242ffa.png). matplotlib == 2.2.3. ![image](https://user-images.githubusercontent.com/48261734/65398560-91e70e00-dd7d-11e9-9f5a-a2fde6c4e20b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:30,performance,error,error,30,"RuntimeError: libpng signaled error; Hello Everyone ... . I tried to run the code in this link. . https://github.com/theislab/paga/blob/master/planaria/planaria.ipynb. I didn't change anything in the code. but still, I have an issue. Can please anyone help me to fix this? . ![image](https://user-images.githubusercontent.com/48261734/65398863-361d8480-dd7f-11e9-9011-cc3ea1242ffa.png). matplotlib == 2.2.3. ![image](https://user-images.githubusercontent.com/48261734/65398560-91e70e00-dd7d-11e9-9f5a-a2fde6c4e20b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:30,safety,error,error,30,"RuntimeError: libpng signaled error; Hello Everyone ... . I tried to run the code in this link. . https://github.com/theislab/paga/blob/master/planaria/planaria.ipynb. I didn't change anything in the code. but still, I have an issue. Can please anyone help me to fix this? . ![image](https://user-images.githubusercontent.com/48261734/65398863-361d8480-dd7f-11e9-9011-cc3ea1242ffa.png). matplotlib == 2.2.3. ![image](https://user-images.githubusercontent.com/48261734/65398560-91e70e00-dd7d-11e9-9f5a-a2fde6c4e20b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:21,security,sign,signaled,21,"RuntimeError: libpng signaled error; Hello Everyone ... . I tried to run the code in this link. . https://github.com/theislab/paga/blob/master/planaria/planaria.ipynb. I didn't change anything in the code. but still, I have an issue. Can please anyone help me to fix this? . ![image](https://user-images.githubusercontent.com/48261734/65398863-361d8480-dd7f-11e9-9011-cc3ea1242ffa.png). matplotlib == 2.2.3. ![image](https://user-images.githubusercontent.com/48261734/65398560-91e70e00-dd7d-11e9-9f5a-a2fde6c4e20b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:143,testability,plan,planaria,143,"RuntimeError: libpng signaled error; Hello Everyone ... . I tried to run the code in this link. . https://github.com/theislab/paga/blob/master/planaria/planaria.ipynb. I didn't change anything in the code. but still, I have an issue. Can please anyone help me to fix this? . ![image](https://user-images.githubusercontent.com/48261734/65398863-361d8480-dd7f-11e9-9011-cc3ea1242ffa.png). matplotlib == 2.2.3. ![image](https://user-images.githubusercontent.com/48261734/65398560-91e70e00-dd7d-11e9-9f5a-a2fde6c4e20b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:152,testability,plan,planaria,152,"RuntimeError: libpng signaled error; Hello Everyone ... . I tried to run the code in this link. . https://github.com/theislab/paga/blob/master/planaria/planaria.ipynb. I didn't change anything in the code. but still, I have an issue. Can please anyone help me to fix this? . ![image](https://user-images.githubusercontent.com/48261734/65398863-361d8480-dd7f-11e9-9011-cc3ea1242ffa.png). matplotlib == 2.2.3. ![image](https://user-images.githubusercontent.com/48261734/65398560-91e70e00-dd7d-11e9-9f5a-a2fde6c4e20b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:30,usability,error,error,30,"RuntimeError: libpng signaled error; Hello Everyone ... . I tried to run the code in this link. . https://github.com/theislab/paga/blob/master/planaria/planaria.ipynb. I didn't change anything in the code. but still, I have an issue. Can please anyone help me to fix this? . ![image](https://user-images.githubusercontent.com/48261734/65398863-361d8480-dd7f-11e9-9011-cc3ea1242ffa.png). matplotlib == 2.2.3. ![image](https://user-images.githubusercontent.com/48261734/65398560-91e70e00-dd7d-11e9-9f5a-a2fde6c4e20b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:252,usability,help,help,252,"RuntimeError: libpng signaled error; Hello Everyone ... . I tried to run the code in this link. . https://github.com/theislab/paga/blob/master/planaria/planaria.ipynb. I didn't change anything in the code. but still, I have an issue. Can please anyone help me to fix this? . ![image](https://user-images.githubusercontent.com/48261734/65398863-361d8480-dd7f-11e9-9011-cc3ea1242ffa.png). matplotlib == 2.2.3. ![image](https://user-images.githubusercontent.com/48261734/65398560-91e70e00-dd7d-11e9-9f5a-a2fde6c4e20b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:292,usability,user,user-images,292,"RuntimeError: libpng signaled error; Hello Everyone ... . I tried to run the code in this link. . https://github.com/theislab/paga/blob/master/planaria/planaria.ipynb. I didn't change anything in the code. but still, I have an issue. Can please anyone help me to fix this? . ![image](https://user-images.githubusercontent.com/48261734/65398863-361d8480-dd7f-11e9-9011-cc3ea1242ffa.png). matplotlib == 2.2.3. ![image](https://user-images.githubusercontent.com/48261734/65398560-91e70e00-dd7d-11e9-9f5a-a2fde6c4e20b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/852:425,usability,user,user-images,425,"RuntimeError: libpng signaled error; Hello Everyone ... . I tried to run the code in this link. . https://github.com/theislab/paga/blob/master/planaria/planaria.ipynb. I didn't change anything in the code. but still, I have an issue. Can please anyone help me to fix this? . ![image](https://user-images.githubusercontent.com/48261734/65398863-361d8480-dd7f-11e9-9011-cc3ea1242ffa.png). matplotlib == 2.2.3. ![image](https://user-images.githubusercontent.com/48261734/65398560-91e70e00-dd7d-11e9-9f5a-a2fde6c4e20b.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/852
https://github.com/scverse/scanpy/issues/853:205,availability,down,downloaded,205,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:345,availability,error,errors,345,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:458,availability,error,error,458,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:500,availability,error,error,500,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:624,availability,error,error,624,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:16,deployability,modul,module,16,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:220,deployability,instal,installed,220,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:252,deployability,version,version,252,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:369,deployability,modul,module,369,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:475,deployability,modul,module,475,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:252,integrability,version,version,252,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:506,integrability,messag,message,506,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:545,integrability,messag,message,545,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:506,interoperability,messag,message,506,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:545,interoperability,messag,message,545,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:16,modifiability,modul,module,16,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:252,modifiability,version,version,252,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:369,modifiability,modul,module,369,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:475,modifiability,modul,module,475,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:345,performance,error,errors,345,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:458,performance,error,error,458,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:500,performance,error,error,500,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:624,performance,error,error,624,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:465,reliability,Doe,Does,465,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:16,safety,modul,module,16,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:345,safety,error,errors,345,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:369,safety,modul,module,369,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:458,safety,error,error,458,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:475,safety,modul,module,475,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:500,safety,error,error,500,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:624,safety,error,error,624,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:301,usability,command,command,301,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:345,usability,error,errors,345,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:458,usability,error,error,458,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:500,usability,error,error,500,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/853:624,usability,error,error,624,"AttributeError: module 'tables' has no attribute 'which_lib_version'; Hello,. I am a beginner in the bioinformatics field. I have been used scanpy for one year, and I really like it. But yesterday after I downloaded and installed the new developmental version from scanpy github, when I ran the first command ""import scanpy as sc"", it displayed errors :AttributeError: module 'tables' has no attribute 'which_lib_version'. I don't know how to deal with this error. Does the ""module 'tables' "" in the error message mean pytables? After I saw the message, I reinstalled both pytables and scanpy through conda channel. But the error is not fixed. I thus wish to get some suggestions. Thank you so much!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/853
https://github.com/scverse/scanpy/issues/854:1375,availability,stead,steady,1375,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:1382,availability,state,states,1382,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:1487,availability,stead,steady,1487,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:1494,availability,state,state,1494,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:23,deployability,toggl,toggle-switch,23,"Incorrect dpt on dense toggle-switch; Hi, thanks for this great library! I was playing around with data simulated from the `toggleswitch` model and was very much surprised to see the result of running the `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:124,deployability,toggl,toggleswitch,124,"Incorrect dpt on dense toggle-switch; Hi, thanks for this great library! I was playing around with data simulated from the `toggleswitch` model and was very much surprised to see the result of running the `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:306,deployability,toggl,toggleswitch,306,"Incorrect dpt on dense toggle-switch; Hi, thanks for this great library! I was playing around with data simulated from the `toggleswitch` model and was very much surprised to see the result of running the `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:693,deployability,toggl,toggleswitch,693,"Incorrect dpt on dense toggle-switch; Hi, thanks for this great library! I was playing around with data simulated from the `toggleswitch` model and was very much surprised to see the result of running the `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:138,energy efficiency,model,model,138,"Incorrect dpt on dense toggle-switch; Hi, thanks for this great library! I was playing around with data simulated from the `toggleswitch` model and was very much surprised to see the result of running the `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:707,energy efficiency,model,model,707,"Incorrect dpt on dense toggle-switch; Hi, thanks for this great library! I was playing around with data simulated from the `toggleswitch` model and was very much surprised to see the result of running the `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:1382,integrability,state,states,1382,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:1494,integrability,state,state,1494,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:2038,modifiability,exten,extent,2038,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:722,performance,time,time,722,"Incorrect dpt on dense toggle-switch; Hi, thanks for this great library! I was playing around with data simulated from the `toggleswitch` model and was very much surprised to see the result of running the `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:928,performance,time,time,928,"Incorrect dpt on dense toggle-switch; Hi, thanks for this great library! I was playing around with data simulated from the `toggleswitch` model and was very much surprised to see the result of running the `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:1425,performance,time,time,1425,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:1902,performance,time,time,1902,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:1577,reliability,doe,doesn,1577,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:2069,safety,test,tests,2069,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:138,security,model,model,138,"Incorrect dpt on dense toggle-switch; Hi, thanks for this great library! I was playing around with data simulated from the `toggleswitch` model and was very much surprised to see the result of running the `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:707,security,model,model,707,"Incorrect dpt on dense toggle-switch; Hi, thanks for this great library! I was playing around with data simulated from the `toggleswitch` model and was very much surprised to see the result of running the `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:104,testability,simul,simulated,104,"Incorrect dpt on dense toggle-switch; Hi, thanks for this great library! I was playing around with data simulated from the `toggleswitch` model and was very much surprised to see the result of running the `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:217,testability,simul,simulated,217,"Incorrect dpt on dense toggle-switch; Hi, thanks for this great library! I was playing around with data simulated from the `toggleswitch` model and was very much surprised to see the result of running the `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:646,testability,simul,simulating,646,"Incorrect dpt on dense toggle-switch; Hi, thanks for this great library! I was playing around with data simulated from the `toggleswitch` model and was very much surprised to see the result of running the `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region c",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:1518,testability,simul,simulation,1518,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:2069,testability,test,tests,2069,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:1081,usability,user,user-images,1081,"around with data simulated from the `toggleswitch` model and was very much surprised to see the result of running the `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could sti",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:1198,usability,user,user-images,1198," `dpt` on my simulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falex",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:1775,usability,help,help,1775,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/854:1935,usability,intuit,intuition,1935,"imulated data. The script I am running is the following:. ```python. adata = sc.tl.sim('toggleswitch',. nrRealizations=5,. tmax=200,. branching=False). sc.pl.sim(adata). sc.pp.neighbors(adata). sc.tl.louvain(adata). sc.tl.diffmap(adata). adata.uns['iroot'] = 0. sc.tl.dpt(adata). sc.tl.umap(adata). sc.pl.umap(adata,. edges=True,. edges_width=1,. color=['louvain', 'dpt_pseudotime'],. legend_loc='on data'). ```. In short, I am simulating five realizations of the two-genes `toggleswitch` model with 200 time steps in each realization and allowing to have multiple realizations from the same branch (otherwise there would be only two possible realizations). The problem I face is that the result of the pseudo-time computation seems (and is, since I know how the data was generated) wrong. Here are the plots displayed when running this script:. ![image](https://user-images.githubusercontent.com/6624306/65698989-b0028600-e07d-11e9-94ec-1ef5ac690658.png). and . ![image](https://user-images.githubusercontent.com/6624306/65698967-a6791e00-e07d-11e9-9202-8603e49c4675.png). As you can see, for some reason, there is like a ""jump"" being made between the two steady states of the system, resulting in a final time (t=1) being somewhere between the starting point and one steady state. Running the same simulation with `branching=True` and only two realizations doesn't seem to have this issue. Since I'm a working student at the ICB, I talked to some people here about it, and we found out the following:. - using `method='gauss'` in the `neighbors` call did help in some situations, but playing with more random datasets later on invalidated this hypothesis. - using less data points (time steps per realization), the intuition was that the high density of points in similar region could have an impact, did work to some extent, but after running some tests, I could still find very pathological situations. It would be great if someone could look into this, I was told to mention @falexwolf . Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/854
https://github.com/scverse/scanpy/issues/855:7,availability,error,error,7,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:83,availability,error,error,83,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:193,deployability,modul,module,193,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:280,deployability,modul,module,280,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:420,deployability,modul,module,420,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:631,deployability,instal,installed,631,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:658,deployability,instal,install,658,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:89,integrability,messag,message,89,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:89,interoperability,messag,message,89,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:598,interoperability,platform,platform,598,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:193,modifiability,modul,module,193,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:238,modifiability,pac,packages,238,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:280,modifiability,modul,module,280,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:380,modifiability,pac,packages,380,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:420,modifiability,modul,module,420,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:498,modifiability,pac,packages,498,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:7,performance,error,error,7,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:83,performance,error,error,83,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:7,safety,error,error,7,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:83,safety,error,error,83,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:193,safety,modul,module,193,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:280,safety,modul,module,280,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:420,safety,modul,module,420,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:129,testability,Trace,Traceback,129,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:7,usability,error,error,7,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/855:83,usability,error,error,83,"import error; When I tried to import scanpy into python 3.5.2, I got the following error message,. ```. >>> import scanpy as sc. Traceback (most recent call last):. File ""<stdin>"", line 1, in <module>. File ""/usr/local/lib/python3.5/dist-packages/scanpy/__init__.py"", line 3, in <module>. from .utils import check_versions, annotate_doc_types. File ""/usr/local/lib/python3.5/dist-packages/scanpy/utils.py"", line 18, in <module>. from ._settings import settings. File ""/usr/local/lib/python3.5/dist-packages/scanpy/_settings.py"", line 351. f'{k} = {v!r}'. ^. SyntaxError: invalid syntax. ```. My OS platform is `Ubuntu 16.04` and I installed `scanpy` by `pip install scanpy`. How could I resolve this issue? Thanks.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/855
https://github.com/scverse/scanpy/issues/856:1335,energy efficiency,load,loading,1335,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:1063,interoperability,format,format,1063,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:1113,interoperability,format,format,1113,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:1212,interoperability,format,format,1212,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:1277,interoperability,format,formats,1277,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:120,modifiability,maintain,maintaining,120,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:1324,performance,disk,disk,1324,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:1335,performance,load,loading,1335,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:1343,performance,time,time,1343,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:1352,performance,memor,memory,1352,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:120,safety,maintain,maintaining,120,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:31,security,team,team,31,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:545,security,auth,author,545,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:226,usability,help,helpful,226,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:309,usability,stop,stop,309,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:559,usability,tool,tool,559,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:1008,usability,support,supporting,1008,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/issues/856:1352,usability,memor,memory,1352,"Tutorial on quantification; Hi team Scanpy,. Thanks for developing this awesome suite of library and above all actively maintaining it. I found [this](https://scanpy.readthedocs.io/en/stable/tutorials.html) tutorial page very helpful to get started for exploring the world of single-cell in python, using one stop libraries of scanpy. I am curious if you are interested in adding some tutorials related to quantification of the RNA-seq data i.e. in scanpy world it would be preprocessing step to generate the cell-v-gene counts. I am one of the author of the tool [alevin](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1670-y), we developed for quantifying 3' droplet based single cell data, somewhat in the line of Cellranger, although ours is a principled approach to consume the gene multi-mapping reads instead of dropping them all together. I have been writing some of the tutorials myself [here](https://combine-lab.github.io/alevin-tutorial/) but was wondering is there a way of supporting them directly in scanpy. The default output format (cell-v-gene matrix) of alevin is a binary format but it would be interesting to check if we can directly dump or post process it to `annData`format. I have been doing some analysis comparing various output formats (h5, mtx, csv, loom Eds) through their disk size, loading time and memory usage [here](https://github.com/COMBINE-lab/EDS). Looking forward to hearing back.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/856
https://github.com/scverse/scanpy/pull/857:233,energy efficiency,current,current,233,"Improve accuracy and speed of _get_mean_var; This is a PR I've been meaning to get around to for a while. Previously discussed with @fidelram and @falexwolf. This calculates mean and variance for sparse matrices much faster than the current implementation. In addition, this accumulates to 64bit floats, which can greatly increase accuracy for large datasets and values. Some numbers using 50k cells from tabula muris using the call `sc.pp._utils._get_mean_var`. | | speed | memory (`%memit`) |. |--|-------|---------|. |current master| ~6.2 seconds | ~4.7 GB |. |this PR | 1.2 seconds | 0.5 MB |. Pretty good improvements. For accuracy, I'll compare against `sklearn.utils.sparsefuncs.mean_variance_axis`:. ```python. import numba. import numpy as np. from scipy import sparse. from sklearn.utils.sparsefuncs import mean_variance_axis. import matplotlib.pyplot as plt. import seaborn as sns. csr64 = sparse.random(10000, 1000, format=""csr"", dtype=np.float64). csr32 = csr64.astype(np.float32). m_my64, v_my64 = sparse_mean_variance_axis(csr64, 0). m_sk64, v_sk64 = mean_variance_axis(csr64, 0). m_my32, v_my32 = sparse_mean_variance_axis(csr32, 0). m_sk32, v_sk32 = mean_variance_axis(csr32, 0). sns.distplot(m_sk64 - m_sk32, color=""blue""). sns.distplot(m_my64 - m_my32, color=""red""). plt.show(). ```. <details>. <summary> plot </summary>. ![residual_mean](https://user-images.githubusercontent.com/8238804/65942102-ed955380-e46f-11e9-8293-334f1e7d6488.png). </details>. ```python. sns.distplot(m_sk64 - m_sk32, color=""blue""). sns.distplot(m_my64 - m_my32, color=""red""). plt.show(). ```. <details>. <summary> plot </summary>. ![residual_var](https://user-images.githubusercontent.com/8238804/65942056-d0608500-e46f-11e9-86da-58f89371c10f.png). </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:521,energy efficiency,current,current,521,"Improve accuracy and speed of _get_mean_var; This is a PR I've been meaning to get around to for a while. Previously discussed with @fidelram and @falexwolf. This calculates mean and variance for sparse matrices much faster than the current implementation. In addition, this accumulates to 64bit floats, which can greatly increase accuracy for large datasets and values. Some numbers using 50k cells from tabula muris using the call `sc.pp._utils._get_mean_var`. | | speed | memory (`%memit`) |. |--|-------|---------|. |current master| ~6.2 seconds | ~4.7 GB |. |this PR | 1.2 seconds | 0.5 MB |. Pretty good improvements. For accuracy, I'll compare against `sklearn.utils.sparsefuncs.mean_variance_axis`:. ```python. import numba. import numpy as np. from scipy import sparse. from sklearn.utils.sparsefuncs import mean_variance_axis. import matplotlib.pyplot as plt. import seaborn as sns. csr64 = sparse.random(10000, 1000, format=""csr"", dtype=np.float64). csr32 = csr64.astype(np.float32). m_my64, v_my64 = sparse_mean_variance_axis(csr64, 0). m_sk64, v_sk64 = mean_variance_axis(csr64, 0). m_my32, v_my32 = sparse_mean_variance_axis(csr32, 0). m_sk32, v_sk32 = mean_variance_axis(csr32, 0). sns.distplot(m_sk64 - m_sk32, color=""blue""). sns.distplot(m_my64 - m_my32, color=""red""). plt.show(). ```. <details>. <summary> plot </summary>. ![residual_mean](https://user-images.githubusercontent.com/8238804/65942102-ed955380-e46f-11e9-8293-334f1e7d6488.png). </details>. ```python. sns.distplot(m_sk64 - m_sk32, color=""blue""). sns.distplot(m_my64 - m_my32, color=""red""). plt.show(). ```. <details>. <summary> plot </summary>. ![residual_var](https://user-images.githubusercontent.com/8238804/65942056-d0608500-e46f-11e9-86da-58f89371c10f.png). </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:928,interoperability,format,format,928,"Improve accuracy and speed of _get_mean_var; This is a PR I've been meaning to get around to for a while. Previously discussed with @fidelram and @falexwolf. This calculates mean and variance for sparse matrices much faster than the current implementation. In addition, this accumulates to 64bit floats, which can greatly increase accuracy for large datasets and values. Some numbers using 50k cells from tabula muris using the call `sc.pp._utils._get_mean_var`. | | speed | memory (`%memit`) |. |--|-------|---------|. |current master| ~6.2 seconds | ~4.7 GB |. |this PR | 1.2 seconds | 0.5 MB |. Pretty good improvements. For accuracy, I'll compare against `sklearn.utils.sparsefuncs.mean_variance_axis`:. ```python. import numba. import numpy as np. from scipy import sparse. from sklearn.utils.sparsefuncs import mean_variance_axis. import matplotlib.pyplot as plt. import seaborn as sns. csr64 = sparse.random(10000, 1000, format=""csr"", dtype=np.float64). csr32 = csr64.astype(np.float32). m_my64, v_my64 = sparse_mean_variance_axis(csr64, 0). m_sk64, v_sk64 = mean_variance_axis(csr64, 0). m_my32, v_my32 = sparse_mean_variance_axis(csr32, 0). m_sk32, v_sk32 = mean_variance_axis(csr32, 0). sns.distplot(m_sk64 - m_sk32, color=""blue""). sns.distplot(m_my64 - m_my32, color=""red""). plt.show(). ```. <details>. <summary> plot </summary>. ![residual_mean](https://user-images.githubusercontent.com/8238804/65942102-ed955380-e46f-11e9-8293-334f1e7d6488.png). </details>. ```python. sns.distplot(m_sk64 - m_sk32, color=""blue""). sns.distplot(m_my64 - m_my32, color=""red""). plt.show(). ```. <details>. <summary> plot </summary>. ![residual_var](https://user-images.githubusercontent.com/8238804/65942056-d0608500-e46f-11e9-86da-58f89371c10f.png). </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:475,performance,memor,memory,475,"Improve accuracy and speed of _get_mean_var; This is a PR I've been meaning to get around to for a while. Previously discussed with @fidelram and @falexwolf. This calculates mean and variance for sparse matrices much faster than the current implementation. In addition, this accumulates to 64bit floats, which can greatly increase accuracy for large datasets and values. Some numbers using 50k cells from tabula muris using the call `sc.pp._utils._get_mean_var`. | | speed | memory (`%memit`) |. |--|-------|---------|. |current master| ~6.2 seconds | ~4.7 GB |. |this PR | 1.2 seconds | 0.5 MB |. Pretty good improvements. For accuracy, I'll compare against `sklearn.utils.sparsefuncs.mean_variance_axis`:. ```python. import numba. import numpy as np. from scipy import sparse. from sklearn.utils.sparsefuncs import mean_variance_axis. import matplotlib.pyplot as plt. import seaborn as sns. csr64 = sparse.random(10000, 1000, format=""csr"", dtype=np.float64). csr32 = csr64.astype(np.float32). m_my64, v_my64 = sparse_mean_variance_axis(csr64, 0). m_sk64, v_sk64 = mean_variance_axis(csr64, 0). m_my32, v_my32 = sparse_mean_variance_axis(csr32, 0). m_sk32, v_sk32 = mean_variance_axis(csr32, 0). sns.distplot(m_sk64 - m_sk32, color=""blue""). sns.distplot(m_my64 - m_my32, color=""red""). plt.show(). ```. <details>. <summary> plot </summary>. ![residual_mean](https://user-images.githubusercontent.com/8238804/65942102-ed955380-e46f-11e9-8293-334f1e7d6488.png). </details>. ```python. sns.distplot(m_sk64 - m_sk32, color=""blue""). sns.distplot(m_my64 - m_my32, color=""red""). plt.show(). ```. <details>. <summary> plot </summary>. ![residual_var](https://user-images.githubusercontent.com/8238804/65942056-d0608500-e46f-11e9-86da-58f89371c10f.png). </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:475,usability,memor,memory,475,"Improve accuracy and speed of _get_mean_var; This is a PR I've been meaning to get around to for a while. Previously discussed with @fidelram and @falexwolf. This calculates mean and variance for sparse matrices much faster than the current implementation. In addition, this accumulates to 64bit floats, which can greatly increase accuracy for large datasets and values. Some numbers using 50k cells from tabula muris using the call `sc.pp._utils._get_mean_var`. | | speed | memory (`%memit`) |. |--|-------|---------|. |current master| ~6.2 seconds | ~4.7 GB |. |this PR | 1.2 seconds | 0.5 MB |. Pretty good improvements. For accuracy, I'll compare against `sklearn.utils.sparsefuncs.mean_variance_axis`:. ```python. import numba. import numpy as np. from scipy import sparse. from sklearn.utils.sparsefuncs import mean_variance_axis. import matplotlib.pyplot as plt. import seaborn as sns. csr64 = sparse.random(10000, 1000, format=""csr"", dtype=np.float64). csr32 = csr64.astype(np.float32). m_my64, v_my64 = sparse_mean_variance_axis(csr64, 0). m_sk64, v_sk64 = mean_variance_axis(csr64, 0). m_my32, v_my32 = sparse_mean_variance_axis(csr32, 0). m_sk32, v_sk32 = mean_variance_axis(csr32, 0). sns.distplot(m_sk64 - m_sk32, color=""blue""). sns.distplot(m_my64 - m_my32, color=""red""). plt.show(). ```. <details>. <summary> plot </summary>. ![residual_mean](https://user-images.githubusercontent.com/8238804/65942102-ed955380-e46f-11e9-8293-334f1e7d6488.png). </details>. ```python. sns.distplot(m_sk64 - m_sk32, color=""blue""). sns.distplot(m_my64 - m_my32, color=""red""). plt.show(). ```. <details>. <summary> plot </summary>. ![residual_var](https://user-images.githubusercontent.com/8238804/65942056-d0608500-e46f-11e9-86da-58f89371c10f.png). </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:1366,usability,user,user-images,1366,"Improve accuracy and speed of _get_mean_var; This is a PR I've been meaning to get around to for a while. Previously discussed with @fidelram and @falexwolf. This calculates mean and variance for sparse matrices much faster than the current implementation. In addition, this accumulates to 64bit floats, which can greatly increase accuracy for large datasets and values. Some numbers using 50k cells from tabula muris using the call `sc.pp._utils._get_mean_var`. | | speed | memory (`%memit`) |. |--|-------|---------|. |current master| ~6.2 seconds | ~4.7 GB |. |this PR | 1.2 seconds | 0.5 MB |. Pretty good improvements. For accuracy, I'll compare against `sklearn.utils.sparsefuncs.mean_variance_axis`:. ```python. import numba. import numpy as np. from scipy import sparse. from sklearn.utils.sparsefuncs import mean_variance_axis. import matplotlib.pyplot as plt. import seaborn as sns. csr64 = sparse.random(10000, 1000, format=""csr"", dtype=np.float64). csr32 = csr64.astype(np.float32). m_my64, v_my64 = sparse_mean_variance_axis(csr64, 0). m_sk64, v_sk64 = mean_variance_axis(csr64, 0). m_my32, v_my32 = sparse_mean_variance_axis(csr32, 0). m_sk32, v_sk32 = mean_variance_axis(csr32, 0). sns.distplot(m_sk64 - m_sk32, color=""blue""). sns.distplot(m_my64 - m_my32, color=""red""). plt.show(). ```. <details>. <summary> plot </summary>. ![residual_mean](https://user-images.githubusercontent.com/8238804/65942102-ed955380-e46f-11e9-8293-334f1e7d6488.png). </details>. ```python. sns.distplot(m_sk64 - m_sk32, color=""blue""). sns.distplot(m_my64 - m_my32, color=""red""). plt.show(). ```. <details>. <summary> plot </summary>. ![residual_var](https://user-images.githubusercontent.com/8238804/65942056-d0608500-e46f-11e9-86da-58f89371c10f.png). </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/857:1651,usability,user,user-images,1651,"Improve accuracy and speed of _get_mean_var; This is a PR I've been meaning to get around to for a while. Previously discussed with @fidelram and @falexwolf. This calculates mean and variance for sparse matrices much faster than the current implementation. In addition, this accumulates to 64bit floats, which can greatly increase accuracy for large datasets and values. Some numbers using 50k cells from tabula muris using the call `sc.pp._utils._get_mean_var`. | | speed | memory (`%memit`) |. |--|-------|---------|. |current master| ~6.2 seconds | ~4.7 GB |. |this PR | 1.2 seconds | 0.5 MB |. Pretty good improvements. For accuracy, I'll compare against `sklearn.utils.sparsefuncs.mean_variance_axis`:. ```python. import numba. import numpy as np. from scipy import sparse. from sklearn.utils.sparsefuncs import mean_variance_axis. import matplotlib.pyplot as plt. import seaborn as sns. csr64 = sparse.random(10000, 1000, format=""csr"", dtype=np.float64). csr32 = csr64.astype(np.float32). m_my64, v_my64 = sparse_mean_variance_axis(csr64, 0). m_sk64, v_sk64 = mean_variance_axis(csr64, 0). m_my32, v_my32 = sparse_mean_variance_axis(csr32, 0). m_sk32, v_sk32 = mean_variance_axis(csr32, 0). sns.distplot(m_sk64 - m_sk32, color=""blue""). sns.distplot(m_my64 - m_my32, color=""red""). plt.show(). ```. <details>. <summary> plot </summary>. ![residual_mean](https://user-images.githubusercontent.com/8238804/65942102-ed955380-e46f-11e9-8293-334f1e7d6488.png). </details>. ```python. sns.distplot(m_sk64 - m_sk32, color=""blue""). sns.distplot(m_my64 - m_my32, color=""red""). plt.show(). ```. <details>. <summary> plot </summary>. ![residual_var](https://user-images.githubusercontent.com/8238804/65942056-d0608500-e46f-11e9-86da-58f89371c10f.png). </details>",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/857
https://github.com/scverse/scanpy/pull/858:0,deployability,updat,update,0,update ValueError message in pca; replaced the deprecated filter_gene_dispersion with highly_variable_genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/858
https://github.com/scverse/scanpy/pull/858:18,integrability,messag,message,18,update ValueError message in pca; replaced the deprecated filter_gene_dispersion with highly_variable_genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/858
https://github.com/scverse/scanpy/pull/858:18,interoperability,messag,message,18,update ValueError message in pca; replaced the deprecated filter_gene_dispersion with highly_variable_genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/858
https://github.com/scverse/scanpy/pull/858:0,safety,updat,update,0,update ValueError message in pca; replaced the deprecated filter_gene_dispersion with highly_variable_genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/858
https://github.com/scverse/scanpy/pull/858:0,security,updat,update,0,update ValueError message in pca; replaced the deprecated filter_gene_dispersion with highly_variable_genes,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/858
https://github.com/scverse/scanpy/issues/859:798,availability,cluster,cluster,798,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:904,availability,cluster,cluster,904,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1025,availability,cluster,cluster,1025,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1262,availability,cluster,cluster,1262,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:0,deployability,Integr,Integrate,0,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:705,deployability,integr,integrate,705,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:756,deployability,Integr,IntegrateData,756,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:798,deployability,cluster,cluster,798,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:822,deployability,integr,integrated,822,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:904,deployability,cluster,cluster,904,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:958,deployability,integr,integrate,958,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1025,deployability,cluster,cluster,1025,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1049,deployability,integr,integrated,1049,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1262,deployability,cluster,cluster,1262,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1308,deployability,integr,integrating,1308,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1432,deployability,integr,integrate,1432,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:0,integrability,Integr,Integrate,0,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:705,integrability,integr,integrate,705,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:756,integrability,Integr,IntegrateData,756,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:822,integrability,integr,integrated,822,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:958,integrability,integr,integrate,958,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1049,integrability,integr,integrated,1049,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1308,integrability,integr,integrating,1308,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1432,integrability,integr,integrate,1432,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:0,interoperability,Integr,Integrate,0,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:705,interoperability,integr,integrate,705,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:756,interoperability,Integr,IntegrateData,756,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:822,interoperability,integr,integrated,822,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:958,interoperability,integr,integrate,958,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1049,interoperability,integr,integrated,1049,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1308,interoperability,integr,integrating,1308,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1432,interoperability,integr,integrate,1432,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:0,modifiability,Integr,Integrate,0,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:705,modifiability,integr,integrate,705,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:756,modifiability,Integr,IntegrateData,756,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:822,modifiability,integr,integrated,822,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:958,modifiability,integr,integrate,958,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1049,modifiability,integr,integrated,1049,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1308,modifiability,integr,integrating,1308,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1432,modifiability,integr,integrate,1432,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:45,performance,perform,perform,45,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:285,performance,perform,perform,285,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:790,performance,perform,perform,790,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1017,performance,perform,perform,1017,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1234,performance,perform,perform,1234,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1454,performance,perform,perform,1454,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:0,reliability,Integr,Integrate,0,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:705,reliability,integr,integrate,705,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:756,reliability,Integr,IntegrateData,756,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:822,reliability,integr,integrated,822,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:958,reliability,integr,integrate,958,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1049,reliability,integr,integrated,1049,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1308,reliability,integr,integrating,1308,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1432,reliability,integr,integrate,1432,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:846,safety,test,test,846,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1072,safety,test,test,1072,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:0,security,Integr,Integrate,0,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:687,security,control,control,687,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:705,security,integr,integrate,705,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:756,security,Integr,IntegrateData,756,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:822,security,integr,integrated,822,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:958,security,integr,integrate,958,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1049,security,integr,integrated,1049,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1129,security,ident,identified,1129,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1250,security,control,control,1250,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1308,security,integr,integrating,1308,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1432,security,integr,integrate,1432,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:0,testability,Integr,Integrate,0,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:687,testability,control,control,687,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:705,testability,integr,integrate,705,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:756,testability,Integr,IntegrateData,756,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:822,testability,integr,integrated,822,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:846,testability,test,test,846,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:958,testability,integr,integrate,958,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1049,testability,integr,integrated,1049,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1072,testability,test,test,1072,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1250,testability,control,control,1250,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1308,testability,integr,integrating,1308,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1432,testability,integr,integrate,1432,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:45,usability,perform,perform,45,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:203,usability,tool,tool,203,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:285,usability,perform,perform,285,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:790,usability,perform,perform,790,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1017,usability,perform,perform,1017,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1234,usability,perform,perform,1234,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1380,usability,help,helpful,1380,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/issues/859:1454,usability,perform,perform,1454,"Integrate data from different treatments and perform differential gene expression analysis according to treatment and cell type; Hi, . First of all, I would like to thank the developers for this awesome tool! I am new to Scanpy. I am migrating from Seurat to Scanpy as I would like to perform trajectory analysis in my data. I have single cell sequencing data from 12 samples and 3 treatments (so 4 samples per treatment). I merged the samples from the same treatment in a single matrix using cellranger software from 10x Genomics (so I have 3 matrixes from 3 different treatments to import to Scanpy). . In Seurat, I can read the data from my three treatments separated, do quality control, and then integrate them using FindIntegrationAnchors and IntegrateData functions. Then, I perform cluster analysis in the integrated dataset, and test the effect of treatment on the transcriptome of each cluster. . Is there a similar function in Scanpy to integrate different datasets which are labeled in order to perform cluster analyses in the integrated dataset and test for the effect of treatment in the transcriptome of identified cell types? If so, is there a tutorial for that? In Scanpy I am able to import the data and perform quality control and cluster analysis. Thus, if there was a way of integrating the 3 different matrixes in one single object that would be helpful. Any suggestions on how I should proceed to integrate my data and perform differential gene expression analysis according to treatment and cell type? Thank you very much! Joao",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/859
https://github.com/scverse/scanpy/pull/861:0,deployability,updat,update,0,"update bbknn reference; BBKNN came out as an actual paper a couple of months ago, updating the reference to point to it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/861
https://github.com/scverse/scanpy/pull/861:82,deployability,updat,updating,82,"update bbknn reference; BBKNN came out as an actual paper a couple of months ago, updating the reference to point to it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/861
https://github.com/scverse/scanpy/pull/861:60,integrability,coupl,couple,60,"update bbknn reference; BBKNN came out as an actual paper a couple of months ago, updating the reference to point to it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/861
https://github.com/scverse/scanpy/pull/861:60,modifiability,coupl,couple,60,"update bbknn reference; BBKNN came out as an actual paper a couple of months ago, updating the reference to point to it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/861
https://github.com/scverse/scanpy/pull/861:0,safety,updat,update,0,"update bbknn reference; BBKNN came out as an actual paper a couple of months ago, updating the reference to point to it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/861
https://github.com/scverse/scanpy/pull/861:82,safety,updat,updating,82,"update bbknn reference; BBKNN came out as an actual paper a couple of months ago, updating the reference to point to it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/861
https://github.com/scverse/scanpy/pull/861:0,security,updat,update,0,"update bbknn reference; BBKNN came out as an actual paper a couple of months ago, updating the reference to point to it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/861
https://github.com/scverse/scanpy/pull/861:82,security,updat,updating,82,"update bbknn reference; BBKNN came out as an actual paper a couple of months ago, updating the reference to point to it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/861
https://github.com/scverse/scanpy/pull/861:60,testability,coupl,couple,60,"update bbknn reference; BBKNN came out as an actual paper a couple of months ago, updating the reference to point to it.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/861
https://github.com/scverse/scanpy/pull/862:651,availability,cluster,clusters,651,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:36,deployability,scale,scale,36,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:651,deployability,cluster,clusters,651,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:670,deployability,scale,scales,670,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:915,deployability,scale,scales,915,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:36,energy efficiency,scale,scale,36,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:57,energy efficiency,Reduc,Reduction,57,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:110,energy efficiency,reduc,reduction,110,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:545,energy efficiency,reduc,reduction,545,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:670,energy efficiency,scale,scales,670,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:915,energy efficiency,scale,scales,915,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:36,modifiability,scal,scale,36,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:670,modifiability,scal,scales,670,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:915,modifiability,scal,scales,915,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:36,performance,scale,scale,36,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:670,performance,scale,scales,670,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:915,performance,scale,scales,915,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:470,security,sign,significantly,470,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:859,security,sign,significantly,859,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/pull/862:265,usability,close,closer,265,"Add trimap to scanpy; TriMap: Large-scale Dimensionality Reduction Using Triplets. TriMap is a dimensionality reduction method that uses triplet constraints to form a low-dimensional embedding of a set of points. The triplet constraints are of the form ""point i is closer to point j than point k"". The triplets are sampled from the high-dimensional representation of the points and a weighting scheme is used to reflect the importance of each triplet. TriMap provides a significantly better global view of the data than the other dimensionality reduction methods such t-SNE, LargeVis, and UMAP. The global structure includes relative distances of the clusters, multiple scales in the data, and the existence of possible outliers. We define a global score to quantify the quality of an embedding in reflecting the global structure of the data. Also, TriMap is significantly faster than t-SNE, LargeVis, and UMAP and scales better to large datasets. paper: https://arxiv.org/abs/1910.00204. code: https://github.com/eamid/trimap.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/862
https://github.com/scverse/scanpy/issues/863:651,availability,cluster,cluster,651,"filter_rank_genes_groups fold change calculation; `filter_rank_genes_groups` handles log values differently than the `rank_genes_groups` function. The discrepancy gives different DE gene lists when filtering genes based on `log2fc_min=1` with `get.rank_genes_groups_df` vs `min_fold_change=2` with `tl.filter_rank_genes_groups`. In rank_genes_groups, `np.expm1` is used:. ```python. foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. ```python. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). ```. In filter_rank_genes_groups, `np.exp` is used:. ```python. if log:. fold_change_matrix.loc[:, cluster] = (np.exp(mean_obs.loc[True]) / np.exp(mean_obs.loc[False])).values. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:85,deployability,log,log,85,"filter_rank_genes_groups fold change calculation; `filter_rank_genes_groups` handles log values differently than the `rank_genes_groups` function. The discrepancy gives different DE gene lists when filtering genes based on `log2fc_min=1` with `get.rank_genes_groups_df` vs `min_fold_change=2` with `tl.filter_rank_genes_groups`. In rank_genes_groups, `np.expm1` is used:. ```python. foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. ```python. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). ```. In filter_rank_genes_groups, `np.exp` is used:. ```python. if log:. fold_change_matrix.loc[:, cluster] = (np.exp(mean_obs.loc[True]) / np.exp(mean_obs.loc[False])).values. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:619,deployability,log,log,619,"filter_rank_genes_groups fold change calculation; `filter_rank_genes_groups` handles log values differently than the `rank_genes_groups` function. The discrepancy gives different DE gene lists when filtering genes based on `log2fc_min=1` with `get.rank_genes_groups_df` vs `min_fold_change=2` with `tl.filter_rank_genes_groups`. In rank_genes_groups, `np.expm1` is used:. ```python. foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. ```python. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). ```. In filter_rank_genes_groups, `np.exp` is used:. ```python. if log:. fold_change_matrix.loc[:, cluster] = (np.exp(mean_obs.loc[True]) / np.exp(mean_obs.loc[False])).values. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:651,deployability,cluster,cluster,651,"filter_rank_genes_groups fold change calculation; `filter_rank_genes_groups` handles log values differently than the `rank_genes_groups` function. The discrepancy gives different DE gene lists when filtering genes based on `log2fc_min=1` with `get.rank_genes_groups_df` vs `min_fold_change=2` with `tl.filter_rank_genes_groups`. In rank_genes_groups, `np.expm1` is used:. ```python. foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. ```python. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). ```. In filter_rank_genes_groups, `np.exp` is used:. ```python. if log:. fold_change_matrix.loc[:, cluster] = (np.exp(mean_obs.loc[True]) / np.exp(mean_obs.loc[False])).values. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:198,integrability,filter,filtering,198,"filter_rank_genes_groups fold change calculation; `filter_rank_genes_groups` handles log values differently than the `rank_genes_groups` function. The discrepancy gives different DE gene lists when filtering genes based on `log2fc_min=1` with `get.rank_genes_groups_df` vs `min_fold_change=2` with `tl.filter_rank_genes_groups`. In rank_genes_groups, `np.expm1` is used:. ```python. foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. ```python. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). ```. In filter_rank_genes_groups, `np.exp` is used:. ```python. if log:. fold_change_matrix.loc[:, cluster] = (np.exp(mean_obs.loc[True]) / np.exp(mean_obs.loc[False])).values. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:85,safety,log,log,85,"filter_rank_genes_groups fold change calculation; `filter_rank_genes_groups` handles log values differently than the `rank_genes_groups` function. The discrepancy gives different DE gene lists when filtering genes based on `log2fc_min=1` with `get.rank_genes_groups_df` vs `min_fold_change=2` with `tl.filter_rank_genes_groups`. In rank_genes_groups, `np.expm1` is used:. ```python. foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. ```python. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). ```. In filter_rank_genes_groups, `np.exp` is used:. ```python. if log:. fold_change_matrix.loc[:, cluster] = (np.exp(mean_obs.loc[True]) / np.exp(mean_obs.loc[False])).values. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:619,safety,log,log,619,"filter_rank_genes_groups fold change calculation; `filter_rank_genes_groups` handles log values differently than the `rank_genes_groups` function. The discrepancy gives different DE gene lists when filtering genes based on `log2fc_min=1` with `get.rank_genes_groups_df` vs `min_fold_change=2` with `tl.filter_rank_genes_groups`. In rank_genes_groups, `np.expm1` is used:. ```python. foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. ```python. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). ```. In filter_rank_genes_groups, `np.exp` is used:. ```python. if log:. fold_change_matrix.loc[:, cluster] = (np.exp(mean_obs.loc[True]) / np.exp(mean_obs.loc[False])).values. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:85,security,log,log,85,"filter_rank_genes_groups fold change calculation; `filter_rank_genes_groups` handles log values differently than the `rank_genes_groups` function. The discrepancy gives different DE gene lists when filtering genes based on `log2fc_min=1` with `get.rank_genes_groups_df` vs `min_fold_change=2` with `tl.filter_rank_genes_groups`. In rank_genes_groups, `np.expm1` is used:. ```python. foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. ```python. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). ```. In filter_rank_genes_groups, `np.exp` is used:. ```python. if log:. fold_change_matrix.loc[:, cluster] = (np.exp(mean_obs.loc[True]) / np.exp(mean_obs.loc[False])).values. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:619,security,log,log,619,"filter_rank_genes_groups fold change calculation; `filter_rank_genes_groups` handles log values differently than the `rank_genes_groups` function. The discrepancy gives different DE gene lists when filtering genes based on `log2fc_min=1` with `get.rank_genes_groups_df` vs `min_fold_change=2` with `tl.filter_rank_genes_groups`. In rank_genes_groups, `np.expm1` is used:. ```python. foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. ```python. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). ```. In filter_rank_genes_groups, `np.exp` is used:. ```python. if log:. fold_change_matrix.loc[:, cluster] = (np.exp(mean_obs.loc[True]) / np.exp(mean_obs.loc[False])).values. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:85,testability,log,log,85,"filter_rank_genes_groups fold change calculation; `filter_rank_genes_groups` handles log values differently than the `rank_genes_groups` function. The discrepancy gives different DE gene lists when filtering genes based on `log2fc_min=1` with `get.rank_genes_groups_df` vs `min_fold_change=2` with `tl.filter_rank_genes_groups`. In rank_genes_groups, `np.expm1` is used:. ```python. foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. ```python. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). ```. In filter_rank_genes_groups, `np.exp` is used:. ```python. if log:. fold_change_matrix.loc[:, cluster] = (np.exp(mean_obs.loc[True]) / np.exp(mean_obs.loc[False])).values. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/863:619,testability,log,log,619,"filter_rank_genes_groups fold change calculation; `filter_rank_genes_groups` handles log values differently than the `rank_genes_groups` function. The discrepancy gives different DE gene lists when filtering genes based on `log2fc_min=1` with `get.rank_genes_groups_df` vs `min_fold_change=2` with `tl.filter_rank_genes_groups`. In rank_genes_groups, `np.expm1` is used:. ```python. foldchanges = (np.expm1(means[imask]) + 1e-9) / (np.expm1(mean_rest) + 1e-9). ```. ```python. rankings_gene_logfoldchanges.append(np.log2(foldchanges[global_indices])). ```. In filter_rank_genes_groups, `np.exp` is used:. ```python. if log:. fold_change_matrix.loc[:, cluster] = (np.exp(mean_obs.loc[True]) / np.exp(mean_obs.loc[False])).values. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/863
https://github.com/scverse/scanpy/issues/864:18,deployability,log,logfoldchange,18,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:120,deployability,log,logFC,120,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:425,deployability,log,log,425,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:598,deployability,log,log,598,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:694,deployability,log,logFC,694,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:18,safety,log,logfoldchange,18,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:120,safety,log,logFC,120,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:425,safety,log,log,425,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:598,safety,log,log,598,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:694,safety,log,logFC,694,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:18,security,log,logfoldchange,18,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:120,security,log,logFC,120,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:425,security,log,log,425,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:598,security,log,log,598,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:694,security,log,logFC,694,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:18,testability,log,logfoldchange,18,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:120,testability,log,logFC,120,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:425,testability,log,log,425,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:598,testability,log,log,598,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:694,testability,log,logFC,694,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:241,usability,tool,tools,241,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/issues/864:368,usability,tool,tools,368,"rank_genes_groups logfoldchange different than seurat; Hi! I've noticed that the function `rank_genes_groups` calculate logFC differently than seurat. . https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L207-L208. https://github.com/theislab/scanpy/blob/5800db45dde0c2060ad0a9bed8ea931bd41da936/scanpy/tools/_rank_genes_groups.py#L223. Thus the equation is. `log(exp(mean(values))`. while in Seurat is. https://github.com/satijalab/seurat/blob/96d07d80bc4b6513b93e9c10d8a9d57ae7016f9f/R/differential_expression.R#L175-L179 . thus. `log(mean(exp(values)))`. I was thus wondering if this was intended, since it leads to different logFC values.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/864
https://github.com/scverse/scanpy/pull/865:16,availability,down,downsample,16,"dtype fixes for downsample and normalization; Addressing https://github.com/theislab/scanpy/issues/435#issuecomment-538776417. This PR does two things:. 1. `downsample_counts` will convert the resulting downsampled matrix back to the initial dtype by default. 2. `normalize_total` will now work with integer matrices. I think 2 should definitely be the case. 1 does have a performance cost, but it's close to @falexwolf's [suggestion](https://github.com/theislab/scanpy/issues/435#issuecomment-475999342) and removes a minor foot-gun.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:203,availability,down,downsampled,203,"dtype fixes for downsample and normalization; Addressing https://github.com/theislab/scanpy/issues/435#issuecomment-538776417. This PR does two things:. 1. `downsample_counts` will convert the resulting downsampled matrix back to the initial dtype by default. 2. `normalize_total` will now work with integer matrices. I think 2 should definitely be the case. 1 does have a performance cost, but it's close to @falexwolf's [suggestion](https://github.com/theislab/scanpy/issues/435#issuecomment-475999342) and removes a minor foot-gun.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:373,performance,perform,performance,373,"dtype fixes for downsample and normalization; Addressing https://github.com/theislab/scanpy/issues/435#issuecomment-538776417. This PR does two things:. 1. `downsample_counts` will convert the resulting downsampled matrix back to the initial dtype by default. 2. `normalize_total` will now work with integer matrices. I think 2 should definitely be the case. 1 does have a performance cost, but it's close to @falexwolf's [suggestion](https://github.com/theislab/scanpy/issues/435#issuecomment-475999342) and removes a minor foot-gun.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:135,reliability,doe,does,135,"dtype fixes for downsample and normalization; Addressing https://github.com/theislab/scanpy/issues/435#issuecomment-538776417. This PR does two things:. 1. `downsample_counts` will convert the resulting downsampled matrix back to the initial dtype by default. 2. `normalize_total` will now work with integer matrices. I think 2 should definitely be the case. 1 does have a performance cost, but it's close to @falexwolf's [suggestion](https://github.com/theislab/scanpy/issues/435#issuecomment-475999342) and removes a minor foot-gun.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:361,reliability,doe,does,361,"dtype fixes for downsample and normalization; Addressing https://github.com/theislab/scanpy/issues/435#issuecomment-538776417. This PR does two things:. 1. `downsample_counts` will convert the resulting downsampled matrix back to the initial dtype by default. 2. `normalize_total` will now work with integer matrices. I think 2 should definitely be the case. 1 does have a performance cost, but it's close to @falexwolf's [suggestion](https://github.com/theislab/scanpy/issues/435#issuecomment-475999342) and removes a minor foot-gun.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:373,usability,perform,performance,373,"dtype fixes for downsample and normalization; Addressing https://github.com/theislab/scanpy/issues/435#issuecomment-538776417. This PR does two things:. 1. `downsample_counts` will convert the resulting downsampled matrix back to the initial dtype by default. 2. `normalize_total` will now work with integer matrices. I think 2 should definitely be the case. 1 does have a performance cost, but it's close to @falexwolf's [suggestion](https://github.com/theislab/scanpy/issues/435#issuecomment-475999342) and removes a minor foot-gun.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/pull/865:400,usability,close,close,400,"dtype fixes for downsample and normalization; Addressing https://github.com/theislab/scanpy/issues/435#issuecomment-538776417. This PR does two things:. 1. `downsample_counts` will convert the resulting downsampled matrix back to the initial dtype by default. 2. `normalize_total` will now work with integer matrices. I think 2 should definitely be the case. 1 does have a performance cost, but it's close to @falexwolf's [suggestion](https://github.com/theislab/scanpy/issues/435#issuecomment-475999342) and removes a minor foot-gun.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/865
https://github.com/scverse/scanpy/issues/866:361,deployability,automat,automatically,361,"n_genes and normal distribution; Hi, I recently found that the parameter of filtering, n_genes, is determined arbitrarily. Hence, could we use the normal distribution and the threshold value (5% and 95%) to do the job? The scanpy gives the violin plot for n_genes, but I really want a density plot (imagined as attached). Specifically, after the plot, it could automatically do the ""shapiro test"" to see if it fits the normal distribution and give the the threshold value of 5% and 95% respectively. And the users could decide whether they want to filter the data by these parameters through n_genes. ![imagine](https://user-images.githubusercontent.com/49429496/66399459-3752e080-ea12-11e9-8458-af332e620975.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:76,integrability,filter,filtering,76,"n_genes and normal distribution; Hi, I recently found that the parameter of filtering, n_genes, is determined arbitrarily. Hence, could we use the normal distribution and the threshold value (5% and 95%) to do the job? The scanpy gives the violin plot for n_genes, but I really want a density plot (imagined as attached). Specifically, after the plot, it could automatically do the ""shapiro test"" to see if it fits the normal distribution and give the the threshold value of 5% and 95% respectively. And the users could decide whether they want to filter the data by these parameters through n_genes. ![imagine](https://user-images.githubusercontent.com/49429496/66399459-3752e080-ea12-11e9-8458-af332e620975.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:548,integrability,filter,filter,548,"n_genes and normal distribution; Hi, I recently found that the parameter of filtering, n_genes, is determined arbitrarily. Hence, could we use the normal distribution and the threshold value (5% and 95%) to do the job? The scanpy gives the violin plot for n_genes, but I really want a density plot (imagined as attached). Specifically, after the plot, it could automatically do the ""shapiro test"" to see if it fits the normal distribution and give the the threshold value of 5% and 95% respectively. And the users could decide whether they want to filter the data by these parameters through n_genes. ![imagine](https://user-images.githubusercontent.com/49429496/66399459-3752e080-ea12-11e9-8458-af332e620975.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:19,interoperability,distribut,distribution,19,"n_genes and normal distribution; Hi, I recently found that the parameter of filtering, n_genes, is determined arbitrarily. Hence, could we use the normal distribution and the threshold value (5% and 95%) to do the job? The scanpy gives the violin plot for n_genes, but I really want a density plot (imagined as attached). Specifically, after the plot, it could automatically do the ""shapiro test"" to see if it fits the normal distribution and give the the threshold value of 5% and 95% respectively. And the users could decide whether they want to filter the data by these parameters through n_genes. ![imagine](https://user-images.githubusercontent.com/49429496/66399459-3752e080-ea12-11e9-8458-af332e620975.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:154,interoperability,distribut,distribution,154,"n_genes and normal distribution; Hi, I recently found that the parameter of filtering, n_genes, is determined arbitrarily. Hence, could we use the normal distribution and the threshold value (5% and 95%) to do the job? The scanpy gives the violin plot for n_genes, but I really want a density plot (imagined as attached). Specifically, after the plot, it could automatically do the ""shapiro test"" to see if it fits the normal distribution and give the the threshold value of 5% and 95% respectively. And the users could decide whether they want to filter the data by these parameters through n_genes. ![imagine](https://user-images.githubusercontent.com/49429496/66399459-3752e080-ea12-11e9-8458-af332e620975.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:322,interoperability,Specif,Specifically,322,"n_genes and normal distribution; Hi, I recently found that the parameter of filtering, n_genes, is determined arbitrarily. Hence, could we use the normal distribution and the threshold value (5% and 95%) to do the job? The scanpy gives the violin plot for n_genes, but I really want a density plot (imagined as attached). Specifically, after the plot, it could automatically do the ""shapiro test"" to see if it fits the normal distribution and give the the threshold value of 5% and 95% respectively. And the users could decide whether they want to filter the data by these parameters through n_genes. ![imagine](https://user-images.githubusercontent.com/49429496/66399459-3752e080-ea12-11e9-8458-af332e620975.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:426,interoperability,distribut,distribution,426,"n_genes and normal distribution; Hi, I recently found that the parameter of filtering, n_genes, is determined arbitrarily. Hence, could we use the normal distribution and the threshold value (5% and 95%) to do the job? The scanpy gives the violin plot for n_genes, but I really want a density plot (imagined as attached). Specifically, after the plot, it could automatically do the ""shapiro test"" to see if it fits the normal distribution and give the the threshold value of 5% and 95% respectively. And the users could decide whether they want to filter the data by these parameters through n_genes. ![imagine](https://user-images.githubusercontent.com/49429496/66399459-3752e080-ea12-11e9-8458-af332e620975.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:63,modifiability,paramet,parameter,63,"n_genes and normal distribution; Hi, I recently found that the parameter of filtering, n_genes, is determined arbitrarily. Hence, could we use the normal distribution and the threshold value (5% and 95%) to do the job? The scanpy gives the violin plot for n_genes, but I really want a density plot (imagined as attached). Specifically, after the plot, it could automatically do the ""shapiro test"" to see if it fits the normal distribution and give the the threshold value of 5% and 95% respectively. And the users could decide whether they want to filter the data by these parameters through n_genes. ![imagine](https://user-images.githubusercontent.com/49429496/66399459-3752e080-ea12-11e9-8458-af332e620975.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:573,modifiability,paramet,parameters,573,"n_genes and normal distribution; Hi, I recently found that the parameter of filtering, n_genes, is determined arbitrarily. Hence, could we use the normal distribution and the threshold value (5% and 95%) to do the job? The scanpy gives the violin plot for n_genes, but I really want a density plot (imagined as attached). Specifically, after the plot, it could automatically do the ""shapiro test"" to see if it fits the normal distribution and give the the threshold value of 5% and 95% respectively. And the users could decide whether they want to filter the data by these parameters through n_genes. ![imagine](https://user-images.githubusercontent.com/49429496/66399459-3752e080-ea12-11e9-8458-af332e620975.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:391,safety,test,test,391,"n_genes and normal distribution; Hi, I recently found that the parameter of filtering, n_genes, is determined arbitrarily. Hence, could we use the normal distribution and the threshold value (5% and 95%) to do the job? The scanpy gives the violin plot for n_genes, but I really want a density plot (imagined as attached). Specifically, after the plot, it could automatically do the ""shapiro test"" to see if it fits the normal distribution and give the the threshold value of 5% and 95% respectively. And the users could decide whether they want to filter the data by these parameters through n_genes. ![imagine](https://user-images.githubusercontent.com/49429496/66399459-3752e080-ea12-11e9-8458-af332e620975.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:361,testability,automat,automatically,361,"n_genes and normal distribution; Hi, I recently found that the parameter of filtering, n_genes, is determined arbitrarily. Hence, could we use the normal distribution and the threshold value (5% and 95%) to do the job? The scanpy gives the violin plot for n_genes, but I really want a density plot (imagined as attached). Specifically, after the plot, it could automatically do the ""shapiro test"" to see if it fits the normal distribution and give the the threshold value of 5% and 95% respectively. And the users could decide whether they want to filter the data by these parameters through n_genes. ![imagine](https://user-images.githubusercontent.com/49429496/66399459-3752e080-ea12-11e9-8458-af332e620975.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:391,testability,test,test,391,"n_genes and normal distribution; Hi, I recently found that the parameter of filtering, n_genes, is determined arbitrarily. Hence, could we use the normal distribution and the threshold value (5% and 95%) to do the job? The scanpy gives the violin plot for n_genes, but I really want a density plot (imagined as attached). Specifically, after the plot, it could automatically do the ""shapiro test"" to see if it fits the normal distribution and give the the threshold value of 5% and 95% respectively. And the users could decide whether they want to filter the data by these parameters through n_genes. ![imagine](https://user-images.githubusercontent.com/49429496/66399459-3752e080-ea12-11e9-8458-af332e620975.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:508,usability,user,users,508,"n_genes and normal distribution; Hi, I recently found that the parameter of filtering, n_genes, is determined arbitrarily. Hence, could we use the normal distribution and the threshold value (5% and 95%) to do the job? The scanpy gives the violin plot for n_genes, but I really want a density plot (imagined as attached). Specifically, after the plot, it could automatically do the ""shapiro test"" to see if it fits the normal distribution and give the the threshold value of 5% and 95% respectively. And the users could decide whether they want to filter the data by these parameters through n_genes. ![imagine](https://user-images.githubusercontent.com/49429496/66399459-3752e080-ea12-11e9-8458-af332e620975.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/866:620,usability,user,user-images,620,"n_genes and normal distribution; Hi, I recently found that the parameter of filtering, n_genes, is determined arbitrarily. Hence, could we use the normal distribution and the threshold value (5% and 95%) to do the job? The scanpy gives the violin plot for n_genes, but I really want a density plot (imagined as attached). Specifically, after the plot, it could automatically do the ""shapiro test"" to see if it fits the normal distribution and give the the threshold value of 5% and 95% respectively. And the users could decide whether they want to filter the data by these parameters through n_genes. ![imagine](https://user-images.githubusercontent.com/49429496/66399459-3752e080-ea12-11e9-8458-af332e620975.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/866
https://github.com/scverse/scanpy/issues/868:250,integrability,sub,subsequent,250,"Introduce GLM-PCA to scanpy.external; I agree with @adrianveres [here](https://twitter.com/adrian_veres_/status/1105969701094674432?s=11): GLM-PCA by @willtownes seems like a rigorous way to deal with zero inflation, since we use PCA as base for all subsequent processing. Its apparently much faster than e.g. ZINB-WaVE. - [twitter thread](https://twitter.com/sandakano/status/1105458288441864193) explaining the problem and solution. - [code](https://github.com/willtownes/scrna2019/)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:105,usability,statu,status,105,"Introduce GLM-PCA to scanpy.external; I agree with @adrianveres [here](https://twitter.com/adrian_veres_/status/1105969701094674432?s=11): GLM-PCA by @willtownes seems like a rigorous way to deal with zero inflation, since we use PCA as base for all subsequent processing. Its apparently much faster than e.g. ZINB-WaVE. - [twitter thread](https://twitter.com/sandakano/status/1105458288441864193) explaining the problem and solution. - [code](https://github.com/willtownes/scrna2019/)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/issues/868:371,usability,statu,status,371,"Introduce GLM-PCA to scanpy.external; I agree with @adrianveres [here](https://twitter.com/adrian_veres_/status/1105969701094674432?s=11): GLM-PCA by @willtownes seems like a rigorous way to deal with zero inflation, since we use PCA as base for all subsequent processing. Its apparently much faster than e.g. ZINB-WaVE. - [twitter thread](https://twitter.com/sandakano/status/1105458288441864193) explaining the problem and solution. - [code](https://github.com/willtownes/scrna2019/)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/868
https://github.com/scverse/scanpy/pull/869:81,availability,error,error,81,Fix pl paga when adata.uns color list was not set; `sc.pl.paga` was throwing and error when the color list in `.uns` was not previously set. While fixing the error I realized that some functionality was duplicated between legacy scatter plots and the embedding plots and removed the code duplication.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:158,availability,error,error,158,Fix pl paga when adata.uns color list was not set; `sc.pl.paga` was throwing and error when the color list in `.uns` was not previously set. While fixing the error I realized that some functionality was duplicated between legacy scatter plots and the embedding plots and removed the code duplication.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:81,performance,error,error,81,Fix pl paga when adata.uns color list was not set; `sc.pl.paga` was throwing and error when the color list in `.uns` was not previously set. While fixing the error I realized that some functionality was duplicated between legacy scatter plots and the embedding plots and removed the code duplication.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:158,performance,error,error,158,Fix pl paga when adata.uns color list was not set; `sc.pl.paga` was throwing and error when the color list in `.uns` was not previously set. While fixing the error I realized that some functionality was duplicated between legacy scatter plots and the embedding plots and removed the code duplication.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:81,safety,error,error,81,Fix pl paga when adata.uns color list was not set; `sc.pl.paga` was throwing and error when the color list in `.uns` was not previously set. While fixing the error I realized that some functionality was duplicated between legacy scatter plots and the embedding plots and removed the code duplication.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:158,safety,error,error,158,Fix pl paga when adata.uns color list was not set; `sc.pl.paga` was throwing and error when the color list in `.uns` was not previously set. While fixing the error I realized that some functionality was duplicated between legacy scatter plots and the embedding plots and removed the code duplication.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:81,usability,error,error,81,Fix pl paga when adata.uns color list was not set; `sc.pl.paga` was throwing and error when the color list in `.uns` was not previously set. While fixing the error I realized that some functionality was duplicated between legacy scatter plots and the embedding plots and removed the code duplication.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/pull/869:158,usability,error,error,158,Fix pl paga when adata.uns color list was not set; `sc.pl.paga` was throwing and error when the color list in `.uns` was not previously set. While fixing the error I realized that some functionality was duplicated between legacy scatter plots and the embedding plots and removed the code duplication.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/869
https://github.com/scverse/scanpy/issues/870:466,availability,slo,slot,466,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:87,deployability,log,log,87,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:92,deployability,scale,scaled,92,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:92,energy efficiency,scale,scaled,92,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:110,integrability,coupl,couple,110,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:92,modifiability,scal,scaled,92,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:110,modifiability,coupl,couple,110,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:92,performance,scale,scaled,92,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:167,reliability,doe,doesnt,167,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:466,reliability,slo,slot,466,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:87,safety,log,log,87,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:87,security,log,log,87,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:87,testability,log,log,87,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:110,testability,coupl,couple,110,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:202,testability,simpl,simple,202,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/870:202,usability,simpl,simple,202,"getassaydata from scanpy object; Hi guys,. I am trying to get the gene expression raw, log, scaled for just a couple of genes from the anndata object but i have tried doesnt seem to work. I just want a simple dataframe with the gene names as colum indexes and row info of the cells. this is waht i used to do in seurat. I am sure there is an easy way to do this in scanpy but just havent figure out. . geneX_df=as.data.frame(as.matrix(GetAssayData(object = anndata, slot = ""data"")[""geneX"",])). Thanks!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/870
https://github.com/scverse/scanpy/issues/871:135,safety,avoid,avoid,135,"sc.pp.normalize_total() got an unexpected keyword argument 'exclude_highly_expressed'; ```. # normalize, by excluding frequent gene to avoid distorting data. sc.pp.normalize_total(d, exclude_highly_expressed=True, max_fraction=0.05, inplace=True). ```. I was trying to normalize_total and exclude highly expressed genes. But:. I got. TypeError: normalize_total() got an unexpected keyword argument 'exclude_highly_expressed'",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/871
https://github.com/scverse/scanpy/issues/872:190,interoperability,share,share,190,"Jackstraw implementation in scanpy?; Hi,. In the scanpy, has anyone tried implementing jackstraw using anndata? If anyone has written a code to find the significant PCs in scanpy, please do share or any guide to perform it would be greatly appreciated! Thanks so much",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:212,performance,perform,perform,212,"Jackstraw implementation in scanpy?; Hi,. In the scanpy, has anyone tried implementing jackstraw using anndata? If anyone has written a code to find the significant PCs in scanpy, please do share or any guide to perform it would be greatly appreciated! Thanks so much",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:153,security,sign,significant,153,"Jackstraw implementation in scanpy?; Hi,. In the scanpy, has anyone tried implementing jackstraw using anndata? If anyone has written a code to find the significant PCs in scanpy, please do share or any guide to perform it would be greatly appreciated! Thanks so much",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:203,usability,guid,guide,203,"Jackstraw implementation in scanpy?; Hi,. In the scanpy, has anyone tried implementing jackstraw using anndata? If anyone has written a code to find the significant PCs in scanpy, please do share or any guide to perform it would be greatly appreciated! Thanks so much",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/872:212,usability,perform,perform,212,"Jackstraw implementation in scanpy?; Hi,. In the scanpy, has anyone tried implementing jackstraw using anndata? If anyone has written a code to find the significant PCs in scanpy, please do share or any guide to perform it would be greatly appreciated! Thanks so much",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/872
https://github.com/scverse/scanpy/issues/873:22,deployability,Integr,Integrate,22,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:7,integrability,batch,batch,7,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:22,integrability,Integr,Integrate,22,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:219,integrability,batch,batch,219,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:22,interoperability,Integr,Integrate,22,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:476,interoperability,share,shared,476,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:22,modifiability,Integr,Integrate,22,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:318,modifiability,paramet,parameters,318,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:7,performance,batch,batch,7,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:219,performance,batch,batch,219,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:22,reliability,Integr,Integrate,22,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:22,security,Integr,Integrate,22,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:22,testability,Integr,Integrate,22,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:63,usability,user,user-images,63,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:288,usability,command,command,288,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:303,usability,custom,customized,303,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/873:524,usability,help,help,524,"Remove batch effect (""Integrate"" in Seurat""); ![image](https://user-images.githubusercontent.com/49429496/66832622-df295a80-ef8c-11e9-8a5c-98a02069f8ae.png). Recently, I tried `combat`, `bbknn`, and `mnn` to remove the batch effect. However, no visible impact was found after these three command even I customized the parameters. But when using the Seurat, the sample 001, 002and 009 were grouped together (about 70% of those 3 samples were located together in UMAP) as them shared the same biological condition. Could you help me?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/873
https://github.com/scverse/scanpy/issues/874:319,availability,fault,fault,319,"running scanpy.tl.tsne crashes my kernel; This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get . `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:. ```. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.tsne(adata). ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:. `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:381,availability,error,error,381,"running scanpy.tl.tsne crashes my kernel; This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get . `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:. ```. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.tsne(adata). ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:. `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:811,deployability,version,version,811,"running scanpy.tl.tsne crashes my kernel; This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get . `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:. ```. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.tsne(adata). ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:. `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:319,energy efficiency,fault,fault,319,"running scanpy.tl.tsne crashes my kernel; This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get . `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:. ```. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.tsne(adata). ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:. `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:811,integrability,version,version,811,"running scanpy.tl.tsne crashes my kernel; This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get . `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:. ```. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.tsne(adata). ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:. `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:811,modifiability,version,version,811,"running scanpy.tl.tsne crashes my kernel; This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get . `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:. ```. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.tsne(adata). ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:. `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:319,performance,fault,fault,319,"running scanpy.tl.tsne crashes my kernel; This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get . `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:. ```. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.tsne(adata). ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:. `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:381,performance,error,error,381,"running scanpy.tl.tsne crashes my kernel; This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get . `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:. ```. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.tsne(adata). ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:. `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:319,reliability,fault,fault,319,"running scanpy.tl.tsne crashes my kernel; This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get . `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:. ```. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.tsne(adata). ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:. `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:319,safety,fault,fault,319,"running scanpy.tl.tsne crashes my kernel; This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get . `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:. ```. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.tsne(adata). ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:. `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:381,safety,error,error,381,"running scanpy.tl.tsne crashes my kernel; This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get . `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:. ```. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.tsne(adata). ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:. `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:381,usability,error,error,381,"running scanpy.tl.tsne crashes my kernel; This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get . `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:. ```. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.tsne(adata). ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:. `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:581,usability,user,user-images,581,"running scanpy.tl.tsne crashes my kernel; This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get . `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:. ```. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.tsne(adata). ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:. `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/874:925,usability,learn,learn,925,"running scanpy.tl.tsne crashes my kernel; This issue just came up recently when I tried re-running an older notebook of mine. Whenever I try and run my code in Jupyter notebook and execute any cell that uses scanpy.tl.tsne my kernel crashes. When I try and execute the same code from the terminal I get . `Segmentation fault: 11`. . This is the code I used to reproducibly get the error:. ```. import scanpy as sc. adata = sc.datasets.pbmc3k(). sc.pp.pca(adata). sc.pp.neighbors(adata). sc.tl.tsne(adata). ```. <img width=""629"" alt=""Screenshot 2019-10-16 at 14 39 09"" src=""https://user-images.githubusercontent.com/15019107/66920007-c2d7fd00-f022-11e9-93d6-560305d1cc50.png"">. Any ideas what the issue could be and how I can fix this? The function used to work fine for me. I am running scanpy in the following version:. `scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.9 numpy==1.16.4 scipy==1.3.0 pandas==0.24.2 scikit-learn==0.21.2 statsmodels==0.10.0 python-igraph==0.7.1 louvain==0.6.1`.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/874
https://github.com/scverse/scanpy/issues/875:472,availability,error,error,472,"Unable to change vmax in scanpy.pl.scatter; Hi Scanpy team,. A potentially silly question. I'm having problems changing vmax in sc.pl.scatter. I want to set 'dpt_pseudotime' vmax to 0.5, similar to how vmax and vmin can be changed, for example, in sc.pl.umap. I tried the following:. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', color='dpt_pseudotime', size=5, use_raw=False, . color_map='RdYlBu_r', vmax=0.5). But get the following error:. ![image](https://user-images.githubusercontent.com/49629901/66955933-64fee180-f031-11e9-90c2-0f12a392786b.png). I read in the scanpy.pl.scatter documentation that kwargs can be used to set vmax. I tried this approach, but got the same result as above:. kwargs={'vmax':0.5}. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', . color='dpt_pseudotime', size=5, use_raw=False, color_map='RdYlBu_r', **kwargs). I tried to find a solution online with no success. Any help/suggestions will be appreciated. P.S: This is my first post and I'm a huge fan of your package. Keep up the great work!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:1067,modifiability,pac,package,1067,"Unable to change vmax in scanpy.pl.scatter; Hi Scanpy team,. A potentially silly question. I'm having problems changing vmax in sc.pl.scatter. I want to set 'dpt_pseudotime' vmax to 0.5, similar to how vmax and vmin can be changed, for example, in sc.pl.umap. I tried the following:. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', color='dpt_pseudotime', size=5, use_raw=False, . color_map='RdYlBu_r', vmax=0.5). But get the following error:. ![image](https://user-images.githubusercontent.com/49629901/66955933-64fee180-f031-11e9-90c2-0f12a392786b.png). I read in the scanpy.pl.scatter documentation that kwargs can be used to set vmax. I tried this approach, but got the same result as above:. kwargs={'vmax':0.5}. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', . color='dpt_pseudotime', size=5, use_raw=False, color_map='RdYlBu_r', **kwargs). I tried to find a solution online with no success. Any help/suggestions will be appreciated. P.S: This is my first post and I'm a huge fan of your package. Keep up the great work!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:472,performance,error,error,472,"Unable to change vmax in scanpy.pl.scatter; Hi Scanpy team,. A potentially silly question. I'm having problems changing vmax in sc.pl.scatter. I want to set 'dpt_pseudotime' vmax to 0.5, similar to how vmax and vmin can be changed, for example, in sc.pl.umap. I tried the following:. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', color='dpt_pseudotime', size=5, use_raw=False, . color_map='RdYlBu_r', vmax=0.5). But get the following error:. ![image](https://user-images.githubusercontent.com/49629901/66955933-64fee180-f031-11e9-90c2-0f12a392786b.png). I read in the scanpy.pl.scatter documentation that kwargs can be used to set vmax. I tried this approach, but got the same result as above:. kwargs={'vmax':0.5}. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', . color='dpt_pseudotime', size=5, use_raw=False, color_map='RdYlBu_r', **kwargs). I tried to find a solution online with no success. Any help/suggestions will be appreciated. P.S: This is my first post and I'm a huge fan of your package. Keep up the great work!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:472,safety,error,error,472,"Unable to change vmax in scanpy.pl.scatter; Hi Scanpy team,. A potentially silly question. I'm having problems changing vmax in sc.pl.scatter. I want to set 'dpt_pseudotime' vmax to 0.5, similar to how vmax and vmin can be changed, for example, in sc.pl.umap. I tried the following:. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', color='dpt_pseudotime', size=5, use_raw=False, . color_map='RdYlBu_r', vmax=0.5). But get the following error:. ![image](https://user-images.githubusercontent.com/49629901/66955933-64fee180-f031-11e9-90c2-0f12a392786b.png). I read in the scanpy.pl.scatter documentation that kwargs can be used to set vmax. I tried this approach, but got the same result as above:. kwargs={'vmax':0.5}. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', . color='dpt_pseudotime', size=5, use_raw=False, color_map='RdYlBu_r', **kwargs). I tried to find a solution online with no success. Any help/suggestions will be appreciated. P.S: This is my first post and I'm a huge fan of your package. Keep up the great work!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:54,security,team,team,54,"Unable to change vmax in scanpy.pl.scatter; Hi Scanpy team,. A potentially silly question. I'm having problems changing vmax in sc.pl.scatter. I want to set 'dpt_pseudotime' vmax to 0.5, similar to how vmax and vmin can be changed, for example, in sc.pl.umap. I tried the following:. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', color='dpt_pseudotime', size=5, use_raw=False, . color_map='RdYlBu_r', vmax=0.5). But get the following error:. ![image](https://user-images.githubusercontent.com/49629901/66955933-64fee180-f031-11e9-90c2-0f12a392786b.png). I read in the scanpy.pl.scatter documentation that kwargs can be used to set vmax. I tried this approach, but got the same result as above:. kwargs={'vmax':0.5}. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', . color='dpt_pseudotime', size=5, use_raw=False, color_map='RdYlBu_r', **kwargs). I tried to find a solution online with no success. Any help/suggestions will be appreciated. P.S: This is my first post and I'm a huge fan of your package. Keep up the great work!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:472,usability,error,error,472,"Unable to change vmax in scanpy.pl.scatter; Hi Scanpy team,. A potentially silly question. I'm having problems changing vmax in sc.pl.scatter. I want to set 'dpt_pseudotime' vmax to 0.5, similar to how vmax and vmin can be changed, for example, in sc.pl.umap. I tried the following:. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', color='dpt_pseudotime', size=5, use_raw=False, . color_map='RdYlBu_r', vmax=0.5). But get the following error:. ![image](https://user-images.githubusercontent.com/49629901/66955933-64fee180-f031-11e9-90c2-0f12a392786b.png). I read in the scanpy.pl.scatter documentation that kwargs can be used to set vmax. I tried this approach, but got the same result as above:. kwargs={'vmax':0.5}. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', . color='dpt_pseudotime', size=5, use_raw=False, color_map='RdYlBu_r', **kwargs). I tried to find a solution online with no success. Any help/suggestions will be appreciated. P.S: This is my first post and I'm a huge fan of your package. Keep up the great work!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:497,usability,user,user-images,497,"Unable to change vmax in scanpy.pl.scatter; Hi Scanpy team,. A potentially silly question. I'm having problems changing vmax in sc.pl.scatter. I want to set 'dpt_pseudotime' vmax to 0.5, similar to how vmax and vmin can be changed, for example, in sc.pl.umap. I tried the following:. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', color='dpt_pseudotime', size=5, use_raw=False, . color_map='RdYlBu_r', vmax=0.5). But get the following error:. ![image](https://user-images.githubusercontent.com/49629901/66955933-64fee180-f031-11e9-90c2-0f12a392786b.png). I read in the scanpy.pl.scatter documentation that kwargs can be used to set vmax. I tried this approach, but got the same result as above:. kwargs={'vmax':0.5}. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', . color='dpt_pseudotime', size=5, use_raw=False, color_map='RdYlBu_r', **kwargs). I tried to find a solution online with no success. Any help/suggestions will be appreciated. P.S: This is my first post and I'm a huge fan of your package. Keep up the great work!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:624,usability,document,documentation,624,"Unable to change vmax in scanpy.pl.scatter; Hi Scanpy team,. A potentially silly question. I'm having problems changing vmax in sc.pl.scatter. I want to set 'dpt_pseudotime' vmax to 0.5, similar to how vmax and vmin can be changed, for example, in sc.pl.umap. I tried the following:. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', color='dpt_pseudotime', size=5, use_raw=False, . color_map='RdYlBu_r', vmax=0.5). But get the following error:. ![image](https://user-images.githubusercontent.com/49629901/66955933-64fee180-f031-11e9-90c2-0f12a392786b.png). I read in the scanpy.pl.scatter documentation that kwargs can be used to set vmax. I tried this approach, but got the same result as above:. kwargs={'vmax':0.5}. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', . color='dpt_pseudotime', size=5, use_raw=False, color_map='RdYlBu_r', **kwargs). I tried to find a solution online with no success. Any help/suggestions will be appreciated. P.S: This is my first post and I'm a huge fan of your package. Keep up the great work!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/875:975,usability,help,help,975,"Unable to change vmax in scanpy.pl.scatter; Hi Scanpy team,. A potentially silly question. I'm having problems changing vmax in sc.pl.scatter. I want to set 'dpt_pseudotime' vmax to 0.5, similar to how vmax and vmin can be changed, for example, in sc.pl.umap. I tried the following:. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', color='dpt_pseudotime', size=5, use_raw=False, . color_map='RdYlBu_r', vmax=0.5). But get the following error:. ![image](https://user-images.githubusercontent.com/49629901/66955933-64fee180-f031-11e9-90c2-0f12a392786b.png). I read in the scanpy.pl.scatter documentation that kwargs can be used to set vmax. I tried this approach, but got the same result as above:. kwargs={'vmax':0.5}. with axes_style({'axes.grid': False}):. sc.pl.scatter(adata_0_1, x='Vim', y='Lyz2', . color='dpt_pseudotime', size=5, use_raw=False, color_map='RdYlBu_r', **kwargs). I tried to find a solution online with no success. Any help/suggestions will be appreciated. P.S: This is my first post and I'm a huge fan of your package. Keep up the great work!",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/875
https://github.com/scverse/scanpy/issues/876:159,availability,state,state,159,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:6,deployability,depend,dependencies,6,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:82,deployability,instal,installed,82,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:112,deployability,instal,install,112,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:271,deployability,depend,dependencies,271,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:322,deployability,version,versions,322,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:359,deployability,depend,dependency,359,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:6,integrability,depend,dependencies,6,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:159,integrability,state,state,159,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:271,integrability,depend,dependencies,271,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:322,integrability,version,versions,322,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:359,integrability,depend,dependency,359,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:6,modifiability,depend,dependencies,6,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:271,modifiability,depend,dependencies,271,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:322,modifiability,version,versions,322,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:359,modifiability,depend,dependency,359,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:6,safety,depend,dependencies,6,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:271,safety,depend,dependencies,271,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:359,safety,depend,dependency,359,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:6,testability,depend,dependencies,6,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:271,testability,depend,dependencies,271,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/issues/876:359,testability,depend,dependency,359,Wrong dependencies on bioconda for 1.4.4; I just noticed that matplotlib 3.1.1 is installed when using conda to install scanpy 1.4.4. In your requirements you state that this breaks the scatter plot. In the repodata from bioconda however it seems like there are very old dependencies that were not changed when new scanpy versions were included. So still the dependency there ist matplotlib>=2.2.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/876
https://github.com/scverse/scanpy/pull/877:12,interoperability,format,formatting,12,Low hanging formatting fruit;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/877
https://github.com/scverse/scanpy/pull/879:0,deployability,Updat,Update,0,Update pp docs;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/879
https://github.com/scverse/scanpy/pull/879:0,safety,Updat,Update,0,Update pp docs;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/879
https://github.com/scverse/scanpy/pull/879:0,security,Updat,Update,0,Update pp docs;,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/879
https://github.com/scverse/scanpy/issues/882:127,interoperability,specif,specify,127,"read_10x_mtx with different prefix; Hi scanpy team,. I am not sure if I just missed it, but there does not seem to be a way to specify a different filename for ``.mtx`` files. For instance, assuming I have multiple .mtx files in a folder ``sample1.matrix.mtx, sample2.matrix.mtx ... `` with corresponding `` sample1.genes.tsv `` and ``sample1.barcodes.tsv``. It would be useful to be able either specify matrix/genes/barcodes filename, etc. and/or a suffix for the files. . Thanks in advance, Alex .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:396,interoperability,specif,specify,396,"read_10x_mtx with different prefix; Hi scanpy team,. I am not sure if I just missed it, but there does not seem to be a way to specify a different filename for ``.mtx`` files. For instance, assuming I have multiple .mtx files in a folder ``sample1.matrix.mtx, sample2.matrix.mtx ... `` with corresponding `` sample1.genes.tsv `` and ``sample1.barcodes.tsv``. It would be useful to be able either specify matrix/genes/barcodes filename, etc. and/or a suffix for the files. . Thanks in advance, Alex .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:98,reliability,doe,does,98,"read_10x_mtx with different prefix; Hi scanpy team,. I am not sure if I just missed it, but there does not seem to be a way to specify a different filename for ``.mtx`` files. For instance, assuming I have multiple .mtx files in a folder ``sample1.matrix.mtx, sample2.matrix.mtx ... `` with corresponding `` sample1.genes.tsv `` and ``sample1.barcodes.tsv``. It would be useful to be able either specify matrix/genes/barcodes filename, etc. and/or a suffix for the files. . Thanks in advance, Alex .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/882:46,security,team,team,46,"read_10x_mtx with different prefix; Hi scanpy team,. I am not sure if I just missed it, but there does not seem to be a way to specify a different filename for ``.mtx`` files. For instance, assuming I have multiple .mtx files in a folder ``sample1.matrix.mtx, sample2.matrix.mtx ... `` with corresponding `` sample1.genes.tsv `` and ``sample1.barcodes.tsv``. It would be useful to be able either specify matrix/genes/barcodes filename, etc. and/or a suffix for the files. . Thanks in advance, Alex .",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/882
https://github.com/scverse/scanpy/issues/883:621,availability,error,error,621,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:1525,availability,mask,mask,1525,"ed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: setting an array element with a sequence. ```. thanks. Mark.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:788,deployability,modul,module,788,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:886,deployability,version,versions,886,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:1393,deployability,version,versions,1393,"ed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: setting an array element with a sequence. ```. thanks. Mark.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:1742,deployability,version,versions,1742,"ed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: setting an array element with a sequence. ```. thanks. Mark.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:41,energy efficiency,load,loaded,41,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:135,energy efficiency,load,load,135,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:1956,energy efficiency,current,currently,1956,"ed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: setting an array element with a sequence. ```. thanks. Mark.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:886,integrability,version,versions,886,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:1393,integrability,version,versions,1393,"ed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: setting an array element with a sequence. ```. thanks. Mark.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:1742,integrability,version,versions,1742,"ed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: setting an array element with a sequence. ```. thanks. Mark.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:788,modifiability,modul,module,788,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:886,modifiability,version,versions,886,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:922,modifiability,pac,packages,922,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:1393,modifiability,version,versions,1393,"ed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: setting an array element with a sequence. ```. thanks. Mark.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:1429,modifiability,pac,packages,1429,"ed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: setting an array element with a sequence. ```. thanks. Mark.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:1742,modifiability,version,versions,1742,"ed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: setting an array element with a sequence. ```. thanks. Mark.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:1778,modifiability,pac,packages,1778,"ed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: setting an array element with a sequence. ```. thanks. Mark.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:1974,modifiability,scal,scalar,1974,"ed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: setting an array element with a sequence. ```. thanks. Mark.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:41,performance,load,loaded,41,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:135,performance,load,load,135,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:142,performance,disk,disk,142,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:621,performance,error,error,621,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:14,reliability,doe,does,14,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:621,safety,error,error,621,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:760,safety,input,input-,760,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:788,safety,modul,module,788,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:716,testability,Trace,Traceback,716,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:621,usability,error,error,621,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:760,usability,input,input-,760,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/883:938,usability,tool,tools,938,"`score_genes` does not work on a dataset loaded with `backed=""r+""`; Hi - excellent software, thanks! - . but I do have a problem. If i load a disk backed dataset, I cannot run `sc.tl.score_genes`. Given these two sets:. ```py. ad = sc.read_h5ad('scdataset.h5ad', backed='r+'). ad2 = sc.read_h5ad('scdataset.h5ad'). ```. and. ```py. random_genes = list(ad.var_names.to_series().sample(100)). ```. this works perfectly:. ```py. sc.tl.score_genes(ad2, random, score_name=""random100"", random_state=42). ```. but, this:. ```py. sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ```. yields the following error:. ```pytb. -----------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-113-9cb28e089b25> in <module>. ----> 1 sc.tl.score_genes(ad, random, score_name=""random100"", random_state=42). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 90 else:. 91 obs_avg = pd.Series(. ---> 92 np.nanmean(_adata[:, gene_pool].X, axis=0), index=gene_pool) # average expression of genes. 93 . 94 obs_avg = obs_avg[np.isfinite(obs_avg)] # Sometimes (and I don't know how) missing data may be there, with nansfor. <__array_function__ internals> in nanmean(*args, **kwargs). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in nanmean(a, axis, dtype, out, keepdims). 949 cnt = np.sum(~mask, axis=axis, dtype=np.intp, keepdims=keepdims). 950 tot = np.sum(arr, axis=axis, dtype=dtype, out=out, keepdims=keepdims). --> 951 avg = _divide_by_count(tot, cnt, out=out). 952 . 953 isbad = (cnt == 0). ~/.pyenv/versions/mfpy372/lib/python3.7/site-packages/numpy/lib/nanfunctions.py in _divide_by_count(a, b, out). 216 else:. 217 if out is None:. --> 218 return a.dtype.type(a / b). 219 else:. 220 # This is questionable, but currently a numpy scalar can. ValueError: se",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/883
https://github.com/scverse/scanpy/issues/884:548,availability,error,error,548,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1607,availability,error,error,1607," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:631,deployability,modul,module,631,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1566,deployability,observ,observations,1566," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1697,deployability,version,versions,1697," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1723,deployability,instal,installed,1723," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:723,energy efficiency,core,core,723,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:887,energy efficiency,core,core,887,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1076,energy efficiency,core,core,1076,"ntly when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1237,energy efficiency,core,core,1237," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1401,energy efficiency,core,core,1401," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:0,integrability,Sub,Subsetting,0,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:98,integrability,sub,subset,98,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:299,integrability,sub,subsetting,299,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1551,integrability,sub,subsetting,1551," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1697,integrability,version,versions,1697," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:2213,integrability,repositor,repository,2213," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:2213,interoperability,repositor,repository,2213," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:631,modifiability,modul,module,631,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:706,modifiability,pac,packages,706,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:870,modifiability,pac,packages,870,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1059,modifiability,pac,packages,1059,"ome issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent p",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1220,modifiability,pac,packages,1220,"ilename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository ",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1384,modifiability,pac,packages,1384," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1582,modifiability,variab,variables,1582," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1697,modifiability,version,versions,1697," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:142,performance,disk,disk,142,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:199,performance,disk,disk,199,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:289,performance,disk,disk,289,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:356,performance,disk,disk,356,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:548,performance,error,error,548,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1607,performance,error,error,1607," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:310,reliability,doe,doesn,310,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:548,safety,error,error,548,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:609,safety,input,input,609,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:631,safety,modul,module,631,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1607,safety,error,error,1607," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:564,testability,Trace,Traceback,564,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1566,testability,observ,observations,1566," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:548,usability,error,error,548,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:609,usability,input,input,609,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:649,usability,User,Users,649,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:813,usability,User,Users,813,"Subsetting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that d",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1002,usability,User,Users,1002,"tting not working after saving anndata; I've been having some issues recently when trying to subset an anndata object after I save it to disk. Everything works perfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1163,usability,User,Users,1163,"erfectly, but after I save it to disk using `adata.write(filename, compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Le",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1327,usability,User,Users,1327," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1607,usability,error,error,1607," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:1852,usability,learn,learn,1852," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:2006,usability,help,help,2006," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/884:2158,usability,help,help,2158," compression='gzip')` , and then read it back again from disk, the subsetting doesn't work. It only happens after saving to disk, and it seems to only happen after I've run additional analysis (UMAP, rank_genes_groups, etc.... I don't know exactly which entry is the one breaking the writing/reading). . This is the error I get:. > Traceback (most recent call last):. > File ""<input>"", line 48, in <module>. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1230, in __getitem__. > return self._getitem_view(index). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 1234, in _getitem_view. > return AnnData(self, oidx=oidx, vidx=vidx, asview=True). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 561, in __init__. > self._init_as_view(X, oidx, vidx). > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 633, in _init_as_view. > self._raw = adata_ref.raw[oidx]. > File ""/Users/andres/miniconda3/envs/finalenv/lib/python3.7/site-packages/anndata/core/anndata.py"", line 344, in __getitem__. > new._varm = self._varm._view(self, vidx). > AttributeError: 'dict' object has no attribute '_view'. Any subsetting (of observations or variables) gives me this error. I tried removing keys from `.uns`, etc. , but nothing seems to work. These are the versions I'm running, all installed with conda:. > scanpy==1.4.4.post1 anndata==0.6.22.post1 umap==0.3.10 numpy==1.17.2 scipy==1.3.1 pandas==0.25.2 scikit-learn==0.21.3 statsmodels==0.10.1 python-igraph==0.7.1 louvain==0.6.1. I also tried using different (or no) compression to save and read, but that didn't help either. I don't know if it's related to the recent problems with h5py, but either way I am using h5py=2.9.0 and hdf5=1.10.5. Hopefully someone can help! Let me know if I should post this in the anndata repository instead.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/884
https://github.com/scverse/scanpy/issues/885:872,availability,sli,sliced,872,"Cannot subset 2 louvain from adata; adata has many louvains. I want to subset other louvain rather than louvain 30 and 32. hence i write . ```. adata1 = adata[adata.obs['louvain'] != '30', :]. adata2 = adata1[adata1.obs['louvain'] != '32', :]. #or. adata1 = adata[adata.obs['louvain'] != '32', :]. adata2 = adata1[adata1.obs['louvain'] != '30', :]. ```. however , i got the result `IndexError: Item wrong length 36630 instead of 36708.`. and then i write. ```. adata1 = adata[adata.obs['louvain'] not in ['30','32'], :]. #or. adata1 = adata[adata.obs['louvain'] != '30' or adata.obs['louvain'] != '32' , :]. ```. i got `ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().`. and . ```. adata_temp = adata[adata.obs['louvain'] != '32', adata.obs['louvain'] != '30', :]. ```. results in `ValueError: AnnData can only be sliced in rows and columns.`. . could anyone help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/885
https://github.com/scverse/scanpy/issues/885:7,integrability,sub,subset,7,"Cannot subset 2 louvain from adata; adata has many louvains. I want to subset other louvain rather than louvain 30 and 32. hence i write . ```. adata1 = adata[adata.obs['louvain'] != '30', :]. adata2 = adata1[adata1.obs['louvain'] != '32', :]. #or. adata1 = adata[adata.obs['louvain'] != '32', :]. adata2 = adata1[adata1.obs['louvain'] != '30', :]. ```. however , i got the result `IndexError: Item wrong length 36630 instead of 36708.`. and then i write. ```. adata1 = adata[adata.obs['louvain'] not in ['30','32'], :]. #or. adata1 = adata[adata.obs['louvain'] != '30' or adata.obs['louvain'] != '32' , :]. ```. i got `ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().`. and . ```. adata_temp = adata[adata.obs['louvain'] != '32', adata.obs['louvain'] != '30', :]. ```. results in `ValueError: AnnData can only be sliced in rows and columns.`. . could anyone help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/885
https://github.com/scverse/scanpy/issues/885:71,integrability,sub,subset,71,"Cannot subset 2 louvain from adata; adata has many louvains. I want to subset other louvain rather than louvain 30 and 32. hence i write . ```. adata1 = adata[adata.obs['louvain'] != '30', :]. adata2 = adata1[adata1.obs['louvain'] != '32', :]. #or. adata1 = adata[adata.obs['louvain'] != '32', :]. adata2 = adata1[adata1.obs['louvain'] != '30', :]. ```. however , i got the result `IndexError: Item wrong length 36630 instead of 36708.`. and then i write. ```. adata1 = adata[adata.obs['louvain'] not in ['30','32'], :]. #or. adata1 = adata[adata.obs['louvain'] != '30' or adata.obs['louvain'] != '32' , :]. ```. i got `ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().`. and . ```. adata_temp = adata[adata.obs['louvain'] != '32', adata.obs['louvain'] != '30', :]. ```. results in `ValueError: AnnData can only be sliced in rows and columns.`. . could anyone help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/885
https://github.com/scverse/scanpy/issues/885:872,reliability,sli,sliced,872,"Cannot subset 2 louvain from adata; adata has many louvains. I want to subset other louvain rather than louvain 30 and 32. hence i write . ```. adata1 = adata[adata.obs['louvain'] != '30', :]. adata2 = adata1[adata1.obs['louvain'] != '32', :]. #or. adata1 = adata[adata.obs['louvain'] != '32', :]. adata2 = adata1[adata1.obs['louvain'] != '30', :]. ```. however , i got the result `IndexError: Item wrong length 36630 instead of 36708.`. and then i write. ```. adata1 = adata[adata.obs['louvain'] not in ['30','32'], :]. #or. adata1 = adata[adata.obs['louvain'] != '30' or adata.obs['louvain'] != '32' , :]. ```. i got `ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().`. and . ```. adata_temp = adata[adata.obs['louvain'] != '32', adata.obs['louvain'] != '30', :]. ```. results in `ValueError: AnnData can only be sliced in rows and columns.`. . could anyone help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/885
https://github.com/scverse/scanpy/issues/885:917,usability,help,help,917,"Cannot subset 2 louvain from adata; adata has many louvains. I want to subset other louvain rather than louvain 30 and 32. hence i write . ```. adata1 = adata[adata.obs['louvain'] != '30', :]. adata2 = adata1[adata1.obs['louvain'] != '32', :]. #or. adata1 = adata[adata.obs['louvain'] != '32', :]. adata2 = adata1[adata1.obs['louvain'] != '30', :]. ```. however , i got the result `IndexError: Item wrong length 36630 instead of 36708.`. and then i write. ```. adata1 = adata[adata.obs['louvain'] not in ['30','32'], :]. #or. adata1 = adata[adata.obs['louvain'] != '30' or adata.obs['louvain'] != '32' , :]. ```. i got `ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().`. and . ```. adata_temp = adata[adata.obs['louvain'] != '32', adata.obs['louvain'] != '30', :]. ```. results in `ValueError: AnnData can only be sliced in rows and columns.`. . could anyone help me ?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/885
https://github.com/scverse/scanpy/pull/886:24,usability,Feedback,Feedback,24,Create issue templates; Feedback please!,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/886
https://github.com/scverse/scanpy/issues/887:219,deployability,fail,fails,219,"Saving then reading turns list into numpy.ndarray?; Hey! I just noticed that saving and then reading an anndata object turns a `list` in `adata.uns['velocity_settings']['embeddings']` into a `numpy.ndarray`, which then fails in an `scvelo` function. Is this a scanpy or an anndata issue? It seems related to a lot of the stuff I've been reading.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/887:219,reliability,fail,fails,219,"Saving then reading turns list into numpy.ndarray?; Hey! I just noticed that saving and then reading an anndata object turns a `list` in `adata.uns['velocity_settings']['embeddings']` into a `numpy.ndarray`, which then fails in an `scvelo` function. Is this a scanpy or an anndata issue? It seems related to a lot of the stuff I've been reading.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/887
https://github.com/scverse/scanpy/issues/888:544,reliability,doe,does,544,"n_bins not respected in highly_variable_genes(..., flavour='cell_ranger'); This code creates the range `[10, 15, , 100]`, which is 19 values. Taking those percentiles together with -inf and +inf creates 20 bins (from 21 bin borders), understood. https://github.com/theislab/scanpy/blob/e6e08e51d63c78581bb9c86fe6e302b80baef623/scanpy/preprocessing/_highly_variable_genes.py#L90. But 1. why start at 10% and 2. why is n_bins ignored and there are always 20 bins created? @Koncopd @falexwolf you introduced this in #330, is this what cellranger does?",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/888
https://github.com/scverse/scanpy/issues/889:280,availability,error,error,280,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:654,deployability,modul,module,654,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:1055,deployability,continu,continuous,1055,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:1271,energy efficiency,profil,profiles,1271,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:654,modifiability,modul,module,654,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:876,modifiability,pac,packages,876,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:1352,modifiability,pac,packages,1352,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:1655,modifiability,pac,packages,1655,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:280,performance,error,error,280,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:672,performance,time,time,672,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:685,performance,time,time,685,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:690,performance,time,time,690,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:815,performance,time,time,815,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:820,performance,time,time,820,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:1271,performance,profil,profiles,1271,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:280,safety,error,error,280,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:627,safety,input,input-,627,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:654,safety,modul,module,654,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:583,testability,Trace,Traceback,583,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:280,usability,error,error,280,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:627,usability,input,input-,627,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/889:1298,usability,visual,visualizer,1298,"TypeError when using the exporting.spring_project function; I've been trying to export my data to SPRING using the external export feature. I do all my processing in Seurat and then export a loom file to be read in scanpy and export to SPRING. I've been however getting the below error. The notebook with the code and the loom file can be found here: https://github.com/tejas-j/seurat2spring. Is there a better way to achieve what I'm trying to do (data -> Seurat -> scanpy -> SPRING) . ```pytb. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). <ipython-input-13-3b0044b18ade> in <module>. 1 import time. 2 t0 = time.time(). ----> 3 sce.exporting.spring_project(data, './pbmc3k', 'draw_graph', subplot_name='force1', overwrite=True). 4 print(time.time() - t0). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in spring_project(adata, project_dir, embedding_method, subplot_name, cell_groupings, custom_color_tracks, total_counts_key, overwrite). 157 # Write continuous colors. 158 continuous_extras['Uniform'] = np.zeros(E.shape[0]). --> 159 write_color_tracks(continuous_extras, subplot_dir / 'color_data_gene_sets.csv'). 160 . 161 # Create and write a dictionary of color profiles to be used by the visualizer. ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in write_color_tracks(ctracks, fname). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). ~/miniconda2/envs/py36/lib/python3.6/site-packages/scanpy/_exporting.py in <listcomp>(.0). 301 out = []. 302 for name,score in ctracks.items():. --> 303 line = name + ',' + ','.join(['%.3f' %x for x in score]). 304 out += [line]. 305 out = sorted(out,key=lambda x: x.split(',')[0]). TypeError: must be real number, not numpy.str_. ```.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/889
https://github.com/scverse/scanpy/issues/890:23,integrability,Sub,Subplots,23,"Reorder Stacked_Violin Subplots; Hello @fidelram,. This is not an issue but I would really like some help with using the 'order' parameter in the stacked_violin plot function. If I have cell type as the y-axis and the genes in the x-axis, how would I rearrange the order to the cell types in the y-axis. . Providing the 'order' parameter with a list of cell types being used by the function doesn't seem to be working. I am just really confused how else to interpret these instructions for the stacked_violin function: order : list of str, optional (default: True) . Thank you,. Behram.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/890:129,modifiability,paramet,parameter,129,"Reorder Stacked_Violin Subplots; Hello @fidelram,. This is not an issue but I would really like some help with using the 'order' parameter in the stacked_violin plot function. If I have cell type as the y-axis and the genes in the x-axis, how would I rearrange the order to the cell types in the y-axis. . Providing the 'order' parameter with a list of cell types being used by the function doesn't seem to be working. I am just really confused how else to interpret these instructions for the stacked_violin function: order : list of str, optional (default: True) . Thank you,. Behram.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/890:328,modifiability,paramet,parameter,328,"Reorder Stacked_Violin Subplots; Hello @fidelram,. This is not an issue but I would really like some help with using the 'order' parameter in the stacked_violin plot function. If I have cell type as the y-axis and the genes in the x-axis, how would I rearrange the order to the cell types in the y-axis. . Providing the 'order' parameter with a list of cell types being used by the function doesn't seem to be working. I am just really confused how else to interpret these instructions for the stacked_violin function: order : list of str, optional (default: True) . Thank you,. Behram.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/890:391,reliability,doe,doesn,391,"Reorder Stacked_Violin Subplots; Hello @fidelram,. This is not an issue but I would really like some help with using the 'order' parameter in the stacked_violin plot function. If I have cell type as the y-axis and the genes in the x-axis, how would I rearrange the order to the cell types in the y-axis. . Providing the 'order' parameter with a list of cell types being used by the function doesn't seem to be working. I am just really confused how else to interpret these instructions for the stacked_violin function: order : list of str, optional (default: True) . Thank you,. Behram.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/890:101,usability,help,help,101,"Reorder Stacked_Violin Subplots; Hello @fidelram,. This is not an issue but I would really like some help with using the 'order' parameter in the stacked_violin plot function. If I have cell type as the y-axis and the genes in the x-axis, how would I rearrange the order to the cell types in the y-axis. . Providing the 'order' parameter with a list of cell types being used by the function doesn't seem to be working. I am just really confused how else to interpret these instructions for the stacked_violin function: order : list of str, optional (default: True) . Thank you,. Behram.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/890
https://github.com/scverse/scanpy/issues/891:408,integrability,sub,subsetted,408,"Plotting order in embedding plots with groups parameter ; When we use the `groups` parameter of `sc.pl.umap` (and other embedding plots), plotting order of groups is random. This means that gray background group can be plotted over the groups of interest and obscure dots of other colors. Here the top 2 plots are plotted with `groups=` and bottom ones are plotted manually via calling sc.pl.umap twice with subsetted anndata (e.g. adata[adata.obs.x != 'y'] and adata[adata.obs.x == 'y']):. ![image](https://user-images.githubusercontent.com/1140359/67532602-da535d80-f694-11e9-9ed2-3d544013ff78.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/891
https://github.com/scverse/scanpy/issues/891:46,modifiability,paramet,parameter,46,"Plotting order in embedding plots with groups parameter ; When we use the `groups` parameter of `sc.pl.umap` (and other embedding plots), plotting order of groups is random. This means that gray background group can be plotted over the groups of interest and obscure dots of other colors. Here the top 2 plots are plotted with `groups=` and bottom ones are plotted manually via calling sc.pl.umap twice with subsetted anndata (e.g. adata[adata.obs.x != 'y'] and adata[adata.obs.x == 'y']):. ![image](https://user-images.githubusercontent.com/1140359/67532602-da535d80-f694-11e9-9ed2-3d544013ff78.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/891
https://github.com/scverse/scanpy/issues/891:83,modifiability,paramet,parameter,83,"Plotting order in embedding plots with groups parameter ; When we use the `groups` parameter of `sc.pl.umap` (and other embedding plots), plotting order of groups is random. This means that gray background group can be plotted over the groups of interest and obscure dots of other colors. Here the top 2 plots are plotted with `groups=` and bottom ones are plotted manually via calling sc.pl.umap twice with subsetted anndata (e.g. adata[adata.obs.x != 'y'] and adata[adata.obs.x == 'y']):. ![image](https://user-images.githubusercontent.com/1140359/67532602-da535d80-f694-11e9-9ed2-3d544013ff78.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/891
https://github.com/scverse/scanpy/issues/891:508,usability,user,user-images,508,"Plotting order in embedding plots with groups parameter ; When we use the `groups` parameter of `sc.pl.umap` (and other embedding plots), plotting order of groups is random. This means that gray background group can be plotted over the groups of interest and obscure dots of other colors. Here the top 2 plots are plotted with `groups=` and bottom ones are plotted manually via calling sc.pl.umap twice with subsetted anndata (e.g. adata[adata.obs.x != 'y'] and adata[adata.obs.x == 'y']):. ![image](https://user-images.githubusercontent.com/1140359/67532602-da535d80-f694-11e9-9ed2-3d544013ff78.png).",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/891
https://github.com/scverse/scanpy/pull/893:454,safety,test,test,454,"Fix plot issues; Fix issue related to #890 and the reordering of stacked_violin_plots when `swap_axes=False` and for the case when `dendrogram` and `order` are given, in which case dendrogram order takes precedence. . Fix issue #891, now if `groups` is set for an embedding, the cells belonging to those groups are plotted on top. I also added missing documentation, pointing that the marker size could be a list of per-cell size. . Added a new plotting test for the groups option.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:454,testability,test,test,454,"Fix plot issues; Fix issue related to #890 and the reordering of stacked_violin_plots when `swap_axes=False` and for the case when `dendrogram` and `order` are given, in which case dendrogram order takes precedence. . Fix issue #891, now if `groups` is set for an embedding, the cells belonging to those groups are plotted on top. I also added missing documentation, pointing that the marker size could be a list of per-cell size. . Added a new plotting test for the groups option.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/pull/893:352,usability,document,documentation,352,"Fix plot issues; Fix issue related to #890 and the reordering of stacked_violin_plots when `swap_axes=False` and for the case when `dendrogram` and `order` are given, in which case dendrogram order takes precedence. . Fix issue #891, now if `groups` is set for an embedding, the cells belonging to those groups are plotted on top. I also added missing documentation, pointing that the marker size could be a list of per-cell size. . Added a new plotting test for the groups option.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/893
https://github.com/scverse/scanpy/issues/895:73,availability,error,error,73,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:236,availability,avail,available,236,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:16,deployability,modul,module,16,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:31,deployability,api,api,31,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:35,deployability,log,logging,35,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:193,deployability,version,versions,193,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:31,integrability,api,api,31,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:193,integrability,version,versions,193,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:31,interoperability,api,api,31,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:16,modifiability,modul,module,16,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:193,modifiability,version,versions,193,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:73,performance,error,error,73,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:236,reliability,availab,available,236,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:16,safety,modul,module,16,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:35,safety,log,logging,35,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:73,safety,error,error,73,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:236,safety,avail,available,236,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:35,security,log,logging,35,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:236,security,availab,available,236,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:35,testability,log,logging,35,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/issues/895:73,usability,error,error,73,AttributeError: module 'scanpy.api.logging' has no attribute 'msg'; This error appears when using DESC autoencoder (github: eleozzr/desc) under scanpy 1.4.4. This was not an issue with earlier versions of scanpy. Is there a work-around available?,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/895
https://github.com/scverse/scanpy/pull/896:0,deployability,Updat,Update,0,Update MAGIC API and docs; The MAGIC docs and API are a little out of date.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:13,deployability,API,API,13,Update MAGIC API and docs; The MAGIC docs and API are a little out of date.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:46,deployability,API,API,46,Update MAGIC API and docs; The MAGIC docs and API are a little out of date.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:13,integrability,API,API,13,Update MAGIC API and docs; The MAGIC docs and API are a little out of date.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:46,integrability,API,API,46,Update MAGIC API and docs; The MAGIC docs and API are a little out of date.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:13,interoperability,API,API,13,Update MAGIC API and docs; The MAGIC docs and API are a little out of date.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:46,interoperability,API,API,46,Update MAGIC API and docs; The MAGIC docs and API are a little out of date.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:0,safety,Updat,Update,0,Update MAGIC API and docs; The MAGIC docs and API are a little out of date.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/pull/896:0,security,Updat,Update,0,Update MAGIC API and docs; The MAGIC docs and API are a little out of date.,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/896
https://github.com/scverse/scanpy/issues/897:9,deployability,stack,stack,9,"Fix font stack; Continuation from #805, cc @gokceneraslan @falexwolf . > Why do we change Matplotlib ""font.sans-serif"" anyway? Alex started replacing fonts from the beginning:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. He then updated them to the stack we have now:. https://github.com/theislab/scanpy/blob/6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2/scanpy/plotting.py#L763-L767. Theres some known [metrically compatible](https://wiki.archlinux.org/index.php/Metric-compatible_fonts) fonts, but our stack doesnt reflect that, as e.g. DejaVu Sans isnt metrically compatible to Arial and Helvetica. Is the [default matplotlib stack](https://github.com/matplotlib/matplotlib/blob/90fba030f534b3d7068e536fcfc4a2dc8d459eeb/lib/matplotlib/rcsetup.py#L1125-L1129) better or can we at least gain inspiration from it? What would be a safe stack to use on most OSs (even if that means fiddling with our plots if we change the stack to not-Helvetica-like)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:16,deployability,Continu,Continuation,16,"Fix font stack; Continuation from #805, cc @gokceneraslan @falexwolf . > Why do we change Matplotlib ""font.sans-serif"" anyway? Alex started replacing fonts from the beginning:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. He then updated them to the stack we have now:. https://github.com/theislab/scanpy/blob/6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2/scanpy/plotting.py#L763-L767. Theres some known [metrically compatible](https://wiki.archlinux.org/index.php/Metric-compatible_fonts) fonts, but our stack doesnt reflect that, as e.g. DejaVu Sans isnt metrically compatible to Arial and Helvetica. Is the [default matplotlib stack](https://github.com/matplotlib/matplotlib/blob/90fba030f534b3d7068e536fcfc4a2dc8d459eeb/lib/matplotlib/rcsetup.py#L1125-L1129) better or can we at least gain inspiration from it? What would be a safe stack to use on most OSs (even if that means fiddling with our plots if we change the stack to not-Helvetica-like)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:291,deployability,updat,updated,291,"Fix font stack; Continuation from #805, cc @gokceneraslan @falexwolf . > Why do we change Matplotlib ""font.sans-serif"" anyway? Alex started replacing fonts from the beginning:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. He then updated them to the stack we have now:. https://github.com/theislab/scanpy/blob/6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2/scanpy/plotting.py#L763-L767. Theres some known [metrically compatible](https://wiki.archlinux.org/index.php/Metric-compatible_fonts) fonts, but our stack doesnt reflect that, as e.g. DejaVu Sans isnt metrically compatible to Arial and Helvetica. Is the [default matplotlib stack](https://github.com/matplotlib/matplotlib/blob/90fba030f534b3d7068e536fcfc4a2dc8d459eeb/lib/matplotlib/rcsetup.py#L1125-L1129) better or can we at least gain inspiration from it? What would be a safe stack to use on most OSs (even if that means fiddling with our plots if we change the stack to not-Helvetica-like)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:311,deployability,stack,stack,311,"Fix font stack; Continuation from #805, cc @gokceneraslan @falexwolf . > Why do we change Matplotlib ""font.sans-serif"" anyway? Alex started replacing fonts from the beginning:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. He then updated them to the stack we have now:. https://github.com/theislab/scanpy/blob/6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2/scanpy/plotting.py#L763-L767. Theres some known [metrically compatible](https://wiki.archlinux.org/index.php/Metric-compatible_fonts) fonts, but our stack doesnt reflect that, as e.g. DejaVu Sans isnt metrically compatible to Arial and Helvetica. Is the [default matplotlib stack](https://github.com/matplotlib/matplotlib/blob/90fba030f534b3d7068e536fcfc4a2dc8d459eeb/lib/matplotlib/rcsetup.py#L1125-L1129) better or can we at least gain inspiration from it? What would be a safe stack to use on most OSs (even if that means fiddling with our plots if we change the stack to not-Helvetica-like)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:562,deployability,stack,stack,562,"Fix font stack; Continuation from #805, cc @gokceneraslan @falexwolf . > Why do we change Matplotlib ""font.sans-serif"" anyway? Alex started replacing fonts from the beginning:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. He then updated them to the stack we have now:. https://github.com/theislab/scanpy/blob/6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2/scanpy/plotting.py#L763-L767. Theres some known [metrically compatible](https://wiki.archlinux.org/index.php/Metric-compatible_fonts) fonts, but our stack doesnt reflect that, as e.g. DejaVu Sans isnt metrically compatible to Arial and Helvetica. Is the [default matplotlib stack](https://github.com/matplotlib/matplotlib/blob/90fba030f534b3d7068e536fcfc4a2dc8d459eeb/lib/matplotlib/rcsetup.py#L1125-L1129) better or can we at least gain inspiration from it? What would be a safe stack to use on most OSs (even if that means fiddling with our plots if we change the stack to not-Helvetica-like)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:689,deployability,stack,stack,689,"Fix font stack; Continuation from #805, cc @gokceneraslan @falexwolf . > Why do we change Matplotlib ""font.sans-serif"" anyway? Alex started replacing fonts from the beginning:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. He then updated them to the stack we have now:. https://github.com/theislab/scanpy/blob/6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2/scanpy/plotting.py#L763-L767. Theres some known [metrically compatible](https://wiki.archlinux.org/index.php/Metric-compatible_fonts) fonts, but our stack doesnt reflect that, as e.g. DejaVu Sans isnt metrically compatible to Arial and Helvetica. Is the [default matplotlib stack](https://github.com/matplotlib/matplotlib/blob/90fba030f534b3d7068e536fcfc4a2dc8d459eeb/lib/matplotlib/rcsetup.py#L1125-L1129) better or can we at least gain inspiration from it? What would be a safe stack to use on most OSs (even if that means fiddling with our plots if we change the stack to not-Helvetica-like)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:895,deployability,stack,stack,895,"Fix font stack; Continuation from #805, cc @gokceneraslan @falexwolf . > Why do we change Matplotlib ""font.sans-serif"" anyway? Alex started replacing fonts from the beginning:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. He then updated them to the stack we have now:. https://github.com/theislab/scanpy/blob/6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2/scanpy/plotting.py#L763-L767. Theres some known [metrically compatible](https://wiki.archlinux.org/index.php/Metric-compatible_fonts) fonts, but our stack doesnt reflect that, as e.g. DejaVu Sans isnt metrically compatible to Arial and Helvetica. Is the [default matplotlib stack](https://github.com/matplotlib/matplotlib/blob/90fba030f534b3d7068e536fcfc4a2dc8d459eeb/lib/matplotlib/rcsetup.py#L1125-L1129) better or can we at least gain inspiration from it? What would be a safe stack to use on most OSs (even if that means fiddling with our plots if we change the stack to not-Helvetica-like)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:981,deployability,stack,stack,981,"Fix font stack; Continuation from #805, cc @gokceneraslan @falexwolf . > Why do we change Matplotlib ""font.sans-serif"" anyway? Alex started replacing fonts from the beginning:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. He then updated them to the stack we have now:. https://github.com/theislab/scanpy/blob/6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2/scanpy/plotting.py#L763-L767. Theres some known [metrically compatible](https://wiki.archlinux.org/index.php/Metric-compatible_fonts) fonts, but our stack doesnt reflect that, as e.g. DejaVu Sans isnt metrically compatible to Arial and Helvetica. Is the [default matplotlib stack](https://github.com/matplotlib/matplotlib/blob/90fba030f534b3d7068e536fcfc4a2dc8d459eeb/lib/matplotlib/rcsetup.py#L1125-L1129) better or can we at least gain inspiration from it? What would be a safe stack to use on most OSs (even if that means fiddling with our plots if we change the stack to not-Helvetica-like)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:473,interoperability,compatib,compatible,473,"Fix font stack; Continuation from #805, cc @gokceneraslan @falexwolf . > Why do we change Matplotlib ""font.sans-serif"" anyway? Alex started replacing fonts from the beginning:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. He then updated them to the stack we have now:. https://github.com/theislab/scanpy/blob/6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2/scanpy/plotting.py#L763-L767. Theres some known [metrically compatible](https://wiki.archlinux.org/index.php/Metric-compatible_fonts) fonts, but our stack doesnt reflect that, as e.g. DejaVu Sans isnt metrically compatible to Arial and Helvetica. Is the [default matplotlib stack](https://github.com/matplotlib/matplotlib/blob/90fba030f534b3d7068e536fcfc4a2dc8d459eeb/lib/matplotlib/rcsetup.py#L1125-L1129) better or can we at least gain inspiration from it? What would be a safe stack to use on most OSs (even if that means fiddling with our plots if we change the stack to not-Helvetica-like)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:627,interoperability,compatib,compatible,627,"Fix font stack; Continuation from #805, cc @gokceneraslan @falexwolf . > Why do we change Matplotlib ""font.sans-serif"" anyway? Alex started replacing fonts from the beginning:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. He then updated them to the stack we have now:. https://github.com/theislab/scanpy/blob/6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2/scanpy/plotting.py#L763-L767. Theres some known [metrically compatible](https://wiki.archlinux.org/index.php/Metric-compatible_fonts) fonts, but our stack doesnt reflect that, as e.g. DejaVu Sans isnt metrically compatible to Arial and Helvetica. Is the [default matplotlib stack](https://github.com/matplotlib/matplotlib/blob/90fba030f534b3d7068e536fcfc4a2dc8d459eeb/lib/matplotlib/rcsetup.py#L1125-L1129) better or can we at least gain inspiration from it? What would be a safe stack to use on most OSs (even if that means fiddling with our plots if we change the stack to not-Helvetica-like)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:568,reliability,doe,doesn,568,"Fix font stack; Continuation from #805, cc @gokceneraslan @falexwolf . > Why do we change Matplotlib ""font.sans-serif"" anyway? Alex started replacing fonts from the beginning:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. He then updated them to the stack we have now:. https://github.com/theislab/scanpy/blob/6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2/scanpy/plotting.py#L763-L767. Theres some known [metrically compatible](https://wiki.archlinux.org/index.php/Metric-compatible_fonts) fonts, but our stack doesnt reflect that, as e.g. DejaVu Sans isnt metrically compatible to Arial and Helvetica. Is the [default matplotlib stack](https://github.com/matplotlib/matplotlib/blob/90fba030f534b3d7068e536fcfc4a2dc8d459eeb/lib/matplotlib/rcsetup.py#L1125-L1129) better or can we at least gain inspiration from it? What would be a safe stack to use on most OSs (even if that means fiddling with our plots if we change the stack to not-Helvetica-like)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:291,safety,updat,updated,291,"Fix font stack; Continuation from #805, cc @gokceneraslan @falexwolf . > Why do we change Matplotlib ""font.sans-serif"" anyway? Alex started replacing fonts from the beginning:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. He then updated them to the stack we have now:. https://github.com/theislab/scanpy/blob/6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2/scanpy/plotting.py#L763-L767. Theres some known [metrically compatible](https://wiki.archlinux.org/index.php/Metric-compatible_fonts) fonts, but our stack doesnt reflect that, as e.g. DejaVu Sans isnt metrically compatible to Arial and Helvetica. Is the [default matplotlib stack](https://github.com/matplotlib/matplotlib/blob/90fba030f534b3d7068e536fcfc4a2dc8d459eeb/lib/matplotlib/rcsetup.py#L1125-L1129) better or can we at least gain inspiration from it? What would be a safe stack to use on most OSs (even if that means fiddling with our plots if we change the stack to not-Helvetica-like)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:890,safety,safe,safe,890,"Fix font stack; Continuation from #805, cc @gokceneraslan @falexwolf . > Why do we change Matplotlib ""font.sans-serif"" anyway? Alex started replacing fonts from the beginning:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. He then updated them to the stack we have now:. https://github.com/theislab/scanpy/blob/6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2/scanpy/plotting.py#L763-L767. Theres some known [metrically compatible](https://wiki.archlinux.org/index.php/Metric-compatible_fonts) fonts, but our stack doesnt reflect that, as e.g. DejaVu Sans isnt metrically compatible to Arial and Helvetica. Is the [default matplotlib stack](https://github.com/matplotlib/matplotlib/blob/90fba030f534b3d7068e536fcfc4a2dc8d459eeb/lib/matplotlib/rcsetup.py#L1125-L1129) better or can we at least gain inspiration from it? What would be a safe stack to use on most OSs (even if that means fiddling with our plots if we change the stack to not-Helvetica-like)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/issues/897:291,security,updat,updated,291,"Fix font stack; Continuation from #805, cc @gokceneraslan @falexwolf . > Why do we change Matplotlib ""font.sans-serif"" anyway? Alex started replacing fonts from the beginning:. https://github.com/theislab/scanpy/blob/c22e48abe45a6ccca5918bbf689637caa4b31250/scanpy/plotting.py#L605. He then updated them to the stack we have now:. https://github.com/theislab/scanpy/blob/6c68b8ba2821f27bd0b8f499a1d543dff9cc51b2/scanpy/plotting.py#L763-L767. Theres some known [metrically compatible](https://wiki.archlinux.org/index.php/Metric-compatible_fonts) fonts, but our stack doesnt reflect that, as e.g. DejaVu Sans isnt metrically compatible to Arial and Helvetica. Is the [default matplotlib stack](https://github.com/matplotlib/matplotlib/blob/90fba030f534b3d7068e536fcfc4a2dc8d459eeb/lib/matplotlib/rcsetup.py#L1125-L1129) better or can we at least gain inspiration from it? What would be a safe stack to use on most OSs (even if that means fiddling with our plots if we change the stack to not-Helvetica-like)",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/897
https://github.com/scverse/scanpy/pull/898:0,modifiability,Inherit,Inherit,0,"Inherit main requirements; To not repeat ourselves @ivirshup (I think) suggested this. Lets see if readthedocs supports this. If so, this should soon be visible: https://icb-scanpy.readthedocs-hosted.com/en/inherit-requirements/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/898
https://github.com/scverse/scanpy/pull/898:208,modifiability,inherit,inherit-requirements,208,"Inherit main requirements; To not repeat ourselves @ivirshup (I think) suggested this. Lets see if readthedocs supports this. If so, this should soon be visible: https://icb-scanpy.readthedocs-hosted.com/en/inherit-requirements/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/898
https://github.com/scverse/scanpy/pull/898:112,usability,support,supports,112,"Inherit main requirements; To not repeat ourselves @ivirshup (I think) suggested this. Lets see if readthedocs supports this. If so, this should soon be visible: https://icb-scanpy.readthedocs-hosted.com/en/inherit-requirements/",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/898
https://github.com/scverse/scanpy/pull/899:43,integrability,discover,discovered,43,add 'layer' option to sc.pl.violin; I just discovered that this was missing,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/899
https://github.com/scverse/scanpy/pull/899:43,interoperability,discover,discovered,43,add 'layer' option to sc.pl.violin; I just discovered that this was missing,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/899
https://github.com/scverse/scanpy/pull/899:5,modifiability,layer,layer,5,add 'layer' option to sc.pl.violin; I just discovered that this was missing,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/899
https://github.com/scverse/scanpy/pull/899:43,usability,discov,discovered,43,add 'layer' option to sc.pl.violin; I just discovered that this was missing,MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/899
https://github.com/scverse/scanpy/issues/900:122,deployability,modul,module,122,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:262,deployability,modul,module,262,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:415,deployability,version,version,415,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:436,deployability,version,versioneer,436,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:504,deployability,modul,module,504,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:530,deployability,api,api,530,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:628,deployability,log,logging,628,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:639,deployability,log,logg,639,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:721,deployability,modul,module,721,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:832,deployability,log,logging,832,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:850,deployability,log,logging,850,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:952,deployability,log,logging,952,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:967,deployability,modul,module,967,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1036,deployability,log,logging,1036,"ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.sile",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1111,deployability,modul,module,1111,"40> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _regi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1358,deployability,modul,module,1358,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1581,deployability,modul,module,1581,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1814,deployability,modul,module,1814,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:2014,deployability,modul,module,2014,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:2301,deployability,fail,failed,2301,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1136,energy efficiency,core,core,1136,"ort numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _reg",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1338,energy efficiency,core,core,1338,"check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could n",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:2296,energy efficiency,load,load,2296,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:415,integrability,version,version,415,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:436,integrability,version,versioneer,436,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:530,integrability,api,api,530,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:530,interoperability,api,api,530,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:2313,interoperability,specif,specified,2313,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:122,modifiability,modul,module,122,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:230,modifiability,pac,packages,230,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:262,modifiability,modul,module,262,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:415,modifiability,version,version,415,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:436,modifiability,version,versioneer,436,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:475,modifiability,pac,packages,475,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:504,modifiability,modul,module,504,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:688,modifiability,pac,packages,688,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:721,modifiability,modul,module,721,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:936,modifiability,pac,packages,936,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:967,modifiability,modul,module,967,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1078,modifiability,pac,packages,1078,"st). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv imp",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1111,modifiability,modul,module,1111,"40> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _regi",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1321,modifiability,pac,packages,1321," .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified pr",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1358,modifiability,modul,module,1358,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1372,modifiability,Layer,LayersBase,1372,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1384,modifiability,Layer,Layers,1384,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1543,modifiability,pac,packages,1543,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1581,modifiability,modul,module,1581,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1776,modifiability,pac,packages,1776,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1814,modifiability,modul,module,1814,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:1984,modifiability,pac,packages,1984,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:2014,modifiability,modul,module,2014,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:2296,performance,load,load,2296,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:2301,reliability,fail,failed,2301,"ns, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__init__.py in <module>(). 34 _errors.silence_errors(). 35 . ---> 36 from ._conv import register_converters as _register_converters. 37 _register_converters(). 38 . h5py\h5r.pxd in init h5py._conv(). h5py\_objects.pxd in init h5py.h5r(). h5py\_objects.pyx in init h5py._objects(). ImportError: DLL load failed: The specified procedure could not be found.",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:96,safety,input,input-,96,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:122,safety,modul,module,122,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:262,safety,modul,module,262,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:504,safety,modul,module,504,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:628,safety,log,logging,628,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:639,safety,log,logg,639,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
https://github.com/scverse/scanpy/issues/900:721,safety,modul,module,721,"cannot import scanpy in windows system; ImportError Traceback (most recent call last). <ipython-input-1-b6c916879140> in <module>(). 1 import numpy as np. 2 import pandas as pd. ----> 3 import scanpy as sc. ~\Anaconda3-6\lib\site-packages\scanpy\__init__.py in <module>(). 1 # some technical stuff. 2 import sys. ----> 3 from .utils import check_versions, annotate_doc_types. 4 from ._version import get_versions # version generated by versioneer. 5 . ~\Anaconda3-6\lib\site-packages\scanpy\utils.py in <module>(). 17 from pandas.api.types import CategoricalDtype. 18 . ---> 19 from ._settings import settings. 20 from . import logging as logg. 21 import warnings. ~\Anaconda3-6\lib\site-packages\scanpy\_settings.py in <module>(). 7 from typing import Tuple, Union, Any, List, Iterable, TextIO, Optional. 8 . ----> 9 from . import logging. 10 from .logging import _set_log_level, _set_log_file, RootLogger. 11 . ~\Anaconda3-6\lib\site-packages\scanpy\logging.py in <module>(). 7 from typing import Optional. 8 . ----> 9 import anndata.logging. 10 . 11 . ~\Anaconda3-6\lib\site-packages\anndata\__init__.py in <module>(). ----> 1 from .core.anndata import AnnData, Raw. 2 from .readwrite import (. 3 read_h5ad, read_loom, read_hdf,. 4 read_excel, read_umi_tools,. 5 read_csv, read_text, read_mtx,. ~\Anaconda3-6\lib\site-packages\anndata\core\anndata.py in <module>(). 46 LayersBase, Layers. 47 ). ---> 48 from .. import h5py. 49 from .views import ArrayView, SparseCSRView, SparseCSCView, DictView, DataFrameView. 50 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\__init__.py in <module>(). 22 SparseDataset. 23 """""". ---> 24 from .h5sparse import File, Group, SparseDataset, _load_h5_dataset_as_sparse. 25 from h5py import Dataset, special_dtype. 26 . ~\Anaconda3-6\lib\site-packages\anndata\h5py\h5sparse.py in <module>(). 4 from typing import Optional, Union, KeysView, NamedTuple. 5 . ----> 6 import h5py. 7 import numpy as np. 8 import scipy.sparse as ss. ~\Anaconda3-6\lib\site-packages\h5py\__",MatchSource.ISSUE,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/900
