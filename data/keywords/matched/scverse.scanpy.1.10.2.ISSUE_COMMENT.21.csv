id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/scverse/scanpy/issues/1563:184,integrability,configur,configuring,184,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:228,integrability,configur,configurations,228,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:720,integrability,depend,dependencies,720,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1478,integrability,configur,configuring,1478,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:2216,integrability,messag,messages,2216,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:2342,integrability,event,eventually,2342,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:514,interoperability,stub,stubs,514,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1251,interoperability,conflict,conflicting,1251," be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unuse",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1784,interoperability,format,formatting,1784,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:2216,interoperability,messag,messages,2216,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:2231,interoperability,format,formatting,2231,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:25,modifiability,extens,extensive,25,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:184,modifiability,configur,configuring,184,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:228,modifiability,configur,configurations,228,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:720,modifiability,depend,dependencies,720,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1227,modifiability,extens,extensions,1227,"igurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""f",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1377,modifiability,concern,concern,1377," Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1478,modifiability,configur,configuring,1478,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:2415,modifiability,concern,concerns,2415,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:311,performance,load,load,311,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:422,reliability,Doe,Does,422,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:2392,reliability,Doe,Does,2392,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:720,safety,depend,dependencies,720,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1558,safety,compl,complaining,1558,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:184,security,configur,configuring,184,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:228,security,configur,configurations,228,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1478,security,configur,configuring,1478,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1558,security,compl,complaining,1558,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:514,testability,stub,stubs,514,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:720,testability,depend,dependencies,720,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1377,testability,concern,concern,1377," Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on you",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1664,testability,verif,verify,1664,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1813,testability,automat,automated,1813,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:2415,testability,concern,concerns,2415,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:91,usability,tool,tools,91,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:172,usability,learn,learn,172,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:211,usability,tool,tools,211,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:301,usability,cognit,cognitive,301,"@michalk8 thanks for the extensive recommendations! I think I'd like to keep the number of tools used small. It's the worst when you want to fix a bug, but instead have to learn about configuring a linter. More tools means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1213,usability,custom,custom,1213," means more configurations people need to be familiar with, and the goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1278,usability,experien,experience,1278," goal is reducing cognitive load. > Also fixing types for `mypy` takes a while, I'd do it as last. Yeah, I figured this would be the case. Does `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1466,usability,learn,learn,1466,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:1883,usability,tool,tools,1883,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:2003,usability,document,document,2003,"es `mypy` allow partial typing these days? Also, I haven't found the numpy or pandas type stubs to always be great. Have you run into problems around this? I think this would also need to wait at least until we can drop python 3.6 for `anndata`, since adding types there currently means circular dependencies. > `rstcheck` to check the syntax of .rst files. I would particularly like a linter for `rst`. I noticed you also had `doc8`, but you'd recommend `rstcheck` check over this? I'm a little worried, considering its last release was over a year ago. Spell check for prose in doc-strings could also be great, but I could see this being overzealous (is there a good way to notify about misspelled words, while not being annoying about technical terms?). I'm a little worried about some custom sphinx extensions we have, and conflicting with this, any experience here? --------------------------------------------. @Koncopd, I think I agree with your concern, as I said above: it's the worst when you want to fix a bug, but instead have to learn about configuring a linter. I also think it's very easy to add new checks, so someone complaining about new ones is valuable. Per commit, this should always be an option with `git commit --no-verify`, though you could also just not install `pre-commit`. I would like to keep the required checks limited, ideally formatting tasks that can be automated as opposed ""this is poor style"" warnings. I also know these tools can be wrong (e.g. `black` when expression's have many operators, sometimes with chaining) so it would be good to document the escape hatches for this (`# fmt: off` for `black`). That said, we already do require that merged code goes through black before it gets merged, and a benefit of using this would be to not have commit messages like ""formatting"", ""remove unused import"", etc. The pre-commit checks would be a part of CI as well, so it would be *eventually* mandatory – just not on your machine. Does this address your concerns?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:906,deployability,fail,failed,906,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:337,energy efficiency,load,load,337,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:807,integrability,sub,submit,807,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:487,modifiability,concern,concern,487,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:721,modifiability,extens,extensive,721,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:337,performance,load,load,337,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:906,reliability,fail,failed,906,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:487,testability,concern,concern,487,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:697,testability,simpl,simplified,697,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:34,usability,person,personal,34,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:43,usability,experien,experience,43,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:86,usability,tool,tool,86,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:327,usability,cognit,cognitive,327,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:697,usability,simpl,simplified,697,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:744,usability,guid,guide,744,"If I can jump on this, talking by personal experience, it would be a very very useful tool for contributors, especially young/inexperienced ones (like me!). In squidpy @michalk8 put together a very comprehensive check list in pre-commits, and I'm appreciating it more and more as I get familiar with it. yes, there is a lot of cognitive load at the beginning, and yes it can be very (very) painful, but when you get used to it, it soon becomes essential and actually really useful. Only concern of course is that it highers the bar for contributions in the repo, but honestly I'm seeing it being adopted in other large bio-related oss (e.g. https://github.com/napari/napari ). I think this can be simplified by having an extensive contributors guide, and the explicit mention on how to skip pre-commits and submit the PR anyway (and then otehr scanpy dev can jump in and give suggestions on why precommits failed).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:107,deployability,fail,fail,107,"@Koncopd pre-commit doesn’t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess it’s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort! Also we should get #1527 in before doing any big restructuring: It’s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:41,integrability,configur,configured,41,"@Koncopd pre-commit doesn’t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess it’s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort! Also we should get #1527 in before doing any big restructuring: It’s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:356,interoperability,conflict,conflicts,356,"@Koncopd pre-commit doesn’t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess it’s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort! Also we should get #1527 in before doing any big restructuring: It’s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:41,modifiability,configur,configured,41,"@Koncopd pre-commit doesn’t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess it’s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort! Also we should get #1527 in before doing any big restructuring: It’s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:374,performance,time,times,374,"@Koncopd pre-commit doesn’t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess it’s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort! Also we should get #1527 in before doing any big restructuring: It’s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:20,reliability,doe,doesn,20,"@Koncopd pre-commit doesn’t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess it’s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort! Also we should get #1527 in before doing any big restructuring: It’s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:107,reliability,fail,fail,107,"@Koncopd pre-commit doesn’t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess it’s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort! Also we should get #1527 in before doing any big restructuring: It’s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:96,safety,test,tests,96,"@Koncopd pre-commit doesn’t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess it’s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort! Also we should get #1527 in before doing any big restructuring: It’s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:41,security,configur,configured,41,"@Koncopd pre-commit doesn’t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess it’s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort! Also we should get #1527 in before doing any big restructuring: It’s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:220,security,iso,isort,220,"@Koncopd pre-commit doesn’t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess it’s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort! Also we should get #1527 in before doing any big restructuring: It’s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:96,testability,test,tests,96,"@Koncopd pre-commit doesn’t *have* to be configured, you can choose not to enable it. Of course tests will fail then. regarding mypy: I guess it’s possible to make everything return `-> t.Any: # TODO fix typing`. I like isort! Also we should get #1527 in before doing any big restructuring: It’s been through too many rounds of delays and I had to resolve conflicts so many times.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:625,availability,error,errors,625,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:617,deployability,modul,module,617,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:765,energy efficiency,profil,profile,765,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:454,interoperability,conflict,conflicts,454,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:617,modifiability,modul,module,617,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:625,performance,error,errors,625,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:765,performance,profil,profile,765,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:617,safety,modul,module,617,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:625,safety,error,errors,625,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:502,security,iso,isort,502,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:723,security,iso,isort,723,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:757,security,iso,isort,757,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:157,usability,tool,tools,157,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:383,usability,close,close,383,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:625,usability,error,errors,625,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:752,usability,tool,tool,752,"I think it would be nice to start small, then gradually add to this. I'd nominate `black` as the first step, then using `pre-commit` on CI, then adding more tools. I like `black` for a starting place since we can just run it over both dev and stable branches and be confident in nothing breaking. For anything that requires more manual intervention, I think we'll have to wait until close to a new stable branch being cut so we don't have to worry about conflicts during backports. I had also thought `isort` could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (*imperative programming strikes again!*). <details>. <summary> Some initial settings for `isort` </summary>. ```toml. [tool.isort]. profile = ""black"". multi_line_output = 3. skip = ""scanpy/__init__.py"". ```. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:23,security,iso,isort,23,"I just added `black`, `isort`, and `autopep8` to `scprep` which worked pretty seamlessly. . pre-commit config: https://github.com/KrishnaswamyLab/scprep/blob/dev/.pre-commit-config.yaml. precommit github action: https://github.com/KrishnaswamyLab/scprep/blob/dev/.github/workflows/pre-commit.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:271,usability,workflow,workflows,271,"I just added `black`, `isort`, and `autopep8` to `scprep` which worked pretty seamlessly. . pre-commit config: https://github.com/KrishnaswamyLab/scprep/blob/dev/.pre-commit-config.yaml. precommit github action: https://github.com/KrishnaswamyLab/scprep/blob/dev/.github/workflows/pre-commit.yml",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:79,deployability,build,builds,79,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:171,deployability,instal,install,171,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:262,deployability,configurat,configuration,262,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:262,integrability,configur,configuration,262,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:262,modifiability,configur,configuration,262,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:245,performance,time,time,245,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:253,security,hash,hash,253,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:262,security,configur,configuration,262,"I've just merged initial pre-commit stuff via #1684 (just black). Once the doc builds propagate, there will be a section under: ""Getting set up"" in the dev docs on how to install. I would like to see a `flake8` PR, though it might take a bit of time to hash out configuration. Maybe @giovp or @Zethson would be interested in looking into this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:53,performance,time,time,53,I'd prefer to have only black on pre-commit for some time.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:4,usability,prefer,prefer,4,I'd prefer to have only black on pre-commit for some time.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:93,interoperability,format,format,93,> I'd prefer to have only black on pre-commit for some time. flake8 just checks and does not format. It makes aware of some nasty code and these things should get fixed immediately. I strongly advocate for it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:55,performance,time,time,55,> I'd prefer to have only black on pre-commit for some time. flake8 just checks and does not format. It makes aware of some nasty code and these things should get fixed immediately. I strongly advocate for it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:84,reliability,doe,does,84,> I'd prefer to have only black on pre-commit for some time. flake8 just checks and does not format. It makes aware of some nasty code and these things should get fixed immediately. I strongly advocate for it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:6,usability,prefer,prefer,6,> I'd prefer to have only black on pre-commit for some time. flake8 just checks and does not format. It makes aware of some nasty code and these things should get fixed immediately. I strongly advocate for it.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:143,availability,error,errors,143,"> I had also thought isort could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (imperative programming strikes again!). Yeah, I ran into this stuff when creating the flake8 PR (#1689). isort is dangerous with Scanpy and requires good testing and many exceptions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:135,deployability,modul,module,135,"> I had also thought isort could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (imperative programming strikes again!). Yeah, I ran into this stuff when creating the flake8 PR (#1689). isort is dangerous with Scanpy and requires good testing and many exceptions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:135,modifiability,modul,module,135,"> I had also thought isort could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (imperative programming strikes again!). Yeah, I ran into this stuff when creating the flake8 PR (#1689). isort is dangerous with Scanpy and requires good testing and many exceptions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:143,performance,error,errors,143,"> I had also thought isort could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (imperative programming strikes again!). Yeah, I ran into this stuff when creating the flake8 PR (#1689). isort is dangerous with Scanpy and requires good testing and many exceptions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:135,safety,modul,module,135,"> I had also thought isort could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (imperative programming strikes again!). Yeah, I ran into this stuff when creating the flake8 PR (#1689). isort is dangerous with Scanpy and requires good testing and many exceptions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:143,safety,error,errors,143,"> I had also thought isort could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (imperative programming strikes again!). Yeah, I ran into this stuff when creating the flake8 PR (#1689). isort is dangerous with Scanpy and requires good testing and many exceptions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:305,safety,test,testing,305,"> I had also thought isort could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (imperative programming strikes again!). Yeah, I ran into this stuff when creating the flake8 PR (#1689). isort is dangerous with Scanpy and requires good testing and many exceptions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:322,safety,except,exceptions,322,"> I had also thought isort could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (imperative programming strikes again!). Yeah, I ran into this stuff when creating the flake8 PR (#1689). isort is dangerous with Scanpy and requires good testing and many exceptions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:21,security,iso,isort,21,"> I had also thought isort could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (imperative programming strikes again!). Yeah, I ran into this stuff when creating the flake8 PR (#1689). isort is dangerous with Scanpy and requires good testing and many exceptions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:256,security,iso,isort,256,"> I had also thought isort could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (imperative programming strikes again!). Yeah, I ran into this stuff when creating the flake8 PR (#1689). isort is dangerous with Scanpy and requires good testing and many exceptions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:305,testability,test,testing,305,"> I had also thought isort could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (imperative programming strikes again!). Yeah, I ran into this stuff when creating the flake8 PR (#1689). isort is dangerous with Scanpy and requires good testing and many exceptions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:143,usability,error,errors,143,"> I had also thought isort could be a good starting place, but it might actually be some work to turn on due to ""partially initialized module"" errors (imperative programming strikes again!). Yeah, I ran into this stuff when creating the flake8 PR (#1689). isort is dangerous with Scanpy and requires good testing and many exceptions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:212,deployability,automat,automatically,212,"@Koncopd, I would agree, but there have a been a few bugs recently that flake8 definitely would have caught. This is stuff like ""parameters are never used"". Rules that I think are useful:. * Catch bugs. * Can be automatically applied/ prevent ""remove whitespace/ formatting"" commits. If you run into any rules you don't like, you can just add them to the ignore and we can deal with it during review. Does that sound reasonable?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:263,interoperability,format,formatting,263,"@Koncopd, I would agree, but there have a been a few bugs recently that flake8 definitely would have caught. This is stuff like ""parameters are never used"". Rules that I think are useful:. * Catch bugs. * Can be automatically applied/ prevent ""remove whitespace/ formatting"" commits. If you run into any rules you don't like, you can just add them to the ignore and we can deal with it during review. Does that sound reasonable?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:129,modifiability,paramet,parameters,129,"@Koncopd, I would agree, but there have a been a few bugs recently that flake8 definitely would have caught. This is stuff like ""parameters are never used"". Rules that I think are useful:. * Catch bugs. * Can be automatically applied/ prevent ""remove whitespace/ formatting"" commits. If you run into any rules you don't like, you can just add them to the ignore and we can deal with it during review. Does that sound reasonable?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:401,reliability,Doe,Does,401,"@Koncopd, I would agree, but there have a been a few bugs recently that flake8 definitely would have caught. This is stuff like ""parameters are never used"". Rules that I think are useful:. * Catch bugs. * Can be automatically applied/ prevent ""remove whitespace/ formatting"" commits. If you run into any rules you don't like, you can just add them to the ignore and we can deal with it during review. Does that sound reasonable?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:235,safety,prevent,prevent,235,"@Koncopd, I would agree, but there have a been a few bugs recently that flake8 definitely would have caught. This is stuff like ""parameters are never used"". Rules that I think are useful:. * Catch bugs. * Can be automatically applied/ prevent ""remove whitespace/ formatting"" commits. If you run into any rules you don't like, you can just add them to the ignore and we can deal with it during review. Does that sound reasonable?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:393,safety,review,review,393,"@Koncopd, I would agree, but there have a been a few bugs recently that flake8 definitely would have caught. This is stuff like ""parameters are never used"". Rules that I think are useful:. * Catch bugs. * Can be automatically applied/ prevent ""remove whitespace/ formatting"" commits. If you run into any rules you don't like, you can just add them to the ignore and we can deal with it during review. Does that sound reasonable?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:235,security,preven,prevent,235,"@Koncopd, I would agree, but there have a been a few bugs recently that flake8 definitely would have caught. This is stuff like ""parameters are never used"". Rules that I think are useful:. * Catch bugs. * Can be automatically applied/ prevent ""remove whitespace/ formatting"" commits. If you run into any rules you don't like, you can just add them to the ignore and we can deal with it during review. Does that sound reasonable?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:212,testability,automat,automatically,212,"@Koncopd, I would agree, but there have a been a few bugs recently that flake8 definitely would have caught. This is stuff like ""parameters are never used"". Rules that I think are useful:. * Catch bugs. * Can be automatically applied/ prevent ""remove whitespace/ formatting"" commits. If you run into any rules you don't like, you can just add them to the ignore and we can deal with it during review. Does that sound reasonable?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:393,testability,review,review,393,"@Koncopd, I would agree, but there have a been a few bugs recently that flake8 definitely would have caught. This is stuff like ""parameters are never used"". Rules that I think are useful:. * Catch bugs. * Can be automatically applied/ prevent ""remove whitespace/ formatting"" commits. If you run into any rules you don't like, you can just add them to the ignore and we can deal with it during review. Does that sound reasonable?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:51,deployability,modul,modular,51,"Flake8 can indeed be buggy, but the rules are very modular and can be turned off. Even if we end up with only “unused import” and “unused parameter” being left, it’s a win.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:51,integrability,modular,modular,51,"Flake8 can indeed be buggy, but the rules are very modular and can be turned off. Even if we end up with only “unused import” and “unused parameter” being left, it’s a win.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:51,modifiability,modul,modular,51,"Flake8 can indeed be buggy, but the rules are very modular and can be turned off. Even if we end up with only “unused import” and “unused parameter” being left, it’s a win.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:138,modifiability,paramet,parameter,138,"Flake8 can indeed be buggy, but the rules are very modular and can be turned off. Even if we end up with only “unused import” and “unused parameter” being left, it’s a win.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:51,safety,modul,modular,51,"Flake8 can indeed be buggy, but the rules are very modular and can be turned off. Even if we end up with only “unused import” and “unused parameter” being left, it’s a win.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:51,testability,modula,modular,51,"Flake8 can indeed be buggy, but the rules are very modular and can be turned off. Even if we end up with only “unused import” and “unused parameter” being left, it’s a win.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:219,availability,sli,slip,219,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:260,deployability,fail,fail,260,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:80,integrability,queue,queued,80,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:297,integrability,configur,configure,297,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:297,modifiability,configur,configure,297,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:80,performance,queue,queued,80,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:106,reliability,doe,doesn,106,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:188,reliability,doe,doesn,188,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:219,reliability,sli,slip,219,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:260,reliability,fail,fail,260,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:297,security,configur,configure,297,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:39,usability,workflow,workflow,39,"One thing I noticed: The pre-commit GH workflow for a commit I made seems to be queued for quite a while, doesn’t appear in the list of checks, and the PR thinks all checks have run. This doesn’t seem ideal, as PRs can slip in where it will only run (and then fail) after the PR is merged. Can we configure it to be mandatory? Will that change anything?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:19,availability,down,down,19,Github actions are down. It seems like they have problems at least once a week: https://twitter.com/githubstatus. I'd be fine with having pre-commit be a required check. I'm a little antsy about having something that goes down frequently be required though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:222,availability,down,down,222,Github actions are down. It seems like they have problems at least once a week: https://twitter.com/githubstatus. I'd be fine with having pre-commit be a required check. I'm a little antsy about having something that goes down frequently be required though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:365,availability,down,down,365,"I'd be up for all of the numbered ones. IIRC, I had some issues with the trailing whitespace/ end of file fixers and some binary files/ csvs in the test suite. I'm a bit worried about false positives with `check-large-files`, but so long as it's easy to allow certain things (e.g. intentionally added test data) it should be fine. In terms of breaking these things down into small tasks/ PRs how about: (1), (2, 3), (4, 5)? `prettier` looks a bit heavy and like it's targeting a lot of stuff we don't use, so you'd have to make a good case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:148,safety,test,test,148,"I'd be up for all of the numbered ones. IIRC, I had some issues with the trailing whitespace/ end of file fixers and some binary files/ csvs in the test suite. I'm a bit worried about false positives with `check-large-files`, but so long as it's easy to allow certain things (e.g. intentionally added test data) it should be fine. In terms of breaking these things down into small tasks/ PRs how about: (1), (2, 3), (4, 5)? `prettier` looks a bit heavy and like it's targeting a lot of stuff we don't use, so you'd have to make a good case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:301,safety,test,test,301,"I'd be up for all of the numbered ones. IIRC, I had some issues with the trailing whitespace/ end of file fixers and some binary files/ csvs in the test suite. I'm a bit worried about false positives with `check-large-files`, but so long as it's easy to allow certain things (e.g. intentionally added test data) it should be fine. In terms of breaking these things down into small tasks/ PRs how about: (1), (2, 3), (4, 5)? `prettier` looks a bit heavy and like it's targeting a lot of stuff we don't use, so you'd have to make a good case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:148,testability,test,test,148,"I'd be up for all of the numbered ones. IIRC, I had some issues with the trailing whitespace/ end of file fixers and some binary files/ csvs in the test suite. I'm a bit worried about false positives with `check-large-files`, but so long as it's easy to allow certain things (e.g. intentionally added test data) it should be fine. In terms of breaking these things down into small tasks/ PRs how about: (1), (2, 3), (4, 5)? `prettier` looks a bit heavy and like it's targeting a lot of stuff we don't use, so you'd have to make a good case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:301,testability,test,test,301,"I'd be up for all of the numbered ones. IIRC, I had some issues with the trailing whitespace/ end of file fixers and some binary files/ csvs in the test suite. I'm a bit worried about false positives with `check-large-files`, but so long as it's easy to allow certain things (e.g. intentionally added test data) it should be fine. In terms of breaking these things down into small tasks/ PRs how about: (1), (2, 3), (4, 5)? `prettier` looks a bit heavy and like it's targeting a lot of stuff we don't use, so you'd have to make a good case.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:42,usability,close,closed,42,"I think we've done enough to mark this as closed. For any additional checks, please open a new issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:108,energy efficiency,cool,cool,108,does anybody knows of a pre-commit check that checks for spelling in docstrings? I think that would be very cool to have.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/issues/1563:0,reliability,doe,does,0,does anybody knows of a pre-commit check that checks for spelling in docstrings? I think that would be very cool to have.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1563
https://github.com/scverse/scanpy/pull/1564:77,safety,test,test,77,"I think I'll leave codecov to a separate PR, since this immediately improves test reporting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/pull/1564:77,testability,test,test,77,"I think I'll leave codecov to a separate PR, since this immediately improves test reporting.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1564
https://github.com/scverse/scanpy/issues/1566:55,deployability,instal,installed,55,"Hi @Pawan291, It seems `louvain` has not been properly installed in your environment. Could you post the output of `scanpy.logging.print_versions()` as suggested in the template? You should just need to `pip install louvain`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:123,deployability,log,logging,123,"Hi @Pawan291, It seems `louvain` has not been properly installed in your environment. Could you post the output of `scanpy.logging.print_versions()` as suggested in the template? You should just need to `pip install louvain`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:208,deployability,instal,install,208,"Hi @Pawan291, It seems `louvain` has not been properly installed in your environment. Could you post the output of `scanpy.logging.print_versions()` as suggested in the template? You should just need to `pip install louvain`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:123,safety,log,logging,123,"Hi @Pawan291, It seems `louvain` has not been properly installed in your environment. Could you post the output of `scanpy.logging.print_versions()` as suggested in the template? You should just need to `pip install louvain`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:123,security,log,logging,123,"Hi @Pawan291, It seems `louvain` has not been properly installed in your environment. Could you post the output of `scanpy.logging.print_versions()` as suggested in the template? You should just need to `pip install louvain`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:123,testability,log,logging,123,"Hi @Pawan291, It seems `louvain` has not been properly installed in your environment. Could you post the output of `scanpy.logging.print_versions()` as suggested in the template? You should just need to `pip install louvain`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:2,deployability,instal,installed,2,I installed again louvain with this command (although i already tried this command 3 times) and it work for me now. Thank you very much for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:85,performance,time,times,85,I installed again louvain with this command (although i already tried this command 3 times) and it work for me now. Thank you very much for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:36,usability,command,command,36,I installed again louvain with this command (although i already tried this command 3 times) and it work for me now. Thank you very much for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:75,usability,command,command,75,I installed again louvain with this command (although i already tried this command 3 times) and it work for me now. Thank you very much for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1566:144,usability,help,help,144,I installed again louvain with this command (although i already tried this command 3 times) and it work for me now. Thank you very much for the help.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1566
https://github.com/scverse/scanpy/issues/1567:226,availability,failur,failure,226,"Does this cause the same issue? ```python. import numpy as np. import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)). ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message? In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:226,deployability,fail,failure,226,"Does this cause the same issue? ```python. import numpy as np. import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)). ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message? In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:182,integrability,messag,message,182,"Does this cause the same issue? ```python. import numpy as np. import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)). ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message? In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:234,integrability,messag,message,234,"Does this cause the same issue? ```python. import numpy as np. import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)). ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message? In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:346,integrability,repositor,repositories,346,"Does this cause the same issue? ```python. import numpy as np. import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)). ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message? In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:182,interoperability,messag,message,182,"Does this cause the same issue? ```python. import numpy as np. import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)). ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message? In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:234,interoperability,messag,message,234,"Does this cause the same issue? ```python. import numpy as np. import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)). ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message? In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:346,interoperability,repositor,repositories,346,"Does this cause the same issue? ```python. import numpy as np. import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)). ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message? In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:226,performance,failur,failure,226,"Does this cause the same issue? ```python. import numpy as np. import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)). ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message? In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:0,reliability,Doe,Does,0,"Does this cause the same issue? ```python. import numpy as np. import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)). ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message? In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:226,reliability,fail,failure,226,"Does this cause the same issue? ```python. import numpy as np. import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)). ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message? In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:328,usability,learn,learn,328,"Does this cause the same issue? ```python. import numpy as np. import umap. umap.UMAP().fit_transform(np.random.randn(10_000, 20)). ```. And when you say ""dies"", is there a segfault message, or are you seeing a jupyter kernel failure message? In general, this sounds like a numba issue. I'd recommend taking searching the `umap-learn` or `numba` repositories for similar issues.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:138,availability,failur,failure,138,It causes exactly the same issue when I run:. ```. import numpy as np. import umap. ```. There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:138,deployability,fail,failure,138,It causes exactly the same issue when I run:. ```. import numpy as np. import umap. ```. There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:229,deployability,automat,automatically,229,It causes exactly the same issue when I run:. ```. import numpy as np. import umap. ```. There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:110,integrability,messag,message,110,It causes exactly the same issue when I run:. ```. import numpy as np. import umap. ```. There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:146,integrability,messag,message,146,It causes exactly the same issue when I run:. ```. import numpy as np. import umap. ```. There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:110,interoperability,messag,message,110,It causes exactly the same issue when I run:. ```. import numpy as np. import umap. ```. There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:146,interoperability,messag,message,146,It causes exactly the same issue when I run:. ```. import numpy as np. import umap. ```. There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:138,performance,failur,failure,138,It causes exactly the same issue when I run:. ```. import numpy as np. import umap. ```. There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:138,reliability,fail,failure,138,It causes exactly the same issue when I run:. ```. import numpy as np. import umap. ```. There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:229,testability,automat,automatically,229,It causes exactly the same issue when I run:. ```. import numpy as np. import umap. ```. There is no segfault message. The jupyter kernel failure message is: . _Kernel restarting. The kernel appears to have died. It will restart automatically._,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:158,availability,error,error,158,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:383,availability,fault,fault,383,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:22,deployability,version,versions,22,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:270,deployability,fail,fails,270,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:476,deployability,version,versions,476,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:383,energy efficiency,fault,fault,383,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:22,integrability,version,versions,22,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:476,integrability,version,versions,476,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:22,modifiability,version,versions,22,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:476,modifiability,version,versions,476,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:158,performance,error,error,158,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:383,performance,fault,fault,383,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:432,performance,time,times,432,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:270,reliability,fail,fails,270,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:383,reliability,fault,fault,383,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:158,safety,error,error,158,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:383,safety,fault,fault,383,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:158,usability,error,error,158,"First, make sure your versions of numba and umap are up to date. Then running this without jupyter, just in python. I think jupyter is likely hiding whatever error the python process is crashing with. E.g.:. ```sh. $ python3 -c ""import numpy; import umap"". ```. If this fails, I think you should be opening an issue upstream with numba or umap (if you can figure out which one is at fault). I've had this sort of thing happen a few times. Some of the causes were:. * Multiple versions of C libraries being linked to. * Threading backends not working. I've been able to work around both of these in the past by using conda environments.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:30,deployability,log,logs,30,sometimes the jupyter console logs give you an idea of what is happening as well. I've had this happen when you run out of memory.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:123,performance,memor,memory,123,sometimes the jupyter console logs give you an idea of what is happening as well. I've had this happen when you run out of memory.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:30,safety,log,logs,30,sometimes the jupyter console logs give you an idea of what is happening as well. I've had this happen when you run out of memory.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:30,security,log,logs,30,sometimes the jupyter console logs give you an idea of what is happening as well. I've had this happen when you run out of memory.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:30,testability,log,logs,30,sometimes the jupyter console logs give you an idea of what is happening as well. I've had this happen when you run out of memory.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:123,usability,memor,memory,123,sometimes the jupyter console logs give you an idea of what is happening as well. I've had this happen when you run out of memory.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:80,deployability,version,versions,80,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:167,deployability,upgrad,upgrade,167,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:257,deployability,instal,install,257,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:267,deployability,upgrad,upgrade,267,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:286,deployability,instal,install,286,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:296,deployability,upgrad,upgrade,296,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:384,deployability,instal,installation,384,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:418,deployability,instal,install,418,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:481,deployability,instal,install,481,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:533,deployability,instal,install,533,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:585,deployability,version,version,585,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:658,deployability,instal,install,658,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:764,deployability,instal,installations,764,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:80,integrability,version,versions,80,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:585,integrability,version,version,585,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:611,interoperability,incompatib,incompatible,611,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:80,modifiability,version,versions,80,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:167,modifiability,upgrad,upgrade,167,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:267,modifiability,upgrad,upgrade,267,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:296,modifiability,upgrad,upgrade,296,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:585,modifiability,version,version,585,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:309,usability,learn,learn,309,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:441,usability,learn,learn,441,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1022,usability,help,help,1022,"I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. ```bash. pip install --upgrade numba. pip install --upgrade umap-learn. ```. Then I essentially reinstalled scanpy using the steps in their installation docs. ```bash. conda install seaborn scikit-learn statsmodels numba pytables. conda install -c conda-forge python-igraph leidenalg. pip install scanpy. ```. I think I then ended up with a version of numpy that was incompatible with numba so I ran. ```bash. pip install numpy==1.20. ```. After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. ```bash. python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". ```. This seemed to fix my problems; I hope it's able to help others!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:175,availability,state,state,175,"I would recommend creating fresh conda environments frequently, especially if you mix `conda` and `pip` installation. Upgrading them in finicky, and often results in a broken state.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:104,deployability,instal,installation,104,"I would recommend creating fresh conda environments frequently, especially if you mix `conda` and `pip` installation. Upgrading them in finicky, and often results in a broken state.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:118,deployability,Upgrad,Upgrading,118,"I would recommend creating fresh conda environments frequently, especially if you mix `conda` and `pip` installation. Upgrading them in finicky, and often results in a broken state.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:175,integrability,state,state,175,"I would recommend creating fresh conda environments frequently, especially if you mix `conda` and `pip` installation. Upgrading them in finicky, and often results in a broken state.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:118,modifiability,Upgrad,Upgrading,118,"I would recommend creating fresh conda environments frequently, especially if you mix `conda` and `pip` installation. Upgrading them in finicky, and often results in a broken state.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1040,availability,error,error,1040,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:347,deployability,updat,updated,347,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:895,deployability,scale,scale,895,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:895,energy efficiency,scale,scale,895,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:357,modifiability,pac,package,357,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:757,modifiability,layer,layers,757,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:895,modifiability,scal,scale,895,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:895,performance,scale,scale,895,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1040,performance,error,error,1040,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:157,reliability,doe,does,157,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:347,safety,updat,updated,347,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1040,safety,error,error,1040,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:347,security,updat,updated,347,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:410,security,ident,identify,410,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:558,usability,learn,learn,558,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1040,usability,error,error,1040,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1195,usability,help,help,1195,"I am running into the same issue and unfortunately running the steps as described here https://github.com/theislab/scanpy/issues/1567#issuecomment-968181500 does not solve my problem. My kernel systematically dies when I run `sc.pp.neighbors` (even with only 1,000 cells). What I am also confused about is that this used to work - I am guessing I updated a package somewhere that broke everything but I cannot identify what. This is my config:. - MacBook Pro (13-inch, M1, 2020) - macOS Big Sur 11.5.2. - python 3.8.8. - numpy 1.20.0. - numba 0.51.2. - umap-learn 0.5.2. I have tried running the following code in Jupyter and then in a script to see if I could get more info on the bug:. ```. unhealthy_cells = sc.read_h5ad(""path/to/file""). unhealthy_cells.layers[""counts""] = unhealthy_cells.X.copy(). sc.pp.normalize_total(unhealthy_cells,target_sum=10000). sc.pp.log1p(unhealthy_cells). sc.pp.scale(unhealthy_cells). sc.tl.pca(unhealthy_cells). sc.pp.neighbors(unhealthy_cells). ```. When I run it as a python script, I get the following error when getting to `sc.pp.neighbors` (everything else works): . `zsh: illegal hardware instruction`. Is there anything I could do? . Thank you for your help!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:33,deployability,updat,updating,33,"I have the same issue even after updating numba, umap and scanpy :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:33,safety,updat,updating,33,"I have the same issue even after updating numba, umap and scanpy :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:33,security,updat,updating,33,"I have the same issue even after updating numba, umap and scanpy :(",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1164,availability,error,errors,1164,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1376,availability,fault,fault,1376,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:82,deployability,version,versions,82,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:175,deployability,upgrad,upgrade,175,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:274,deployability,instal,install,274,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:284,deployability,upgrad,upgrade,284,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:305,deployability,instal,install,305,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:315,deployability,upgrad,upgrade,315,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:411,deployability,instal,installation,411,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:454,deployability,instal,install,454,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:519,deployability,instal,install,519,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:573,deployability,instal,install,573,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:633,deployability,version,version,633,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:715,deployability,instal,install,715,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:829,deployability,instal,installations,829,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1376,energy efficiency,fault,fault,1376,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:82,integrability,version,versions,82,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:633,integrability,version,version,633,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1235,integrability,messag,messagestream,1235,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1249,integrability,Messag,MessageStream,1249,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:659,interoperability,incompatib,incompatible,659,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1235,interoperability,messag,messagestream,1235,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1249,interoperability,Messag,MessageStream,1249,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1297,interoperability,incompatib,incompatibility,1297,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:82,modifiability,version,versions,82,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:175,modifiability,upgrad,upgrade,175,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:284,modifiability,upgrad,upgrade,284,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:315,modifiability,upgrad,upgrade,315,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:633,modifiability,version,version,633,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1164,performance,error,errors,1164,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1376,performance,fault,fault,1376,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1376,reliability,fault,fault,1376,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1164,safety,error,errors,1164,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1376,safety,fault,fault,1376,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:328,usability,learn,learn,328,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:477,usability,learn,learn,477,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1104,usability,help,help,1104,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1164,usability,error,errors,1164,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:1281,usability,indicat,indicate,1281,"> I didn't keep perfect track of the steps that I took to solve this or the exact versions of everything that I used but I'll try outlining what I did. > . > First I tried to upgrade numba and umap as suggested by the other individuals in the thread:. > . > ```shell. > pip install --upgrade numba. > pip install --upgrade umap-learn. > ```. > . > Then I essentially reinstalled scanpy using the steps in their installation docs. > . > ```shell. > conda install seaborn scikit-learn statsmodels numba pytables. > conda install -c conda-forge python-igraph leidenalg. > pip install scanpy. > ```. > . > I think I then ended up with a version of numpy that was incompatible with numba so I ran. > . > ```shell. > pip install numpy==1.20. > ```. > . > After each step, you should be able to run the code from above to check if your installations worked, which I used to pinpoint what still needed work in my environment:. > . > ```shell. > python3 -c ""import numpy as np; import umap; umap.UMAP().fit_transform(np.random.randn(10_000, 20))"". > ```. > . > This seemed to fix my problems; I hope it's able to help others! I followed your instruction but it still threw errors:. <frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject. Segmentation fault",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:177,availability,state,state,177,"> I would recommend creating fresh conda environments frequently, especially if you mix `conda` and `pip` installation. Upgrading them in finicky, and often results in a broken state. Creating a new environment and reinstalling everything worked for me. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:106,deployability,instal,installation,106,"> I would recommend creating fresh conda environments frequently, especially if you mix `conda` and `pip` installation. Upgrading them in finicky, and often results in a broken state. Creating a new environment and reinstalling everything worked for me. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:120,deployability,Upgrad,Upgrading,120,"> I would recommend creating fresh conda environments frequently, especially if you mix `conda` and `pip` installation. Upgrading them in finicky, and often results in a broken state. Creating a new environment and reinstalling everything worked for me. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:177,integrability,state,state,177,"> I would recommend creating fresh conda environments frequently, especially if you mix `conda` and `pip` installation. Upgrading them in finicky, and often results in a broken state. Creating a new environment and reinstalling everything worked for me. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:120,modifiability,Upgrad,Upgrading,120,"> I would recommend creating fresh conda environments frequently, especially if you mix `conda` and `pip` installation. Upgrading them in finicky, and often results in a broken state. Creating a new environment and reinstalling everything worked for me. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:5,deployability,instal,install,5,`pip install -U pynndescent`. This fix my problems; I hope it's able to help others!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:72,usability,help,help,72,`pip install -U pynndescent`. This fix my problems; I hope it's able to help others!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:6,deployability,instal,install,6,conda install -c conda-forge pynndescent. This also fixed my problem. Thanks @FlyPythons for the hint.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:97,usability,hint,hint,97,conda install -c conda-forge pynndescent. This also fixed my problem. Thanks @FlyPythons for the hint.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:8,deployability,instal,install,8,> conda install -c conda-forge pynndescent. > . > This also fixed my problem. Thanks @FlyPythons for the hint. Thanks!This also work for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1567:105,usability,hint,hint,105,> conda install -c conda-forge pynndescent. > . > This also fixed my problem. Thanks @FlyPythons for the hint. Thanks!This also work for me.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1567
https://github.com/scverse/scanpy/issues/1568:173,usability,user,user-images,173,"Soon, you can do:. ```python. import scanpy as sc. pbmc = sc.datasets.pbmc68k_reduced(). sc.pl.umap(pbmc, na_color=""purple""). ```. <img width=""752"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/103634345-0bb70100-4f9b-11eb-9557-44a94d8fc7cd.png"">. Is this what you're looking for?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1568
https://github.com/scverse/scanpy/pull/1569:69,deployability,automat,automation,69,Thank you! [We could try blurb](https://pypi.org/project/blurb/) for automation (if it’s not to CPython-specific),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1569
https://github.com/scverse/scanpy/pull/1569:104,interoperability,specif,specific,104,Thank you! [We could try blurb](https://pypi.org/project/blurb/) for automation (if it’s not to CPython-specific),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1569
https://github.com/scverse/scanpy/pull/1569:69,testability,automat,automation,69,Thank you! [We could try blurb](https://pypi.org/project/blurb/) for automation (if it’s not to CPython-specific),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1569
https://github.com/scverse/scanpy/pull/1571:176,deployability,integr,integration,176,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy? In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:15,energy efficiency,cool,cool,15,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy? In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:58,energy efficiency,core,core,58,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy? In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:176,integrability,integr,integration,176,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy? In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:176,interoperability,integr,integration,176,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy? In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:176,modifiability,integr,integration,176,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy? In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:176,reliability,integr,integration,176,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy? In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:176,security,integr,integration,176,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy? In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:176,testability,integr,integration,176,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy? In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:146,usability,tool,tools,146,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy? In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:229,usability,tool,tools,229,"This is really cool! How expansive should this be? Scanpy core + scvelo? Or also other scanpy-based things like single-cell-tutorial, scGen, scvi-tools or diffxpy? In our data integration benchmarking we find that 3 of the top 4 tools are in the scanpy ecosystem now: scanorama, scGen, and scANVI.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:28,modifiability,extens,extensive,28,Happy to have this be quite extensive! Not happy with the font styling. We should make this a separate page and just show the top of the thread on the landing page.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:210,deployability,integr,integration,210,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:210,integrability,integr,integration,210,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:210,interoperability,integr,integration,210,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:210,modifiability,integr,integration,210,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:194,performance,perform,performing,194,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:210,reliability,integr,integration,210,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:210,security,integr,integration,210,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:210,testability,integr,integration,210,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/pull/1571:194,usability,perform,performing,194,"Will do once there are things that are big enough... you set the bar quite high with these headlines ;). Maybe things like single-cell-tutorial as F1000 recommended paper, or the news about top performing data integration methods, once the paper is out.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1571
https://github.com/scverse/scanpy/issues/1573:542,availability,cluster,clusters,542,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:148,deployability,integr,integrate,148,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:352,deployability,stack,stacked,352,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:542,deployability,cluster,clusters,542,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:148,integrability,integr,integrate,148,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:148,interoperability,integr,integrate,148,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:248,interoperability,standard,standards,248,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:278,interoperability,share,share,278,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:148,modifiability,integr,integrate,148,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:140,performance,time,time,140,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:148,reliability,integr,integrate,148,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:218,safety,test,tests,218,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:148,security,integr,integrate,148,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:148,testability,integr,integrate,148,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:218,testability,test,tests,218,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:225,usability,document,documentation,225,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:323,usability,help,help,323,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:640,usability,user,user-images,640,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:758,usability,user,user-images,758,"I have several plotting functions that allow to compare any two categorical columns in `.obs` to achieve similar output but never found the time to integrate them into scanpy. Is really quite some effort to add proper tests, documentation and code standards. I will be happy to share the code if other people is willing to help. . One problem with the stacked bar plot is that with lot of samples it is difficult to compare the fractions. To solve this I had used the dot plot with good results, see for example a comparison of the `louvain` clusters and the `bulk labels` annotation from `sc.datasets.pbmc68k_reduced()`:. ![image](https://user-images.githubusercontent.com/4964309/104466204-3e718280-55b5-11eb-9b87-ac3860af7979.png). and . ![image](https://user-images.githubusercontent.com/4964309/104466234-49c4ae00-55b5-11eb-92c8-45140de9e107.png). The dot plot also computes enrichment with respect to random expectations and sorts the rows and columns.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:53,usability,close,close,53,"As we haven't heard back after the follow-up we will close the issue for now, hopefully you obtained the expected behaviour in the end :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1573:114,usability,behavi,behaviour,114,"As we haven't heard back after the follow-up we will close the issue for now, hopefully you obtained the expected behaviour in the end :). However, please don't hesitate to reopen this issue or create a new one if you have any more questions or run into any related problems in the future. Thanks for being a part of our community! :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1573
https://github.com/scverse/scanpy/issues/1574:52,deployability,updat,updated,52,"Hi @vitkl ! that's a great idea yes, should be kept updated with most recent methods. . Unfortuantely I don't have capacity now, do you mind opening an issue in https://github.com/theislab/scanpy-tutorials to keep as reminder? I will close this but of of course you could reference this in the tutorial repo. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:52,safety,updat,updated,52,"Hi @vitkl ! that's a great idea yes, should be kept updated with most recent methods. . Unfortuantely I don't have capacity now, do you mind opening an issue in https://github.com/theislab/scanpy-tutorials to keep as reminder? I will close this but of of course you could reference this in the tutorial repo. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:52,security,updat,updated,52,"Hi @vitkl ! that's a great idea yes, should be kept updated with most recent methods. . Unfortuantely I don't have capacity now, do you mind opening an issue in https://github.com/theislab/scanpy-tutorials to keep as reminder? I will close this but of of course you could reference this in the tutorial repo. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1574:234,usability,close,close,234,"Hi @vitkl ! that's a great idea yes, should be kept updated with most recent methods. . Unfortuantely I don't have capacity now, do you mind opening an issue in https://github.com/theislab/scanpy-tutorials to keep as reminder? I will close this but of of course you could reference this in the tutorial repo. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1574
https://github.com/scverse/scanpy/issues/1576:23,availability,avail,available,23,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:376,availability,avail,available,376,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:102,deployability,pipelin,pipelines,102,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:281,deployability,Pipelin,Pipelines,281,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:366,energy efficiency,current,currently,366,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:409,energy efficiency,core,core,409,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:102,integrability,pipelin,pipelines,102,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:210,integrability,pub,publish,210,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:281,integrability,Pipelin,Pipelines,281,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:469,interoperability,format,format,469,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:545,interoperability,format,formats,545,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:482,modifiability,extens,extension,482,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:23,reliability,availab,available,23,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:376,reliability,availab,available,376,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:23,safety,avail,available,23,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:112,safety,test,test,112,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:376,safety,avail,available,376,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:23,security,availab,available,23,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:376,security,availab,available,376,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:112,testability,test,test,112,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:223,testability,coverag,coverage,223,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:301,testability,coverag,coverage,301,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:452,testability,coverag,coverage,452,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:493,testability,coverag,coverage,493,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:536,testability,coverag,coverage,536,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:354,usability,document,document,354,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:433,usability,Visual,Visual,433,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/issues/1576:504,usability,Support,Support,504,"Looks like this is not available for python yet ([docs](https://docs.microsoft.com/en-us/azure/devops/pipelines/test/codecoverage-for-pullrequests?view=azure-devops#prerequisites)). > While you can collect and publish code coverage results for many different languages using Azure Pipelines, the code coverage for pull requests feature discussed in this document is currently available only for .NET and .NET core projects using the Visual Studio code coverage results format (file extension .coverage). Support for other languages and coverage formats will be added in future milestones.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1576
https://github.com/scverse/scanpy/pull/1577:166,energy efficiency,model,model,166,"Sorry for taking so long to get back on this to you. At the moment, we're not really comfortable with promoting a paid tool on the ecosystem page. Maybe we'll have a model for this at sometime in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:166,security,model,model,166,"Sorry for taking so long to get back on this to you. At the moment, we're not really comfortable with promoting a paid tool on the ecosystem page. Maybe we'll have a model for this at sometime in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:119,usability,tool,tool,119,"Sorry for taking so long to get back on this to you. At the moment, we're not really comfortable with promoting a paid tool on the ecosystem page. Maybe we'll have a model for this at sometime in the future.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:93,energy efficiency,model,model,93,"Isaac,. No need to apologize - thank you for considering it! If you decide to put together a model for this or are open to . sponsorship in the future, please do reach out. Best,. -Nigel Michki. On 3/3/21 06:16, Isaac Virshup wrote:. >. > Sorry for taking so long to get back on this to you. >. > At the moment, we're not really comfortable with promoting a paid tool . > on the ecosystem page. Maybe we'll have a model for this at sometime . > in the future. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub . > <https://github.com/theislab/scanpy/pull/1577#issuecomment-789640540>, . > or unsubscribe . > <https://github.com/notifications/unsubscribe-auth/ABGOKBQI2ZR3NNIAPTVEAS3TBYLCXANCNFSM4V6QOATQ>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:414,energy efficiency,model,model,414,"Isaac,. No need to apologize - thank you for considering it! If you decide to put together a model for this or are open to . sponsorship in the future, please do reach out. Best,. -Nigel Michki. On 3/3/21 06:16, Isaac Virshup wrote:. >. > Sorry for taking so long to get back on this to you. >. > At the moment, we're not really comfortable with promoting a paid tool . > on the ecosystem page. Maybe we'll have a model for this at sometime . > in the future. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub . > <https://github.com/theislab/scanpy/pull/1577#issuecomment-789640540>, . > or unsubscribe . > <https://github.com/notifications/unsubscribe-auth/ABGOKBQI2ZR3NNIAPTVEAS3TBYLCXANCNFSM4V6QOATQ>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:93,security,model,model,93,"Isaac,. No need to apologize - thank you for considering it! If you decide to put together a model for this or are open to . sponsorship in the future, please do reach out. Best,. -Nigel Michki. On 3/3/21 06:16, Isaac Virshup wrote:. >. > Sorry for taking so long to get back on this to you. >. > At the moment, we're not really comfortable with promoting a paid tool . > on the ecosystem page. Maybe we'll have a model for this at sometime . > in the future. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub . > <https://github.com/theislab/scanpy/pull/1577#issuecomment-789640540>, . > or unsubscribe . > <https://github.com/notifications/unsubscribe-auth/ABGOKBQI2ZR3NNIAPTVEAS3TBYLCXANCNFSM4V6QOATQ>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:414,security,model,model,414,"Isaac,. No need to apologize - thank you for considering it! If you decide to put together a model for this or are open to . sponsorship in the future, please do reach out. Best,. -Nigel Michki. On 3/3/21 06:16, Isaac Virshup wrote:. >. > Sorry for taking so long to get back on this to you. >. > At the moment, we're not really comfortable with promoting a paid tool . > on the ecosystem page. Maybe we'll have a model for this at sometime . > in the future. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub . > <https://github.com/theislab/scanpy/pull/1577#issuecomment-789640540>, . > or unsubscribe . > <https://github.com/notifications/unsubscribe-auth/ABGOKBQI2ZR3NNIAPTVEAS3TBYLCXANCNFSM4V6QOATQ>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:505,security,auth,authored,505,"Isaac,. No need to apologize - thank you for considering it! If you decide to put together a model for this or are open to . sponsorship in the future, please do reach out. Best,. -Nigel Michki. On 3/3/21 06:16, Isaac Virshup wrote:. >. > Sorry for taking so long to get back on this to you. >. > At the moment, we're not really comfortable with promoting a paid tool . > on the ecosystem page. Maybe we'll have a model for this at sometime . > in the future. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub . > <https://github.com/theislab/scanpy/pull/1577#issuecomment-789640540>, . > or unsubscribe . > <https://github.com/notifications/unsubscribe-auth/ABGOKBQI2ZR3NNIAPTVEAS3TBYLCXANCNFSM4V6QOATQ>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:720,security,auth,auth,720,"Isaac,. No need to apologize - thank you for considering it! If you decide to put together a model for this or are open to . sponsorship in the future, please do reach out. Best,. -Nigel Michki. On 3/3/21 06:16, Isaac Virshup wrote:. >. > Sorry for taking so long to get back on this to you. >. > At the moment, we're not really comfortable with promoting a paid tool . > on the ecosystem page. Maybe we'll have a model for this at sometime . > in the future. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub . > <https://github.com/theislab/scanpy/pull/1577#issuecomment-789640540>, . > or unsubscribe . > <https://github.com/notifications/unsubscribe-auth/ABGOKBQI2ZR3NNIAPTVEAS3TBYLCXANCNFSM4V6QOATQ>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/pull/1577:363,usability,tool,tool,363,"Isaac,. No need to apologize - thank you for considering it! If you decide to put together a model for this or are open to . sponsorship in the future, please do reach out. Best,. -Nigel Michki. On 3/3/21 06:16, Isaac Virshup wrote:. >. > Sorry for taking so long to get back on this to you. >. > At the moment, we're not really comfortable with promoting a paid tool . > on the ecosystem page. Maybe we'll have a model for this at sometime . > in the future. >. > —. > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub . > <https://github.com/theislab/scanpy/pull/1577#issuecomment-789640540>, . > or unsubscribe . > <https://github.com/notifications/unsubscribe-auth/ABGOKBQI2ZR3NNIAPTVEAS3TBYLCXANCNFSM4V6QOATQ>. >.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1577
https://github.com/scverse/scanpy/issues/1578:229,deployability,api,api,229,"The `batch-key`option in `scanpy.pp.highly_variable_genes` provides this functionality, and adds `highly_variable_intersection` to `adata.var`. For more information, see the documentation: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html#scanpy.pp.highly_variable_genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:5,integrability,batch,batch-key,5,"The `batch-key`option in `scanpy.pp.highly_variable_genes` provides this functionality, and adds `highly_variable_intersection` to `adata.var`. For more information, see the documentation: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html#scanpy.pp.highly_variable_genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:229,integrability,api,api,229,"The `batch-key`option in `scanpy.pp.highly_variable_genes` provides this functionality, and adds `highly_variable_intersection` to `adata.var`. For more information, see the documentation: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html#scanpy.pp.highly_variable_genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:229,interoperability,api,api,229,"The `batch-key`option in `scanpy.pp.highly_variable_genes` provides this functionality, and adds `highly_variable_intersection` to `adata.var`. For more information, see the documentation: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html#scanpy.pp.highly_variable_genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:5,performance,batch,batch-key,5,"The `batch-key`option in `scanpy.pp.highly_variable_genes` provides this functionality, and adds `highly_variable_intersection` to `adata.var`. For more information, see the documentation: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html#scanpy.pp.highly_variable_genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:174,usability,document,documentation,174,"The `batch-key`option in `scanpy.pp.highly_variable_genes` provides this functionality, and adds `highly_variable_intersection` to `adata.var`. For more information, see the documentation: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html#scanpy.pp.highly_variable_genes.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:31,availability,down,downstream,31,"You don't have to use HVGs for downstream analysis, but it is typically done for two reasons (at least):. 1. Using fewer genes is computationally less expensive for downstream analysis. 2. The signal-to-noise ratio is better with highly variable genes than in the full gene set. The second point is usually particularly important, as even if one single gene doesn't contribute as much to the PCA if it has lower variance, if you have 15000 low-variance genes this does affect the embedding. If you'd like a rationale for why HVGs are used, please see our [best-practices review](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:165,availability,down,downstream,165,"You don't have to use HVGs for downstream analysis, but it is typically done for two reasons (at least):. 1. Using fewer genes is computationally less expensive for downstream analysis. 2. The signal-to-noise ratio is better with highly variable genes than in the full gene set. The second point is usually particularly important, as even if one single gene doesn't contribute as much to the PCA if it has lower variance, if you have 15000 low-variance genes this does affect the embedding. If you'd like a rationale for why HVGs are used, please see our [best-practices review](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:237,modifiability,variab,variable,237,"You don't have to use HVGs for downstream analysis, but it is typically done for two reasons (at least):. 1. Using fewer genes is computationally less expensive for downstream analysis. 2. The signal-to-noise ratio is better with highly variable genes than in the full gene set. The second point is usually particularly important, as even if one single gene doesn't contribute as much to the PCA if it has lower variance, if you have 15000 low-variance genes this does affect the embedding. If you'd like a rationale for why HVGs are used, please see our [best-practices review](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:358,reliability,doe,doesn,358,"You don't have to use HVGs for downstream analysis, but it is typically done for two reasons (at least):. 1. Using fewer genes is computationally less expensive for downstream analysis. 2. The signal-to-noise ratio is better with highly variable genes than in the full gene set. The second point is usually particularly important, as even if one single gene doesn't contribute as much to the PCA if it has lower variance, if you have 15000 low-variance genes this does affect the embedding. If you'd like a rationale for why HVGs are used, please see our [best-practices review](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:464,reliability,doe,does,464,"You don't have to use HVGs for downstream analysis, but it is typically done for two reasons (at least):. 1. Using fewer genes is computationally less expensive for downstream analysis. 2. The signal-to-noise ratio is better with highly variable genes than in the full gene set. The second point is usually particularly important, as even if one single gene doesn't contribute as much to the PCA if it has lower variance, if you have 15000 low-variance genes this does affect the embedding. If you'd like a rationale for why HVGs are used, please see our [best-practices review](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:561,reliability,pra,practices,561,"You don't have to use HVGs for downstream analysis, but it is typically done for two reasons (at least):. 1. Using fewer genes is computationally less expensive for downstream analysis. 2. The signal-to-noise ratio is better with highly variable genes than in the full gene set. The second point is usually particularly important, as even if one single gene doesn't contribute as much to the PCA if it has lower variance, if you have 15000 low-variance genes this does affect the embedding. If you'd like a rationale for why HVGs are used, please see our [best-practices review](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:571,safety,review,review,571,"You don't have to use HVGs for downstream analysis, but it is typically done for two reasons (at least):. 1. Using fewer genes is computationally less expensive for downstream analysis. 2. The signal-to-noise ratio is better with highly variable genes than in the full gene set. The second point is usually particularly important, as even if one single gene doesn't contribute as much to the PCA if it has lower variance, if you have 15000 low-variance genes this does affect the embedding. If you'd like a rationale for why HVGs are used, please see our [best-practices review](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:193,security,sign,signal-to-noise,193,"You don't have to use HVGs for downstream analysis, but it is typically done for two reasons (at least):. 1. Using fewer genes is computationally less expensive for downstream analysis. 2. The signal-to-noise ratio is better with highly variable genes than in the full gene set. The second point is usually particularly important, as even if one single gene doesn't contribute as much to the PCA if it has lower variance, if you have 15000 low-variance genes this does affect the embedding. If you'd like a rationale for why HVGs are used, please see our [best-practices review](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:571,testability,review,review,571,"You don't have to use HVGs for downstream analysis, but it is typically done for two reasons (at least):. 1. Using fewer genes is computationally less expensive for downstream analysis. 2. The signal-to-noise ratio is better with highly variable genes than in the full gene set. The second point is usually particularly important, as even if one single gene doesn't contribute as much to the PCA if it has lower variance, if you have 15000 low-variance genes this does affect the embedding. If you'd like a rationale for why HVGs are used, please see our [best-practices review](http://msb.embopress.org/lookup/doi/10.15252/msb.20188746)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1081,availability,cluster,clustered,1081,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:555,deployability,stage,stage,555,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:585,deployability,log,log,585,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:668,deployability,integr,integration,668,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:715,deployability,integr,integrated,715,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:780,deployability,stage,stage,780,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1081,deployability,cluster,clustered,1081,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1290,deployability,stage,stage,1290,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:668,integrability,integr,integration,668,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:684,integrability,batch,batch,684,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:715,integrability,integr,integrated,715,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:907,integrability,batch,batch,907,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1050,integrability,batch,batch,1050,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1187,integrability,batch,batch,1187,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1509,integrability,batch,batch,1509,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1539,integrability,batch,batch,1539,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1669,integrability,discover,discovery,1669,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:668,interoperability,integr,integration,668,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:715,interoperability,integr,integrated,715,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1669,interoperability,discover,discovery,1669,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:307,modifiability,variab,variable,307,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:668,modifiability,integr,integration,668,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:715,modifiability,integr,integrated,715,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1039,modifiability,exten,extent,1039,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:611,performance,time,timeline,611,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:684,performance,batch,batch,684,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:907,performance,batch,batch,907,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1050,performance,batch,batch,1050,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1094,performance,time,time,1094,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1187,performance,batch,batch,1187,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1509,performance,batch,batch,1509,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1539,performance,batch,batch,1539,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:668,reliability,integr,integration,668,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:715,reliability,integr,integrated,715,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1703,reliability,Doe,Does,1703,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:484,safety,compl,complex,484,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:585,safety,log,log,585,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:484,security,compl,complex,484,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:585,security,log,log,585,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:668,security,integr,integration,668,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:715,security,integr,integrated,715,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:585,testability,log,log,585,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:668,testability,integr,integration,668,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:715,testability,integr,integrated,715,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:862,usability,tool,tools,862,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:1669,usability,discov,discovery,1669,"Feature selection refers to excluding uninformative genes such as those which exhibit no meaningful biological variation across samples. Since scRNA-Seq experiments usually examine cells within a single tissue, only a small fraction of genes are expected to be informative since many genes are biologically variable only across different tissues (adopted from https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1861-6). But, in fact some experimental design are very complex, such single-cell RNAseq of tissues from different development stage. The tissues can vary a log along the development timeline. I find that the number of HVGs can affect data integration and batch effects correction. I've integrated seven cell samples collected at different development stage(1day, 2 day, 3 day, 4day, 5 day, 6 day, 7day after fertilization) with SCVI-tools, using 2000 HVGs, which then shows no ""batch effect"" (cells were mixed with no correlation among samples) left; on the other hand, using all genes, which shows still some extent of ""batch effect"" (some cells were clustered by time obviously) left. This could definitely affect the biological explaination, because the ""batch effect"" can be regarded as the difference of true biological difference at different development stage. The tissues are undergoing intensive differentiation process, so that the cell population are changing a lot during this process. Using only HVGs might lost these development process. . In sum, HVGs are good for batch effect correction. The ""batch effects"" become less obvious when using less genes and more obvious when using more genes. However, more genes are good for discovery of new cell population. Does this make sense ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:81,deployability,integr,integration,81,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:81,integrability,integr,integration,81,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:81,interoperability,integr,integration,81,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:81,modifiability,integr,integration,81,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:319,modifiability,variab,variable,319,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:161,performance,content,content,161,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:81,reliability,integr,integration,81,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:66,safety,review,review,66,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:81,security,integr,integration,81,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:328,security,sign,signal-to-noise,328,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:66,testability,review,review,66,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1578:81,testability,integr,integration,81,"Yes, this makes a lot of sense. This is also what we found in our review of data integration methods and pre-processing decisions [here](https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2). I'm not sure I agree with ""only a small fraction of genes are expected to be informative though"". There is definitely a variable signal-to-noise ratio though.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1578
https://github.com/scverse/scanpy/issues/1579:19,deployability,updat,updated,19,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:30,deployability,version,version,30,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:30,integrability,version,version,30,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:64,integrability,interfac,interface,64,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:64,interoperability,interfac,interface,64,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:30,modifiability,version,version,30,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:64,modifiability,interfac,interface,64,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:19,safety,updat,updated,19,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:19,security,updat,updated,19,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:12,usability,learn,learn,12,"Since `umap-learn` updated to version `0.5.0` from `0.4.6`, the interface may have changed.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:27,deployability,version,version,27,+1. Seems like pinning the version to <0.5 for now would solve the problem.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:27,integrability,version,version,27,+1. Seems like pinning the version to <0.5 for now would solve the problem.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:27,modifiability,version,version,27,+1. Seems like pinning the version to <0.5 for now would solve the problem.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:82,deployability,releas,release,82,Looks like this was solved on master last month. Any chance we could get a bugfix release?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:247,availability,sli,slightly,247,"So we've just put out a release of 1.7.0rc1, and will be releasing it proper soon. I'm looking at making a 1.6.1 release where the only change is pinning umap, but there are some CI issues (largely tests failing due to Matplotlib outputs changing slightly).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:24,deployability,releas,release,24,"So we've just put out a release of 1.7.0rc1, and will be releasing it proper soon. I'm looking at making a 1.6.1 release where the only change is pinning umap, but there are some CI issues (largely tests failing due to Matplotlib outputs changing slightly).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:57,deployability,releas,releasing,57,"So we've just put out a release of 1.7.0rc1, and will be releasing it proper soon. I'm looking at making a 1.6.1 release where the only change is pinning umap, but there are some CI issues (largely tests failing due to Matplotlib outputs changing slightly).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:113,deployability,releas,release,113,"So we've just put out a release of 1.7.0rc1, and will be releasing it proper soon. I'm looking at making a 1.6.1 release where the only change is pinning umap, but there are some CI issues (largely tests failing due to Matplotlib outputs changing slightly).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:204,deployability,fail,failing,204,"So we've just put out a release of 1.7.0rc1, and will be releasing it proper soon. I'm looking at making a 1.6.1 release where the only change is pinning umap, but there are some CI issues (largely tests failing due to Matplotlib outputs changing slightly).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:204,reliability,fail,failing,204,"So we've just put out a release of 1.7.0rc1, and will be releasing it proper soon. I'm looking at making a 1.6.1 release where the only change is pinning umap, but there are some CI issues (largely tests failing due to Matplotlib outputs changing slightly).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:247,reliability,sli,slightly,247,"So we've just put out a release of 1.7.0rc1, and will be releasing it proper soon. I'm looking at making a 1.6.1 release where the only change is pinning umap, but there are some CI issues (largely tests failing due to Matplotlib outputs changing slightly).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:198,safety,test,tests,198,"So we've just put out a release of 1.7.0rc1, and will be releasing it proper soon. I'm looking at making a 1.6.1 release where the only change is pinning umap, but there are some CI issues (largely tests failing due to Matplotlib outputs changing slightly).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:198,testability,test,tests,198,"So we've just put out a release of 1.7.0rc1, and will be releasing it proper soon. I'm looking at making a 1.6.1 release where the only change is pinning umap, but there are some CI issues (largely tests failing due to Matplotlib outputs changing slightly).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:18,deployability,releas,release,18,Thank you for the release update! I just had to. `pip install git+https://github.com/theislab/scanpy.git@1.7.0rc1`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:26,deployability,updat,update,26,Thank you for the release update! I just had to. `pip install git+https://github.com/theislab/scanpy.git@1.7.0rc1`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:54,deployability,instal,install,54,Thank you for the release update! I just had to. `pip install git+https://github.com/theislab/scanpy.git@1.7.0rc1`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:26,safety,updat,update,26,Thank you for the release update! I just had to. `pip install git+https://github.com/theislab/scanpy.git@1.7.0rc1`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:26,security,updat,update,26,Thank you for the release update! I just had to. `pip install git+https://github.com/theislab/scanpy.git@1.7.0rc1`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:59,deployability,releas,release,59,"I've just pushed 1.6.1 (pinning umap), but you can get the release candidate for 1.7.0 with `pip install ""scanpy==1.7.0rc1""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:97,deployability,instal,install,97,"I've just pushed 1.6.1 (pinning umap), but you can get the release candidate for 1.7.0 with `pip install ""scanpy==1.7.0rc1""`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:9,availability,error,error,9,Has this error been fixed? Facing the same issue for scanpy version 1.7.2,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:60,deployability,version,version,60,Has this error been fixed? Facing the same issue for scanpy version 1.7.2,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:60,integrability,version,version,60,Has this error been fixed? Facing the same issue for scanpy version 1.7.2,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:60,modifiability,version,version,60,Has this error been fixed? Facing the same issue for scanpy version 1.7.2,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:9,performance,error,error,9,Has this error been fixed? Facing the same issue for scanpy version 1.7.2,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:9,safety,error,error,9,Has this error been fixed? Facing the same issue for scanpy version 1.7.2,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:9,usability,error,error,9,Has this error been fixed? Facing the same issue for scanpy version 1.7.2,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:0,deployability,Version,Version,0,Version 1.8.0 works well.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:0,integrability,Version,Version,0,Version 1.8.0 works well.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:0,modifiability,Version,Version,0,Version 1.8.0 works well.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:8,availability,down,downgrading,8,I tried downgrading umap-learn to 0.4.6 but then sc.pp.neighbors won't work. I've been using scanpy 1.5.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:25,usability,learn,learn,25,I tried downgrading umap-learn to 0.4.6 but then sc.pp.neighbors won't work. I've been using scanpy 1.5.0,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:60,availability,down,downgraded,60,OK I computed the neighbors using umap-learn 0.5.1 and then downgraded to 0.4.6 for UMAP. Not elegant but so far so good.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:39,usability,learn,learn,39,OK I computed the neighbors using umap-learn 0.5.1 and then downgraded to 0.4.6 for UMAP. Not elegant but so far so good.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:0,deployability,Updat,Updating,0,Updating to Scanpy 1.8.2 worked!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:0,safety,Updat,Updating,0,Updating to Scanpy 1.8.2 worked!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:0,security,Updat,Updating,0,Updating to Scanpy 1.8.2 worked!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:100,availability,error,error,100,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:234,availability,error,error,234,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:545,availability,cluster,cluster,545,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1913,availability,servic,service,1913,"ies'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib3 1.26.8. velocyto 0.17.17. wheel 0.37.1. xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:428,deployability,modul,module,428,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:545,deployability,cluster,cluster,545,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1236,deployability,version,versions,1236,"message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1732,deployability,instal,install,1732,", n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1786,deployability,api,api-wrap,1786,"_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1913,deployability,servic,service,1913,"ies'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib3 1.26.8. velocyto 0.17.17. wheel 0.37.1. xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:2336,deployability,api,api,2336,"ies'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib3 1.26.8. velocyto 0.17.17. wheel 0.37.1. xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:240,integrability,messag,message,240,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1236,integrability,version,versions,1236,"message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1786,integrability,api,api-wrap,1786,"_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1913,integrability,servic,service,1913,"ies'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib3 1.26.8. velocyto 0.17.17. wheel 0.37.1. xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:2336,integrability,api,api,2336,"ies'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib3 1.26.8. velocyto 0.17.17. wheel 0.37.1. xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:240,interoperability,messag,message,240,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1786,interoperability,api,api-wrap,1786,"_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:2336,interoperability,api,api,2336,"ies'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib3 1.26.8. velocyto 0.17.17. wheel 0.37.1. xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:428,modifiability,modul,module,428,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:674,modifiability,pac,packages,674,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1236,modifiability,version,versions,1236,"message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1510,modifiability,pac,package-handling,1510,"sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1913,modifiability,servic,service,1913,"ies'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib3 1.26.8. velocyto 0.17.17. wheel 0.37.1. xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:2062,modifiability,pac,packaging,2062,"ies'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib3 1.26.8. velocyto 0.17.17. wheel 0.37.1. xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:100,performance,error,error,100,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:234,performance,error,error,234,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1311,performance,Bottleneck,Bottleneck,1311,"--------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWeb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1345,performance,cach,cached-property,1345,"most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1962,performance,network,networkx,1962,"ies'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib3 1.26.8. velocyto 0.17.17. wheel 0.37.1. xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:100,safety,error,error,100,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:234,safety,error,error,234,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:428,safety,modul,module,428,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1368,security,certif,certifi,1368,"st). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1534,security,cryptograph,cryptography,1534,"lor=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. se",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:1962,security,network,networkx,1962,"ies'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib3 1.26.8. velocyto 0.17.17. wheel 0.37.1. xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:342,testability,Trace,Traceback,342,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:100,usability,error,error,100,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:234,usability,error,error,234,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:690,usability,tool,tools,690,"Sorry for opening this thread again, but I think I've run into the same problem. Here's my code and error:. ```. mat_all = sc.read_loom(filename=""RSV.loom""). sc.pp.pca(mat_all). sc.pp.neighbors(mat_all). sc.tl.umap(mat_all). ```. The error message:. ```. ---------------------------------------------------------------------------. TypeError Traceback (most recent call last). /tmp/31048.tmpdir/ipykernel_3245/2128514342.py in <module>. 3 sc.pp.pca(mat_all). 4 sc.pp.neighbors(mat_all). ----> 5 sc.tl.umap(mat_all). 6 sc.pl.tsne(mat_all, color=""cluster"",legend_loc=""on data"",. 7 size=20, save=True). /storage1/fs1/leyao.wang/Active/conda/envs/velocyto3.9/lib/python3.9/site-packages/scanpy/tools/_umap.py in umap(adata, min_dist, spread, n_components, maxiter, alpha, gamma, negative_sample_rate, init_pos, random_state, a, b, copy, method, neighbors_key). 192 default_epochs = 500 if neighbors['connectivities'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:2512,usability,learn,learn,2512,"ies'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib3 1.26.8. velocyto 0.17.17. wheel 0.37.1. xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:2750,usability,learn,learn,2750,"ies'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib3 1.26.8. velocyto 0.17.17. wheel 0.37.1. xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:2858,usability,help,help,2858,"ies'].shape[0] <= 10000 else 200. 193 n_epochs = default_epochs if maxiter is None else maxiter. --> 194 X_umap = simplicial_set_embedding(. 195 X,. 196 neighbors['connectivities'].tocoo(),. TypeError: simplicial_set_embedding() missing 3 required positional arguments: 'densmap', 'densmap_kwds', and 'output_dens'. ```. And the versions I've been running:. anndata 0.7.8. asttokens 2.0.5. bcrypt 3.2.0. Bottleneck 1.3.2. brotlipy 0.7.0. cached-property 1.5.2. certifi 2021.10.8. cffi 1.15.0. charset-normalizer 2.0.12. chart-studio 1.1.0. click 8.0.4. cmake 3.22.2. colorama 0.4.4. conda 4.11.0. conda-package-handling 1.7.3. cryptography 36.0.1. cycler 0.11.0. Cython 0.29.20. devtools 0.8.0. dunamai 1.9.0. executing 0.8.2. fa2 0.3.5. Fabric 1.6.1. fonttools 4.29.1. get_version 3.5.4. h5py 3.6.0. idna 3.3. igraph 0.9.9. install 1.3.5. joblib 1.1.0. kiwisolver 1.3.2. legacy-api-wrap 1.2. llvmlite 0.38.0. loom 0.0.18. loompy 3.0.6. mamba 0.15.3. matplotlib 3.5.1. mkl-fft 1.3.1. mkl-random 1.2.2. mkl-service 2.4.0. MulticoreTSNE 0.1. natsort 8.1.0. networkx 2.6.3. numba 0.55.1. numexpr 2.8.1. numpy 1.21.2. numpy-groupies 0.9.14. opt-einsum 3.3.0. packaging 21.3. pandas 1.4.1. paramiko 2.9.2. patsy 0.5.2. Pillow 9.0.1. pip 21.2.4. plotly 5.6.0. pycosat 0.6.3. pycparser 2.21. PyNaCl 1.5.0. pynndescent 0.5.6. pyOpenSSL 22.0.0. pyparsing 3.0.7. PyQt5 5.12.3. PyQt5_sip 4.19.18. PyQtChart 5.12. PyQtWebEngine 5.12.1. pyro-api 0.1.2. pyro-ppl 1.8.0. pysam 0.18.0. PySocks 1.7.1. python-dateutil 2.8.2. pytz 2021.3. requests 2.27.1. retrying 1.3.3. ruamel-yaml-conda 0.15.80. scanpy 1.7.0rc1. scikit-learn 1.0.2. scipy 1.7.3. seaborn 0.11.2. setuptools 58.0.4. sinfo 0.3.4. six 1.16.0. statsmodels 0.13.2. stdlib-list 0.8.0. tables 3.7.0. tenacity 8.0.1. texttable 1.6.4. threadpoolctl 3.1.0. torch 1.10.2. tornado 6.1. tqdm 4.62.3. umap-learn 0.4.6. unicodedata2 14.0.0. urllib3 1.26.8. velocyto 0.17.17. wheel 0.37.1. xlrd 1.2.0. If anyone can help me resolve this that would be great. Thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:5,deployability,updat,updated,5,"Just updated to scanpy 1.8.2, but the problem persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:5,safety,updat,updated,5,"Just updated to scanpy 1.8.2, but the problem persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:5,security,updat,updated,5,"Just updated to scanpy 1.8.2, but the problem persists.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:39,deployability,instal,install,39,udpating umap-learn work for me . `pip install umap-learn==0.5.3`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:14,usability,learn,learn,14,udpating umap-learn work for me . `pip install umap-learn==0.5.3`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1579:52,usability,learn,learn,52,udpating umap-learn work for me . `pip install umap-learn==0.5.3`,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1579
https://github.com/scverse/scanpy/issues/1581:31,usability,close,closed,31,Think this is fixed and can be closed right?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1581
https://github.com/scverse/scanpy/pull/1583:42,deployability,updat,updates,42,"> * Shouldn't `var_df` should get similar updates to `obs_df`? I would suggest a different PR to address this. . > * Could we get tests for `get.obs_df`/ `get.var_df` for the issues you addressed here (repeated indices)? Sure, I added new tests to `get.obs_df` to check duplicated keys.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:42,safety,updat,updates,42,"> * Shouldn't `var_df` should get similar updates to `obs_df`? I would suggest a different PR to address this. . > * Could we get tests for `get.obs_df`/ `get.var_df` for the issues you addressed here (repeated indices)? Sure, I added new tests to `get.obs_df` to check duplicated keys.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:130,safety,test,tests,130,"> * Shouldn't `var_df` should get similar updates to `obs_df`? I would suggest a different PR to address this. . > * Could we get tests for `get.obs_df`/ `get.var_df` for the issues you addressed here (repeated indices)? Sure, I added new tests to `get.obs_df` to check duplicated keys.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:239,safety,test,tests,239,"> * Shouldn't `var_df` should get similar updates to `obs_df`? I would suggest a different PR to address this. . > * Could we get tests for `get.obs_df`/ `get.var_df` for the issues you addressed here (repeated indices)? Sure, I added new tests to `get.obs_df` to check duplicated keys.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:42,security,updat,updates,42,"> * Shouldn't `var_df` should get similar updates to `obs_df`? I would suggest a different PR to address this. . > * Could we get tests for `get.obs_df`/ `get.var_df` for the issues you addressed here (repeated indices)? Sure, I added new tests to `get.obs_df` to check duplicated keys.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:130,testability,test,tests,130,"> * Shouldn't `var_df` should get similar updates to `obs_df`? I would suggest a different PR to address this. . > * Could we get tests for `get.obs_df`/ `get.var_df` for the issues you addressed here (repeated indices)? Sure, I added new tests to `get.obs_df` to check duplicated keys.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:239,testability,test,tests,239,"> * Shouldn't `var_df` should get similar updates to `obs_df`? I would suggest a different PR to address this. . > * Could we get tests for `get.obs_df`/ `get.var_df` for the issues you addressed here (repeated indices)? Sure, I added new tests to `get.obs_df` to check duplicated keys.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:60,deployability,fail,failing,60,@ivirshup I think this PR is ready to merge. readthedocs is failing because of a missing link from scvelo unrelated to the PR,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:60,reliability,fail,failing,60,@ivirshup I think this PR is ready to merge. readthedocs is failing because of a missing link from scvelo unrelated to the PR,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:732,availability,error,error,732,"Fidel, sorry to say, but I've run into some issues. Most of these actually didn't have to do with this PR, but were additional things that broke from #1499. I'll give you a few examples of what I've found, mostly by contrast with the current behaviour of 1.6.1. ```python. import scanpy as sc, pandas as pd, numpy as np. M, N = (5, 3). adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 3).reshape((M, 3)),. columns=[""repeated_col"", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:755,availability,down,downstream,755,"Fidel, sorry to say, but I've run into some issues. Most of these actually didn't have to do with this PR, but were additional things that broke from #1499. I'll give you a few examples of what I've found, mostly by contrast with the current behaviour of 1.6.1. ```python. import scanpy as sc, pandas as pd, numpy as np. M, N = (5, 3). adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 3).reshape((M, 3)),. columns=[""repeated_col"", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1452,availability,error,errors,1452,", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1909,availability,error,error,1909,"viour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ----------------------------------------------------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2116,availability,error,error,2116,"ol"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2842,availability,error,errors,2842,"number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4237,availability,error,error,4237,"eys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexErro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4515,availability,error,errors,4515,"layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5162,availability,toler,tolerance,5162,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5475,availability,error,error,5475,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5519,availability,error,error,5519,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3019,deployability,modul,module,3019,"biguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4699,deployability,modul,module,4699,"y-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This shoul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:6009,deployability,releas,release,6009,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:6030,deployability,patch,patch,6030,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:6069,deployability,releas,release,6069,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:234,energy efficiency,current,current,234,"Fidel, sorry to say, but I've run into some issues. Most of these actually didn't have to do with this PR, but were additional things that broke from #1499. I'll give you a few examples of what I've found, mostly by contrast with the current behaviour of 1.6.1. ```python. import scanpy as sc, pandas as pd, numpy as np. M, N = (5, 3). adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 3).reshape((M, 3)),. columns=[""repeated_col"", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:792,energy efficiency,current,currently,792,"Fidel, sorry to say, but I've run into some issues. Most of these actually didn't have to do with this PR, but were additional things that broke from #1499. I'll give you a few examples of what I've found, mostly by contrast with the current behaviour of 1.6.1. ```python. import scanpy as sc, pandas as pd, numpy as np. M, N = (5, 3). adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 3).reshape((M, 3)),. columns=[""repeated_col"", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1534,energy efficiency,core,core,1534,"e=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""])",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5097,energy efficiency,core,core,5097,"(pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree wi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5597,energy efficiency,current,current,5597,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5525,integrability,messag,message,5525,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5525,interoperability,messag,message,5525,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1518,modifiability,pac,packages,1518,"range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adat",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3019,modifiability,modul,module,3019,"biguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3118,modifiability,pac,packages,3118," I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occure",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3175,modifiability,layer,layer,3175,"imes, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3330,modifiability,layer,layer,3330,"aFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3336,modifiability,layer,layer,3336,"(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.o",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3440,modifiability,pac,packages,3440,"ex=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3497,modifiability,layer,layer,3497," ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3518,modifiability,layer,layer,3518,"ta, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3581,modifiability,layer,layer,3581,"thub/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3587,modifiability,layer,layer,3587,"canpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3634,modifiability,layer,layer,3634,"var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3727,modifiability,pac,packages,3727,"rnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(a",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3799,modifiability,layer,layer,3799," 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4699,modifiability,modul,module,4699,"y-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This shoul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4819,modifiability,layer,layer,4819,"n_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially m",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5081,modifiability,pac,packages,5081,"n `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. D",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:732,performance,error,error,732,"Fidel, sorry to say, but I've run into some issues. Most of these actually didn't have to do with this PR, but were additional things that broke from #1499. I'll give you a few examples of what I've found, mostly by contrast with the current behaviour of 1.6.1. ```python. import scanpy as sc, pandas as pd, numpy as np. M, N = (5, 3). adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 3).reshape((M, 3)),. columns=[""repeated_col"", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1452,performance,error,errors,1452,", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1909,performance,error,error,1909,"viour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ----------------------------------------------------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2116,performance,error,error,2116,"ol"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2177,performance,time,times,2177,". ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2842,performance,error,errors,2842,"number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4237,performance,error,error,4237,"eys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexErro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4515,performance,error,errors,4515,"layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5250,performance,Reindex,Reindexing,5250,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5336,performance,Reindex,Reindexing,5336,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5475,performance,error,error,5475,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5519,performance,error,error,5519,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:6039,performance,perform,performance,6039,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2188,reliability,doe,doesn,2188,"d.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symb",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5162,reliability,toleran,tolerance,5162,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5470,reliability,doe,does,5470,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:732,safety,error,error,732,"Fidel, sorry to say, but I've run into some issues. Most of these actually didn't have to do with this PR, but were additional things that broke from #1499. I'll give you a few examples of what I've found, mostly by contrast with the current behaviour of 1.6.1. ```python. import scanpy as sc, pandas as pd, numpy as np. M, N = (5, 3). adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 3).reshape((M, 3)),. columns=[""repeated_col"", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:827,safety,input,input,827,"Fidel, sorry to say, but I've run into some issues. Most of these actually didn't have to do with this PR, but were additional things that broke from #1499. I'll give you a few examples of what I've found, mostly by contrast with the current behaviour of 1.6.1. ```python. import scanpy as sc, pandas as pd, numpy as np. M, N = (5, 3). adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 3).reshape((M, 3)),. columns=[""repeated_col"", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1452,safety,error,errors,1452,", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1909,safety,error,error,1909,"viour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ----------------------------------------------------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2116,safety,error,error,2116,"ol"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2842,safety,error,errors,2842,"number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2992,safety,input,input-,2992,"In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:3019,safety,modul,module,3019,"biguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4237,safety,error,error,4237,"eys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexErro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4515,safety,error,errors,4515,"layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4672,safety,input,input-,4672,"y:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4699,safety,modul,module,4699,"y-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This shoul",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5266,safety,valid,valid,5266,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5352,safety,valid,valid,5352,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5475,safety,error,error,5475,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5519,safety,error,error,5519,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5969,safety,test,tests,5969,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:6030,safety,patch,patch,6030,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5761,security,loss,loss,5761,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:6030,security,patch,patch,6030,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:6107,security,assess,assessment,6107,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2948,testability,Trace,Traceback,2948,"in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4628,testability,Trace,Traceback,4628," layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert mos",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5969,testability,test,tests,5969,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:242,usability,behavi,behaviour,242,"Fidel, sorry to say, but I've run into some issues. Most of these actually didn't have to do with this PR, but were additional things that broke from #1499. I'll give you a few examples of what I've found, mostly by contrast with the current behaviour of 1.6.1. ```python. import scanpy as sc, pandas as pd, numpy as np. M, N = (5, 3). adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 3).reshape((M, 3)),. columns=[""repeated_col"", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:732,usability,error,error,732,"Fidel, sorry to say, but I've run into some issues. Most of these actually didn't have to do with this PR, but were additional things that broke from #1499. I'll give you a few examples of what I've found, mostly by contrast with the current behaviour of 1.6.1. ```python. import scanpy as sc, pandas as pd, numpy as np. M, N = (5, 3). adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 3).reshape((M, 3)),. columns=[""repeated_col"", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:827,usability,input,input,827,"Fidel, sorry to say, but I've run into some issues. Most of these actually didn't have to do with this PR, but were additional things that broke from #1499. I'll give you a few examples of what I've found, mostly by contrast with the current behaviour of 1.6.1. ```python. import scanpy as sc, pandas as pd, numpy as np. M, N = (5, 3). adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 3).reshape((M, 3)),. columns=[""repeated_col"", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:908,usability,behavi,behaviour,908,"Fidel, sorry to say, but I've run into some issues. Most of these actually didn't have to do with this PR, but were additional things that broke from #1499. I'll give you a few examples of what I've found, mostly by contrast with the current behaviour of 1.6.1. ```python. import scanpy as sc, pandas as pd, numpy as np. M, N = (5, 3). adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 3).reshape((M, 3)),. columns=[""repeated_col"", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1452,usability,error,errors,1452,", ""repeated_col"", ""var_id""],. index=pd.Index([f""cell_{i}"" for i in range(M)], name=""obs_index""),. ),. var=pd.DataFrame(. index=pd.Index([""var_id""] + [f""gene_{i}"" for i in range(N-1)], name=""var_index""),. ),. ). ```. ## Repeated column in `adata.obs`. I think this should be an error. This is because downstream functions (like plotting) currently assume that for each key input here, there will be one output column. Turns out this isn't exactly pandas behaviour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1909,usability,error,error,1909,"viour with repeated column values, but I do think it's reasonable. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M * 2).reshape((M, 2)),. columns=[""repeated_col"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ----------------------------------------------------",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2116,usability,error,error,2116,"ol"", ""repeated_col""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[f""gene_{i}"" for i in range(N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2234,usability,behavi,behaviour,2234,"N)],. ), . ). sc.get.obs_df(adata, [""repeated_col""]). ```. ### This pr (gets both columns). ```. repeated_col repeated_col. obs_index . cell_0 0 1. cell_1 3 4. cell_2 6 7. cell_3 9 10. cell_4 12 13. ```. ### 1.6 (errors). ```pytb. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/pandas/core/internals/blocks.py in __init__(self, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2570,usability,User,Users,2570,"lf, values, placement, ndim). 140 . 141 if self._validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"",",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2615,usability,User,UserWarning,2615,"_validate_ndim and self.ndim and len(self.mgr_locs) != len(self.values):. --> 142 raise ValueError(. 143 f""Wrong number of items passed {len(self.values)}, "". 144 f""placement implies {len(self.mgr_locs)}"". ValueError: Wrong number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2842,usability,error,errors,2842,"number of items passed 2, placement implies 1. ```. Not a great error, could definitley be improved. ## Key in adata.obs.columns and adata.var_names. In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2992,usability,input,input-,2992,"In this case, the key is ambiguous (should it get the gene values or the column from obs?). I think this means it should error. I feel like this point has been discussed a number of times, but doesn't seem to have been discussed when this behaviour was changed. ```python. M, N = 5, 3. adata = sc.AnnData(. X=np.zeros((M, N)),. obs=pd.DataFrame(. np.arange(M),. columns=[""var_id""],. index=[f""cell_{i}"" for i in range(M)],. ),. var=pd.DataFrame(. index=[""var_id""] + [f""gene_{i}"" for i in range(N-1)],. ), . ). sc.get.obs_df(adata, [""var_id""]). ```. ### This pr (warns). ```. /Users/isaac/github/scanpy/scanpy/get.py:177: UserWarning: The key `var_id` is found in both adata.obs and adata.var_names.Only the adata.obs key will be used. warnings.warn(. Out[58]: . var_id. obs_index . cell_0 2. cell_1 5. cell_2 8. cell_3 11. cell_4 14. ```. ### 1.6 (errors). ```pytb. ---------------------------------------------------------------------------. ValueError Traceback (most recent call last). <ipython-input-16-69be169f6a4f> in <module>. ----> 1 sc.get.obs_df(adata, [""var_id""]). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 171 for k, l in zip(keys, lookup_keys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found i",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4237,usability,error,error,4237,"eys):. 172 if not use_raw or k in adata.obs.columns:. --> 173 df[k] = adata.obs_vector(l, layer=layer). 174 else:. 175 df[k] = adata.raw.obs_vector(l). ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/anndata.py in obs_vector(self, k, layer). 1362 ). 1363 layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexErro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4515,usability,error,errors,4515,"layer = None. -> 1364 return get_vector(self, k, ""obs"", ""var"", layer=layer). 1365 . 1366 def var_vector(self, k, *, layer: Optional[str] = None) -> np.ndarray:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:4672,usability,input,input-,4672,"y:. ~/miniconda3/envs/scanpy-1.6/lib/python3.8/site-packages/anndata/_core/index.py in get_vector(adata, k, coldim, idxdim, layer). 156 . 157 if (in_col + in_idx) == 2:. --> 158 raise ValueError(. 159 f""Key {k} could be found in both .{idxdim}_names and .{coldim}.columns"". 160 ). ValueError: Key var_id could be found in both .var_names and .obs.columns. ```. ## Repeats in var_names. When there are repeats in `var_names` (pretty frequent occurence), getting a dataframe with keys that aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from th",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5475,usability,error,error,5475,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5519,usability,error,error,5519,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:5936,usability,user,user,5936,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:6039,usability,perform,performance,6039,"at aren't repeated. I think it's fine for this to work. I do think it should error if the key is one values that is duplicated in the index. ```python. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). sc.get.obs_df(adata, [""gene-1""]). ``````. ### This PR (errors). ```pytb. ---------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-62-405d671e2970> in <module>. ----> 1 sc.get.obs_df(adata, [""a"", ""gene-1""]). ~/github/scanpy/scanpy/get.py in obs_df(adata, keys, obsm_keys, layer, gene_symbols, use_raw). 213 var_idx = adata.raw.var_names.get_indexer(var_names). 214 else:. --> 215 var_idx = adata.var_names.get_indexer(var_names). 216 . 217 # for backed AnnData is important that the indices are ordered. /usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. ### 1.6 (suceeds). ```python. gene-1. cell-0 1.0. cell-1 1.0. ```. 1.6 does error if I use `""gene-0""` as a key, but the error message could definitley be better. ## What should we do about this? My current inclination is to revert most changes to `obs_df` and `var_df` from this PR and #1499. This should leave the use of indices as groupby untouched. Also, the loss of perfomance from reverting #1499 should be partially mitigated by improvements in pandas (see https://github.com/pandas-dev/pandas/issues/37954). We would keep all the user facing changes, and all the tests from both PRs. We can then make a release now, and can patch in performance boosts during the release cycle. Do you agree with this assessment? If not, could you propose an alternative?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:108,availability,error,error,108,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:293,availability,error,error,293,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:447,availability,error,error,447,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:587,availability,error,error,587,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1191,availability,sli,sliced,1191,"ich will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(sel",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1493,availability,sli,slice,1493,"s. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1500,availability,sli,slice,1500,"ally, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2220,availability,toler,tolerance,2220,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2902,availability,sli,slicing,2902,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:478,deployability,contain,contains,478,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1010,deployability,modul,module,1010,"arding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string arr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2155,energy efficiency,core,core,2155,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1010,modifiability,modul,module,1010,"arding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string arr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1054,modifiability,pac,packages,1054,"lumns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(ind",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1349,modifiability,pac,packages,1349," suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued In",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1638,modifiability,pac,packages,1638,"not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this wil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1866,modifiability,pac,packages,1866,"-------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should resp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2139,modifiability,pac,packages,2139,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:108,performance,error,error,108,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:293,performance,error,error,293,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:447,performance,error,error,447,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:587,performance,error,error,587,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2308,performance,Reindex,Reindexing,2308,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2394,performance,Reindex,Reindexing,2394,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1191,reliability,sli,sliced,1191,"ich will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(sel",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1493,reliability,sli,slice,1493,"s. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1500,reliability,sli,slice,1500,"ally, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2220,reliability,toleran,tolerance,2220,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2902,reliability,sli,slicing,2902,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:108,safety,error,error,108,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:293,safety,error,error,293,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:447,safety,error,error,447,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:587,safety,error,error,587,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:983,safety,input,input-,983,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1010,safety,modul,module,1010,"arding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string arr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1621,safety,compl,complete,1621,"he var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you sug",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2324,safety,valid,valid,2324,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2410,safety,valid,valid,2410,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2805,safety,except,except,2805,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2947,safety,test,tests,2947,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2995,safety,test,test,2995,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:1621,security,compl,complete,1621,"he var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you sug",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:939,testability,Trace,Traceback,939,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2947,testability,test,tests,2947,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:2995,testability,test,test,2995,"data[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should be string array. ---> 97 positions = index.get_indexer(indexer). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. site-packages/pandas/core/indexes/base.py in get_indexer(self, target, method, limit, tolerance). 3169 . 3170 if not self.is_unique:. -> 3171 raise InvalidIndexError(. 3172 ""Reindexing only valid with uniquely valued Index objects"". 3173 ). InvalidIndexError: Reindexing only valid with uniquely valued Index objects. ```. * `key` in both `adata.obs.columns` and `adata.var.index`: I changed this to raise a ValueError. I though about reverting back to the original implementation as you suggest but this will not work with `_anndata._prepare_dataframe` as I introduced some changes to work with this function. . The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the `AnnData` object. I added new tests based on your examples. I added checks to test for unique adata.obs.columns",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:108,usability,error,error,108,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:293,usability,error,error,293,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:447,usability,error,error,447,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:587,usability,error,error,587,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:983,usability,input,input-,983,"@ivirshup regarding the three comments:. . * `adata.obs.columns` non_unique: I also think this should be an error as this will cause trouble at some point or another. I added a check for this which will raise a ValueError if duplicates are found. * `adata.var.index` Here I wanted to raise an error only if a `key` matched a duplicated `var_name` as you suggested. However, the selection by index, instead of `var_name`, from the matrix causes an error whenever the `var_names` contains duplicates. Actually, doing an AnnData selection when the `var_names` are not unique also raises an error. Thus, I added a ValueError if the var_names are not unique. ```PYTHON. adata = sc.AnnData(. X=np.ones((2, 3)),. obs=pd.DataFrame(index=[""cell-0"", ""cell-1""]),. var=pd.DataFrame(index=[""gene-0"", ""gene-0"", ""gene-1""]),. ). adata[:, ['gene-1']]. ```. ```. --------------------------------------------------------------------------. InvalidIndexError Traceback (most recent call last). <ipython-input-58-8d1a96772653> in <module>. ----> 1 adata[:, ['gene-0']]. site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 33 ax0, ax1 = unpack_index(index). 34 ax0 = _normalize_index(ax0, names0). ---> 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. 37 . site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 95 return positions # np.ndarray[int]. 96 else: # indexer should ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:42,interoperability,incompatib,incompatible,42,"@fidelram, what are the changes which are incompatible with `_anndata._prepare_dataframe`? I've tried reverting dealing with `X` and those seemed to work fine for me.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:157,availability,sli,slicing,157,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:317,availability,error,error,317,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:317,performance,error,error,317,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:157,reliability,sli,slicing,157,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:60,safety,except,except,60,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:242,safety,compl,complained,242,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:317,safety,error,error,317,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:337,safety,compl,complaints,337,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:242,security,compl,complained,242,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:337,security,compl,complaints,337,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:486,security,access,accessed,486,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:510,security,ident,identifiers,510,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1583:317,usability,error,error,317,"> The just added changes should mimic the response from 1.6 except for duplicate names in var_names which I think should respond similarly like when doing a slicing on the AnnData object. Thinking about this more. Considering that no one has complained about this so far. I think I'm actually fine with this being an error. If there are complaints, I think we should change it back. I do think it's important that `gene_symbols` can have duplicates as long as those values aren't being accessed (as non-unique identifiers is the whole point of `gene_symbols`).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1583
https://github.com/scverse/scanpy/pull/1584:191,availability,consist,consistent,191,"I think that having the case with identical values is actually useful, since otherwise we're not checking this edge case. Ideally, I think we should be using a stable sort so results will be consistent in the case of similar values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:34,security,ident,identical,34,"I think that having the case with identical values is actually useful, since otherwise we're not checking this edge case. Ideally, I think we should be using a stable sort so results will be consistent in the case of similar values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:191,usability,consist,consistent,191,"I think that having the case with identical values is actually useful, since otherwise we're not checking this edge case. Ideally, I think we should be using a stable sort so results will be consistent in the case of similar values.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:26,reliability,doe,does,26,"Huh, it looks like pandas does not have the option for doing a stable sort here, so maybe we should change the test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:111,safety,test,test,111,"Huh, it looks like pandas does not have the option for doing a stable sort here, so maybe we should change the test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:111,testability,test,test,111,"Huh, it looks like pandas does not have the option for doing a stable sort here, so maybe we should change the test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:95,deployability,version,version,95,"Yeah, I check to see if there was some option to get the same sorting regardless of the pandas version, but could not find anything. I will merge the change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:95,integrability,version,version,95,"Yeah, I check to see if there was some option to get the same sorting regardless of the pandas version, but could not find anything. I will merge the change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/pull/1584:95,modifiability,version,version,95,"Yeah, I check to see if there was some option to get the same sorting regardless of the pandas version, but could not find anything. I will merge the change.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1584
https://github.com/scverse/scanpy/issues/1585:0,energy efficiency,Current,Currently,0,Currently being added to sfaira:. https://github.com/theislab/sfaira/pull/72/files#diff-de563ce0a02335898f4a4e1bca2fc074614122bfce30b980d7065142806e4defR38. Feel free to just copy the code.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:79,deployability,build,builds,79,"That'd be great, thanks! I was thinking it could replace this step in the test builds:. https://github.com/theislab/scanpy/blob/5fc12f4a918e21f0c57937b787d52040db046f01/.azure-pipelines.yml#L78-L81. And was thinking of using azure for it instead of actions, just to consolidate CI stuff a bit. I was thinking a build and check could just be a separate job? Open to suggestions on this however.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:176,deployability,pipelin,pipelines,176,"That'd be great, thanks! I was thinking it could replace this step in the test builds:. https://github.com/theislab/scanpy/blob/5fc12f4a918e21f0c57937b787d52040db046f01/.azure-pipelines.yml#L78-L81. And was thinking of using azure for it instead of actions, just to consolidate CI stuff a bit. I was thinking a build and check could just be a separate job? Open to suggestions on this however.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:311,deployability,build,build,311,"That'd be great, thanks! I was thinking it could replace this step in the test builds:. https://github.com/theislab/scanpy/blob/5fc12f4a918e21f0c57937b787d52040db046f01/.azure-pipelines.yml#L78-L81. And was thinking of using azure for it instead of actions, just to consolidate CI stuff a bit. I was thinking a build and check could just be a separate job? Open to suggestions on this however.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:176,integrability,pipelin,pipelines,176,"That'd be great, thanks! I was thinking it could replace this step in the test builds:. https://github.com/theislab/scanpy/blob/5fc12f4a918e21f0c57937b787d52040db046f01/.azure-pipelines.yml#L78-L81. And was thinking of using azure for it instead of actions, just to consolidate CI stuff a bit. I was thinking a build and check could just be a separate job? Open to suggestions on this however.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:74,safety,test,test,74,"That'd be great, thanks! I was thinking it could replace this step in the test builds:. https://github.com/theislab/scanpy/blob/5fc12f4a918e21f0c57937b787d52040db046f01/.azure-pipelines.yml#L78-L81. And was thinking of using azure for it instead of actions, just to consolidate CI stuff a bit. I was thinking a build and check could just be a separate job? Open to suggestions on this however.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:74,testability,test,test,74,"That'd be great, thanks! I was thinking it could replace this step in the test builds:. https://github.com/theislab/scanpy/blob/5fc12f4a918e21f0c57937b787d52040db046f01/.azure-pipelines.yml#L78-L81. And was thinking of using azure for it instead of actions, just to consolidate CI stuff a bit. I was thinking a build and check could just be a separate job? Open to suggestions on this however.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:50,availability,servic,service,50,"Yeah, guess eventually we should settle on one CI service. Effectively we should get rid of Travis, since they have serious problems. > I was thinking a build and check could just be a separate job? Well you have to build the wheels to run the check anyways. But I did not have a look at the azure setup yet, so will see what I can come up with :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:50,deployability,servic,service,50,"Yeah, guess eventually we should settle on one CI service. Effectively we should get rid of Travis, since they have serious problems. > I was thinking a build and check could just be a separate job? Well you have to build the wheels to run the check anyways. But I did not have a look at the azure setup yet, so will see what I can come up with :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:153,deployability,build,build,153,"Yeah, guess eventually we should settle on one CI service. Effectively we should get rid of Travis, since they have serious problems. > I was thinking a build and check could just be a separate job? Well you have to build the wheels to run the check anyways. But I did not have a look at the azure setup yet, so will see what I can come up with :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:216,deployability,build,build,216,"Yeah, guess eventually we should settle on one CI service. Effectively we should get rid of Travis, since they have serious problems. > I was thinking a build and check could just be a separate job? Well you have to build the wheels to run the check anyways. But I did not have a look at the azure setup yet, so will see what I can come up with :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:12,integrability,event,eventually,12,"Yeah, guess eventually we should settle on one CI service. Effectively we should get rid of Travis, since they have serious problems. > I was thinking a build and check could just be a separate job? Well you have to build the wheels to run the check anyways. But I did not have a look at the azure setup yet, so will see what I can come up with :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:50,integrability,servic,service,50,"Yeah, guess eventually we should settle on one CI service. Effectively we should get rid of Travis, since they have serious problems. > I was thinking a build and check could just be a separate job? Well you have to build the wheels to run the check anyways. But I did not have a look at the azure setup yet, so will see what I can come up with :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:50,modifiability,servic,service,50,"Yeah, guess eventually we should settle on one CI service. Effectively we should get rid of Travis, since they have serious problems. > I was thinking a build and check could just be a separate job? Well you have to build the wheels to run the check anyways. But I did not have a look at the azure setup yet, so will see what I can come up with :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:59,usability,Effectiv,Effectively,59,"Yeah, guess eventually we should settle on one CI service. Effectively we should get rid of Travis, since they have serious problems. > I was thinking a build and check could just be a separate job? Well you have to build the wheels to run the check anyways. But I did not have a look at the azure setup yet, so will see what I can come up with :)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:19,deployability,build,build,19,"> Well you have to build the wheels to run the check anyways. Oh, what I meant is that we sorta do a check in the same job as running the tests. I was thinking this should be separated out into a job which does ""build and check"" together, rather than including it in the testing job.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:212,deployability,build,build,212,"> Well you have to build the wheels to run the check anyways. Oh, what I meant is that we sorta do a check in the same job as running the tests. I was thinking this should be separated out into a job which does ""build and check"" together, rather than including it in the testing job.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:206,reliability,doe,does,206,"> Well you have to build the wheels to run the check anyways. Oh, what I meant is that we sorta do a check in the same job as running the tests. I was thinking this should be separated out into a job which does ""build and check"" together, rather than including it in the testing job.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:138,safety,test,tests,138,"> Well you have to build the wheels to run the check anyways. Oh, what I meant is that we sorta do a check in the same job as running the tests. I was thinking this should be separated out into a job which does ""build and check"" together, rather than including it in the testing job.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:271,safety,test,testing,271,"> Well you have to build the wheels to run the check anyways. Oh, what I meant is that we sorta do a check in the same job as running the tests. I was thinking this should be separated out into a job which does ""build and check"" together, rather than including it in the testing job.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:138,testability,test,tests,138,"> Well you have to build the wheels to run the check anyways. Oh, what I meant is that we sorta do a check in the same job as running the tests. I was thinking this should be separated out into a job which does ""build and check"" together, rather than including it in the testing job.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/issues/1585:271,testability,test,testing,271,"> Well you have to build the wheels to run the check anyways. Oh, what I meant is that we sorta do a check in the same job as running the tests. I was thinking this should be separated out into a job which does ""build and check"" together, rather than including it in the testing job.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1585
https://github.com/scverse/scanpy/pull/1587:91,deployability,fail,failed,91,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live. - [x] `.gitignore` update. - [x] Remove failing test (just there as an example). - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:404,deployability,updat,update,404,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live. - [x] `.gitignore` update. - [x] Remove failing test (just there as an example). - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:425,deployability,fail,failing,425,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live. - [x] `.gitignore` update. - [x] Remove failing test (just there as an example). - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:368,interoperability,xml,xml,368,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live. - [x] `.gitignore` update. - [x] Remove failing test (just there as an example). - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:91,reliability,fail,failed,91,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live. - [x] `.gitignore` update. - [x] Remove failing test (just there as an example). - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:425,reliability,fail,failing,425,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live. - [x] `.gitignore` update. - [x] Remove failing test (just there as an example). - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:86,safety,test,test,86,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live. - [x] `.gitignore` update. - [x] Remove failing test (just there as an example). - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:404,safety,updat,update,404,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live. - [x] `.gitignore` update. - [x] Remove failing test (just there as an example). - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:433,safety,test,test,433,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live. - [x] `.gitignore` update. - [x] Remove failing test (just there as an example). - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:404,security,updat,update,404,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live. - [x] `.gitignore` update. - [x] Remove failing test (just there as an example). - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:86,testability,test,test,86,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live. - [x] `.gitignore` update. - [x] Remove failing test (just there as an example). - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:433,testability,test,test,433,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live. - [x] `.gitignore` update. - [x] Remove failing test (just there as an example). - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:472,usability,Document,Document,472,"> yes, they are annoying particularly because is not possible to actually check why a test failed on the server while passes locally. . @fidelram, from your comment (https://github.com/theislab/scanpy/pull/1551#issuecomment-761117523), makes me think you'd like to enable this? If you okay this, all this needs to be ready to merge is: . - [x] Figure out where result xml should live. - [x] `.gitignore` update. - [x] Remove failing test (just there as an example). - [x] Document where to find this stuff",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:141,interoperability,xml,xml,141,"This is great Isaac! Very welcome feature. Took me a while to find the 'attachments' in the azure webpage. Regarding the location or results.xml, I honestly don't have an opinion were to locate it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:78,usability,document,document,78,Great! > Took me a while to find the 'attachments' in the azure webpage. I'll document this!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:97,usability,user,user-images,97,"@ivirshup do you know why are there sometimes three:. <img width=""1581"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110840734-16c82f00-8273-11eb-8a75-b0ac45ee9db1.png"">. and sometimes two plots:. <img width=""1476"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110840809-29426880-8273-11eb-836d-dc024b28d7e6.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/pull/1587:262,usability,user,user-images,262,"@ivirshup do you know why are there sometimes three:. <img width=""1581"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110840734-16c82f00-8273-11eb-8a75-b0ac45ee9db1.png"">. and sometimes two plots:. <img width=""1476"" alt=""image"" src=""https://user-images.githubusercontent.com/1140359/110840809-29426880-8273-11eb-836d-dc024b28d7e6.png"">.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1587
https://github.com/scverse/scanpy/issues/1588:434,availability,cluster,clusters,434,"I thought we didn't sort for categorical plots? Other than when there are ""null"" values, then `sort_order=True` puts non-null values on top, which `sort_order=False` disables. https://github.com/theislab/scanpy/blob/1878644614ecba4b265241e68b07cc32ac13db4d/scanpy/plotting/_tools/scatterplots.py#L255-L262. Do you have examples of this not being the case? I have been thinking about what we should do if there are cells from multiple clusters overlapping in a pixel. Some way of letting users know something is being hidden.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:434,deployability,cluster,clusters,434,"I thought we didn't sort for categorical plots? Other than when there are ""null"" values, then `sort_order=True` puts non-null values on top, which `sort_order=False` disables. https://github.com/theislab/scanpy/blob/1878644614ecba4b265241e68b07cc32ac13db4d/scanpy/plotting/_tools/scatterplots.py#L255-L262. Do you have examples of this not being the case? I have been thinking about what we should do if there are cells from multiple clusters overlapping in a pixel. Some way of letting users know something is being hidden.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:487,usability,user,users,487,"I thought we didn't sort for categorical plots? Other than when there are ""null"" values, then `sort_order=True` puts non-null values on top, which `sort_order=False` disables. https://github.com/theislab/scanpy/blob/1878644614ecba4b265241e68b07cc32ac13db4d/scanpy/plotting/_tools/scatterplots.py#L255-L262. Do you have examples of this not being the case? I have been thinking about what we should do if there are cells from multiple clusters overlapping in a pixel. Some way of letting users know something is being hidden.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:168,deployability,integr,integration,168,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:168,integrability,integr,integration,168,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:203,integrability,batch,batch,203,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:259,integrability,batch,batch,259,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:345,integrability,batch,batch-ordered,345,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:168,interoperability,integr,integration,168,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:168,modifiability,integr,integration,168,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:498,modifiability,variab,variables,498,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:203,performance,batch,batch,203,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:259,performance,batch,batch,259,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:345,performance,batch,batch-ordered,345,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:168,reliability,integr,integration,168,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:168,security,integr,integration,168,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/issues/1588:168,testability,integr,integration,168,"Hmm, you're right. I think it must have been the that the ordering of cells in the `adata` object was also non-random. We had this quite a bit in the benchmarking data integration project while plotting batch. In several methods (e.g., scanorama), individual batch anndata objects are concatenated to generate the final output, which results in batch-ordered anndata objects. . Maybe instead of just having `sort_order=False` it would be better to have randomized ordering for plotting categorical variables? Unless it is an ordered categorical I guess.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1588
https://github.com/scverse/scanpy/pull/1589:109,deployability,version,version,109,I am investigating what could be done now to fix ingest. I really don't like requiring a different and older version of umap for ingest.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:109,integrability,version,version,109,I am investigating what could be done now to fix ingest. I really don't like requiring a different and older version of umap for ingest.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:109,modifiability,version,version,109,I am investigating what could be done now to fix ingest. I really don't like requiring a different and older version of umap for ingest.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:50,security,loss,loss,50,"Ideally yes, but it would be nice to minimize the loss of functionality in one environment. I think a short term, sorta hacky, solution could be mimicking `NNDescent.__getstate__`/ `NNDescent.__setstate__`. Longer term I think we could try to get something in `pynndescent` like initialization from a prebuilt graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:120,security,hack,hacky,120,"Ideally yes, but it would be nice to minimize the loss of functionality in one environment. I think a short term, sorta hacky, solution could be mimicking `NNDescent.__getstate__`/ `NNDescent.__setstate__`. Longer term I think we could try to get something in `pynndescent` like initialization from a prebuilt graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:37,usability,minim,minimize,37,"Ideally yes, but it would be nice to minimize the loss of functionality in one environment. I think a short term, sorta hacky, solution could be mimicking `NNDescent.__getstate__`/ `NNDescent.__setstate__`. Longer term I think we could try to get something in `pynndescent` like initialization from a prebuilt graph.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:143,deployability,releas,release,143,"@ivirshup i see you opened the issue about graph initialization, though i haven't checked yet the things that were added. Do we wait for a new release of pynndescent and use the new features or we try to fix ingest with the current version of pynndescent (i have some code already that partially works)? upd: ah, i see `init_graph` should work without hacks now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:232,deployability,version,version,232,"@ivirshup i see you opened the issue about graph initialization, though i haven't checked yet the things that were added. Do we wait for a new release of pynndescent and use the new features or we try to fix ingest with the current version of pynndescent (i have some code already that partially works)? upd: ah, i see `init_graph` should work without hacks now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:224,energy efficiency,current,current,224,"@ivirshup i see you opened the issue about graph initialization, though i haven't checked yet the things that were added. Do we wait for a new release of pynndescent and use the new features or we try to fix ingest with the current version of pynndescent (i have some code already that partially works)? upd: ah, i see `init_graph` should work without hacks now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:232,integrability,version,version,232,"@ivirshup i see you opened the issue about graph initialization, though i haven't checked yet the things that were added. Do we wait for a new release of pynndescent and use the new features or we try to fix ingest with the current version of pynndescent (i have some code already that partially works)? upd: ah, i see `init_graph` should work without hacks now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:232,modifiability,version,version,232,"@ivirshup i see you opened the issue about graph initialization, though i haven't checked yet the things that were added. Do we wait for a new release of pynndescent and use the new features or we try to fix ingest with the current version of pynndescent (i have some code already that partially works)? upd: ah, i see `init_graph` should work without hacks now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:352,security,hack,hacks,352,"@ivirshup i see you opened the issue about graph initialization, though i haven't checked yet the things that were added. Do we wait for a new release of pynndescent and use the new features or we try to fix ingest with the current version of pynndescent (i have some code already that partially works)? upd: ah, i see `init_graph` should work without hacks now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:693,availability,down,downside,693,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:27,deployability,releas,release,27,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:390,deployability,releas,release,390,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:609,deployability,releas,release,609,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:844,deployability,version,version,844,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:844,integrability,version,version,844,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:742,modifiability,interm,intermediate,742,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:844,modifiability,version,version,844,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:371,safety,test,test,371,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:105,security,hack,hacky,105,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:143,security,hack,hacky,143,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:523,security,hack,hacky,523,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:721,security,hack,hacky,721,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:371,testability,test,test,371,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:430,usability,support,support,430,"Since we don't know when a release of `pynndescent` will go out, I think it's fine to keep this a little hacky for now. I think it can be less hacky than now doing something like this:. ```python. from_init = pynndescent.NNDescent(train, n_neighbors=15, init_graph=indices). from_init._rp_forest = rp_forest. query_indices_init, query_distances_init = from_scratch.query(test). ```. Once a release of pynndescent comes out we can support doing it the proper way. . I'd say it's up to you whether you want to have the kinda hacky solution or not. I definitely don't want UMAP to be pinned to below 0.5 when we release 1.7 proper, and it would be good for ingest to work with UMAP 0.5. The only downside I see to the kinda hacky solution as an intermediate is that you're fixing it twice. I don't think it'll be hard to go from this to the clean version however. -------------------------. I haven't looked into what needs to happen for the UMAP embedding transfer stuff to work. Is that pretty straight forward?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/pull/1589:77,usability,support,support,77,"Are you happy with me to merge this as is, then you can open a PR for ingest support?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1589
https://github.com/scverse/scanpy/issues/1591:0,availability,Ping,Ping,0,Ping @fidelram,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:48,availability,error,error,48,Thanks for the nice report submission. The code error is caused by the categories being integers when the code expect an 'str'. This is an easy fix. . The mapping of labels to color being different when colors are not in `adata.uns` is because the mapping is not saved (Fig 2 and 3). It is also an easy fix but requires the modification of `adata.uns` to save the colors. This is already done in `sc.pl.embedding` so should be OK to do but I would like to know @ivirshup opinion. For Fig 1. when there are more cells the problem is not there or maybe is simply not visible But I have some idea on how to address it. For Fig 6 I really don't know. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:27,integrability,sub,submission,27,Thanks for the nice report submission. The code error is caused by the categories being integers when the code expect an 'str'. This is an easy fix. . The mapping of labels to color being different when colors are not in `adata.uns` is because the mapping is not saved (Fig 2 and 3). It is also an easy fix but requires the modification of `adata.uns` to save the colors. This is already done in `sc.pl.embedding` so should be OK to do but I would like to know @ivirshup opinion. For Fig 1. when there are more cells the problem is not there or maybe is simply not visible But I have some idea on how to address it. For Fig 6 I really don't know. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:48,performance,error,error,48,Thanks for the nice report submission. The code error is caused by the categories being integers when the code expect an 'str'. This is an easy fix. . The mapping of labels to color being different when colors are not in `adata.uns` is because the mapping is not saved (Fig 2 and 3). It is also an easy fix but requires the modification of `adata.uns` to save the colors. This is already done in `sc.pl.embedding` so should be OK to do but I would like to know @ivirshup opinion. For Fig 1. when there are more cells the problem is not there or maybe is simply not visible But I have some idea on how to address it. For Fig 6 I really don't know. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:48,safety,error,error,48,Thanks for the nice report submission. The code error is caused by the categories being integers when the code expect an 'str'. This is an easy fix. . The mapping of labels to color being different when colors are not in `adata.uns` is because the mapping is not saved (Fig 2 and 3). It is also an easy fix but requires the modification of `adata.uns` to save the colors. This is already done in `sc.pl.embedding` so should be OK to do but I would like to know @ivirshup opinion. For Fig 1. when there are more cells the problem is not there or maybe is simply not visible But I have some idea on how to address it. For Fig 6 I really don't know. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:324,security,modif,modification,324,Thanks for the nice report submission. The code error is caused by the categories being integers when the code expect an 'str'. This is an easy fix. . The mapping of labels to color being different when colors are not in `adata.uns` is because the mapping is not saved (Fig 2 and 3). It is also an easy fix but requires the modification of `adata.uns` to save the colors. This is already done in `sc.pl.embedding` so should be OK to do but I would like to know @ivirshup opinion. For Fig 1. when there are more cells the problem is not there or maybe is simply not visible But I have some idea on how to address it. For Fig 6 I really don't know. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:554,testability,simpl,simply,554,Thanks for the nice report submission. The code error is caused by the categories being integers when the code expect an 'str'. This is an easy fix. . The mapping of labels to color being different when colors are not in `adata.uns` is because the mapping is not saved (Fig 2 and 3). It is also an easy fix but requires the modification of `adata.uns` to save the colors. This is already done in `sc.pl.embedding` so should be OK to do but I would like to know @ivirshup opinion. For Fig 1. when there are more cells the problem is not there or maybe is simply not visible But I have some idea on how to address it. For Fig 6 I really don't know. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:48,usability,error,error,48,Thanks for the nice report submission. The code error is caused by the categories being integers when the code expect an 'str'. This is an easy fix. . The mapping of labels to color being different when colors are not in `adata.uns` is because the mapping is not saved (Fig 2 and 3). It is also an easy fix but requires the modification of `adata.uns` to save the colors. This is already done in `sc.pl.embedding` so should be OK to do but I would like to know @ivirshup opinion. For Fig 1. when there are more cells the problem is not there or maybe is simply not visible But I have some idea on how to address it. For Fig 6 I really don't know. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:554,usability,simpl,simply,554,Thanks for the nice report submission. The code error is caused by the categories being integers when the code expect an 'str'. This is an easy fix. . The mapping of labels to color being different when colors are not in `adata.uns` is because the mapping is not saved (Fig 2 and 3). It is also an easy fix but requires the modification of `adata.uns` to save the colors. This is already done in `sc.pl.embedding` so should be OK to do but I would like to know @ivirshup opinion. For Fig 1. when there are more cells the problem is not there or maybe is simply not visible But I have some idea on how to address it. For Fig 6 I really don't know. .,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:310,deployability,log,logic,310,> It is also an easy fix but requires the modification of adata.uns to save the colors. This seems reasonable since we already do it. Can you make sure to use the same `_get_palette` function for this? Maybe that should be moved out of `scatterplots.py` to somewhere more general? We should probably have some logic for not saving the colors if the object is a view. Would have to think about this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:310,safety,log,logic,310,> It is also an easy fix but requires the modification of adata.uns to save the colors. This seems reasonable since we already do it. Can you make sure to use the same `_get_palette` function for this? Maybe that should be moved out of `scatterplots.py` to somewhere more general? We should probably have some logic for not saving the colors if the object is a view. Would have to think about this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:42,security,modif,modification,42,> It is also an easy fix but requires the modification of adata.uns to save the colors. This seems reasonable since we already do it. Can you make sure to use the same `_get_palette` function for this? Maybe that should be moved out of `scatterplots.py` to somewhere more general? We should probably have some logic for not saving the colors if the object is a view. Would have to think about this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:310,security,log,logic,310,> It is also an easy fix but requires the modification of adata.uns to save the colors. This seems reasonable since we already do it. Can you make sure to use the same `_get_palette` function for this? Maybe that should be moved out of `scatterplots.py` to somewhere more general? We should probably have some logic for not saving the colors if the object is a view. Would have to think about this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1591:310,testability,log,logic,310,> It is also an easy fix but requires the modification of adata.uns to save the colors. This seems reasonable since we already do it. Can you make sure to use the same `_get_palette` function for this? Maybe that should be moved out of `scatterplots.py` to somewhere more general? We should probably have some logic for not saving the colors if the object is a view. Would have to think about this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1591
https://github.com/scverse/scanpy/issues/1599:59,deployability,contain,contain,59,"Hi @Pawan291,. It looks like your `adata.var_names` do not contain the cell cycle genes you are testing for. Is your dataset human or mouse? Could you just print `adata.var_names[:10]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:96,safety,test,testing,96,"Hi @Pawan291,. It looks like your `adata.var_names` do not contain the cell cycle genes you are testing for. Is your dataset human or mouse? Could you just print `adata.var_names[:10]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:96,testability,test,testing,96,"Hi @Pawan291,. It looks like your `adata.var_names` do not contain the cell cycle genes you are testing for. Is your dataset human or mouse? Could you just print `adata.var_names[:10]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:134,usability,mous,mouse,134,"Hi @Pawan291,. It looks like your `adata.var_names` do not contain the cell cycle genes you are testing for. Is your dataset human or mouse? Could you just print `adata.var_names[:10]`?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:37,usability,mous,mouse,37,"Hi@Lucky MD,. the data sets are from mouse. Here is the output of the command adata.var_names[:10]. Index(['XKR4', 'GM37180', 'GM37363', 'GM37686', 'GM37329', 'GM38148',. 'GM19938', 'SOX17', 'GM37587', 'GM22307'],. dtype='object')",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:70,usability,command,command,70,"Hi@Lucky MD,. the data sets are from mouse. Here is the output of the command adata.var_names[:10]. Index(['XKR4', 'GM37180', 'GM37363', 'GM37686', 'GM37329', 'GM38148',. 'GM19938', 'SOX17', 'GM37587', 'GM22307'],. dtype='object')",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:205,availability,error,error,205,"So the format seems to be correct. Do you have the genes that are apparently missing in your object? You can check this for all the values that are mentioned in the `KeyError` output at the bottom of your error report. For example, is this statement `True`? `'USP1' in [str(i) for i in adata.var_names]`. If not, your `adata` object might just be missing the cell cycle genes you are looking for. How many genes are in your object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:240,availability,state,statement,240,"So the format seems to be correct. Do you have the genes that are apparently missing in your object? You can check this for all the values that are mentioned in the `KeyError` output at the bottom of your error report. For example, is this statement `True`? `'USP1' in [str(i) for i in adata.var_names]`. If not, your `adata` object might just be missing the cell cycle genes you are looking for. How many genes are in your object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:240,integrability,state,statement,240,"So the format seems to be correct. Do you have the genes that are apparently missing in your object? You can check this for all the values that are mentioned in the `KeyError` output at the bottom of your error report. For example, is this statement `True`? `'USP1' in [str(i) for i in adata.var_names]`. If not, your `adata` object might just be missing the cell cycle genes you are looking for. How many genes are in your object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:7,interoperability,format,format,7,"So the format seems to be correct. Do you have the genes that are apparently missing in your object? You can check this for all the values that are mentioned in the `KeyError` output at the bottom of your error report. For example, is this statement `True`? `'USP1' in [str(i) for i in adata.var_names]`. If not, your `adata` object might just be missing the cell cycle genes you are looking for. How many genes are in your object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:205,performance,error,error,205,"So the format seems to be correct. Do you have the genes that are apparently missing in your object? You can check this for all the values that are mentioned in the `KeyError` output at the bottom of your error report. For example, is this statement `True`? `'USP1' in [str(i) for i in adata.var_names]`. If not, your `adata` object might just be missing the cell cycle genes you are looking for. How many genes are in your object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:205,safety,error,error,205,"So the format seems to be correct. Do you have the genes that are apparently missing in your object? You can check this for all the values that are mentioned in the `KeyError` output at the bottom of your error report. For example, is this statement `True`? `'USP1' in [str(i) for i in adata.var_names]`. If not, your `adata` object might just be missing the cell cycle genes you are looking for. How many genes are in your object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:205,usability,error,error,205,"So the format seems to be correct. Do you have the genes that are apparently missing in your object? You can check this for all the values that are mentioned in the `KeyError` output at the bottom of your error report. For example, is this statement `True`? `'USP1' in [str(i) for i in adata.var_names]`. If not, your `adata` object might just be missing the cell cycle genes you are looking for. How many genes are in your object?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:78,testability,regress,regress,78,Thank you very much it works. now I am able to calculate cell cycle score and regress out these genes from my datasets :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:343,availability,error,error,343,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:350,availability,Error,Error,350,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:965,deployability,modul,module,965,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1151,deployability,version,versions,1151,"previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._nor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1593,deployability,version,versions,1593,"RSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1992,deployability,version,versions,1992,"ex). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ', 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11', 'PER3', 'ERRFI1', 'CA6', 'SLC2A5', 'G",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:2234,deployability,version,versions,2234,"cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ', 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11', 'PER3', 'ERRFI1', 'CA6', 'SLC2A5', 'GPR157', 'PIK3CD-AS2', 'CTNNBIP1', 'KIF1B', 'PEX14', 'AL109811.1', 'FBXO2', 'TNFRSF1B', 'DHRS3', 'KAZN', 'FHAD1', 'PADI4', 'EIF4G3', 'NBPF3', 'CDC42-AS1', 'C1QA', 'C1QB', 'AL031428.1', 'ZNF436', 'TCEA3', 'ID3', 'MDS2', 'ELOA-AS1', 'CNR2', 'IFN",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:2529,deployability,version,versions,2529,"_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ', 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11', 'PER3', 'ERRFI1', 'CA6', 'SLC2A5', 'GPR157', 'PIK3CD-AS2', 'CTNNBIP1', 'KIF1B', 'PEX14', 'AL109811.1', 'FBXO2', 'TNFRSF1B', 'DHRS3', 'KAZN', 'FHAD1', 'PADI4', 'EIF4G3', 'NBPF3', 'CDC42-AS1', 'C1QA', 'C1QB', 'AL031428.1', 'ZNF436', 'TCEA3', 'ID3', 'MDS2', 'ELOA-AS1', 'CNR2', 'IFNLR1', 'STPG1', 'NIPAL3', 'RCAN3AS', 'RCAN3', 'NCMAP-DT', 'CLIC4', 'RHD', 'MAN1C1', 'AL031280.1', 'STMN1', 'PAFAH2', 'CRYBG2', 'ZNF683', 'AHDC1', 'SMPDL3B', 'PTAFR', 'LINC01715', 'MECR', 'LINC01226', 'YARS', 'AZIN2', 'TRIM62', 'CSF3R', 'LINC01137', 'INPP5B', 'AL139260.1', 'RHBDL2', 'HPCAL4', 'LI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:356,integrability,messag,message,356,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1151,integrability,version,versions,1151,"previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._nor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1593,integrability,version,versions,1593,"RSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1992,integrability,version,versions,1992,"ex). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ', 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11', 'PER3', 'ERRFI1', 'CA6', 'SLC2A5', 'G",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:2234,integrability,version,versions,2234,"cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ', 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11', 'PER3', 'ERRFI1', 'CA6', 'SLC2A5', 'GPR157', 'PIK3CD-AS2', 'CTNNBIP1', 'KIF1B', 'PEX14', 'AL109811.1', 'FBXO2', 'TNFRSF1B', 'DHRS3', 'KAZN', 'FHAD1', 'PADI4', 'EIF4G3', 'NBPF3', 'CDC42-AS1', 'C1QA', 'C1QB', 'AL031428.1', 'ZNF436', 'TCEA3', 'ID3', 'MDS2', 'ELOA-AS1', 'CNR2', 'IFN",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:2529,integrability,version,versions,2529,"_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ', 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11', 'PER3', 'ERRFI1', 'CA6', 'SLC2A5', 'GPR157', 'PIK3CD-AS2', 'CTNNBIP1', 'KIF1B', 'PEX14', 'AL109811.1', 'FBXO2', 'TNFRSF1B', 'DHRS3', 'KAZN', 'FHAD1', 'PADI4', 'EIF4G3', 'NBPF3', 'CDC42-AS1', 'C1QA', 'C1QB', 'AL031428.1', 'ZNF436', 'TCEA3', 'ID3', 'MDS2', 'ELOA-AS1', 'CNR2', 'IFNLR1', 'STPG1', 'NIPAL3', 'RCAN3AS', 'RCAN3', 'NCMAP-DT', 'CLIC4', 'RHD', 'MAN1C1', 'AL031280.1', 'STMN1', 'PAFAH2', 'CRYBG2', 'ZNF683', 'AHDC1', 'SMPDL3B', 'PTAFR', 'LINC01715', 'MECR', 'LINC01226', 'YARS', 'AZIN2', 'TRIM62', 'CSF3R', 'LINC01137', 'INPP5B', 'AL139260.1', 'RHBDL2', 'HPCAL4', 'LI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:356,interoperability,messag,message,356,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:965,modifiability,modul,module,965,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1151,modifiability,version,versions,1151,"previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._nor",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1185,modifiability,pac,packages,1185,"s and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 #",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1593,modifiability,version,versions,1593,"RSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1627,modifiability,pac,packages,1627,"PANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1992,modifiability,version,versions,1992,"ex). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ', 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11', 'PER3', 'ERRFI1', 'CA6', 'SLC2A5', 'G",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:2026,modifiability,pac,packages,2026,"s_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ', 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11', 'PER3', 'ERRFI1', 'CA6', 'SLC2A5', 'GPR157', 'PIK3CD-AS2', 'CTNNBIP1', ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:2234,modifiability,version,versions,2234,"cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ', 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11', 'PER3', 'ERRFI1', 'CA6', 'SLC2A5', 'GPR157', 'PIK3CD-AS2', 'CTNNBIP1', 'KIF1B', 'PEX14', 'AL109811.1', 'FBXO2', 'TNFRSF1B', 'DHRS3', 'KAZN', 'FHAD1', 'PADI4', 'EIF4G3', 'NBPF3', 'CDC42-AS1', 'C1QA', 'C1QB', 'AL031428.1', 'ZNF436', 'TCEA3', 'ID3', 'MDS2', 'ELOA-AS1', 'CNR2', 'IFN",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:2268,modifiability,pac,packages,2268,"es, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ', 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11', 'PER3', 'ERRFI1', 'CA6', 'SLC2A5', 'GPR157', 'PIK3CD-AS2', 'CTNNBIP1', 'KIF1B', 'PEX14', 'AL109811.1', 'FBXO2', 'TNFRSF1B', 'DHRS3', 'KAZN', 'FHAD1', 'PADI4', 'EIF4G3', 'NBPF3', 'CDC42-AS1', 'C1QA', 'C1QB', 'AL031428.1', 'ZNF436', 'TCEA3', 'ID3', 'MDS2', 'ELOA-AS1', 'CNR2', 'IFNLR1', 'STPG1', 'NIPAL3', 'RCAN3AS'",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:2529,modifiability,version,versions,2529,"_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ', 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11', 'PER3', 'ERRFI1', 'CA6', 'SLC2A5', 'GPR157', 'PIK3CD-AS2', 'CTNNBIP1', 'KIF1B', 'PEX14', 'AL109811.1', 'FBXO2', 'TNFRSF1B', 'DHRS3', 'KAZN', 'FHAD1', 'PADI4', 'EIF4G3', 'NBPF3', 'CDC42-AS1', 'C1QA', 'C1QB', 'AL031428.1', 'ZNF436', 'TCEA3', 'ID3', 'MDS2', 'ELOA-AS1', 'CNR2', 'IFNLR1', 'STPG1', 'NIPAL3', 'RCAN3AS', 'RCAN3', 'NCMAP-DT', 'CLIC4', 'RHD', 'MAN1C1', 'AL031280.1', 'STMN1', 'PAFAH2', 'CRYBG2', 'ZNF683', 'AHDC1', 'SMPDL3B', 'PTAFR', 'LINC01715', 'MECR', 'LINC01226', 'YARS', 'AZIN2', 'TRIM62', 'CSF3R', 'LINC01137', 'INPP5B', 'AL139260.1', 'RHBDL2', 'HPCAL4', 'LI",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:2563,modifiability,pac,packages,2563,"size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ', 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11', 'PER3', 'ERRFI1', 'CA6', 'SLC2A5', 'GPR157', 'PIK3CD-AS2', 'CTNNBIP1', 'KIF1B', 'PEX14', 'AL109811.1', 'FBXO2', 'TNFRSF1B', 'DHRS3', 'KAZN', 'FHAD1', 'PADI4', 'EIF4G3', 'NBPF3', 'CDC42-AS1', 'C1QA', 'C1QB', 'AL031428.1', 'ZNF436', 'TCEA3', 'ID3', 'MDS2', 'ELOA-AS1', 'CNR2', 'IFNLR1', 'STPG1', 'NIPAL3', 'RCAN3AS', 'RCAN3', 'NCMAP-DT', 'CLIC4', 'RHD', 'MAN1C1', 'AL031280.1', 'STMN1', 'PAFAH2', 'CRYBG2', 'ZNF683', 'AHDC1', 'SMPDL3B', 'PTAFR', 'LINC01715', 'MECR', 'LINC01226', 'YARS', 'AZIN2', 'TRIM62', 'CSF3R', 'LINC01137', 'INPP5B', 'AL139260.1', 'RHBDL2', 'HPCAL4', 'LINC02811', 'AL603839.3', 'SLFNL1-AS",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:18375,modifiability,PAC,PACRG,18375,"'KLHL32', 'MMS22L', 'COQ3', 'USP45', 'TSTD3', 'AL096794.1', 'PRDM1', 'RTN4IP1', 'QRSL1', 'SOBP', 'SCML4', 'AFG1L', 'FOXO3', 'ARMC2', 'SESN1', 'AK9', 'METTL24', 'SLC16A10', 'AL360227.1', 'TRAF3IP2-AS1', 'FAM229B', 'MARCKS', 'NT5DC1', 'COL10A1', 'CALHM6', 'AL137009.1', 'FAM184A', 'MAN1A1', 'TBC1D32', 'PKIB', 'NKAIN2', 'RNF217-AS1', 'TRMT11', 'SOGA3', 'THEMIS', 'PTPRK', 'AL590006.1', 'AL356124.1', 'AL355581.1', 'SAMD3', 'TMEM200A', 'EPB41L2', 'AKAP7', 'ARG1', 'LINC01013', 'MOXD1', 'STX7', 'VNN1', 'VNN3', 'SGK1', 'ALDH8A1', 'MYB', 'AHI1', 'PDE7B', 'AL360178.1', 'AL138828.1', 'MAP7', 'PEX7', 'WAKMAR2', 'TNFAIP3', 'NHSL1', 'TXLNB', 'AL592429.2', 'AL035446.2', 'LINC01277', 'AIG1', 'PHACTR2', 'PLAGL1', 'STX11', 'UTRN', 'EPM2A', 'AL356599.1', 'STXBP5-AS1', 'SASH1', 'UST', 'ZC3H12D', 'PLEKHG1', 'MTHFD1L', 'AL138733.1', 'RMND1', 'CCDC170', 'ESR1', 'SYNE1', 'RGS17', 'OPRM1', 'AL589693.1', 'AL512658.2', 'ZDHHC14', 'SYNJ2', 'AL139330.1', 'SYTL3', 'RSPH3', 'AL078604.4', 'SOD2', 'AGPAT4', 'PRKN', 'PACRG', 'AL078602.1', 'RPS6KA2', 'Z94721.1', 'FGFR1OP', 'CCR6', 'AFDN', 'WDR27', 'PRKAR1B', 'CARD11', 'AC024028.1', 'SDK1', 'CYTH3', 'AC006042.2', 'GLCCI1', 'ICA1', 'SCIN', 'AC011287.1', 'CRPPA', 'AC073333.1', 'TSPAN13', 'AHR', 'AC073332.1', 'HDAC9', 'MACC1', 'DNAH11', 'CDCA7L', 'RAPGEF5', 'STEAP1B', 'AC002480.2', 'IL6', 'KLHL7', 'CCDC126', 'MPP6', 'OSBPL3', 'C7orf31', 'AC005165.1', 'SKAP2', 'HIBADH', 'JAZF1', 'CREB5', 'CPVL', 'PLEKHA8', 'MINDY4', 'RP9', 'BBS9', 'EEPD1', 'AOAH', 'ELMO1-AS1', 'EPDR1', 'TRGC2', 'TRGC1', 'TRGV9', 'TRG-AS1', 'AMPH', 'POU6F2', 'YAE1', 'SUGCT', 'URGCP', 'IGFBP3', 'AC073115.1', 'TNS3', 'PKD1L1', 'ZPBP', 'GRB10', 'LANCL2', 'ZNF713', 'PSPH', 'LINC02848', 'ZNF138', 'AC073349.4', 'VKORC1L1', 'AC068533.3', 'TPST1', 'LINC00174', 'AC006001.2', 'RABGEF1', 'AUTS2', 'TYW1B', 'CLIP2', 'NCF1', 'CASTOR2', 'SPDYE5', 'HIP1', 'SSC4D', 'FGL2', 'PTPN12', 'MAGI2', 'GNAI1', 'CD36', 'SEMA3C', 'SEMA3A', 'KIAA1324L', 'CROT', 'ABCB4', 'ABCB1', 'ADAM22', 'CLDN12', 'CDK14', 'MTERF1', 'CD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:56542,modifiability,PAC,PACRG,56542,"'KLHL32', 'MMS22L', 'COQ3', 'USP45', 'TSTD3', 'AL096794.1', 'PRDM1', 'RTN4IP1', 'QRSL1', 'SOBP', 'SCML4', 'AFG1L', 'FOXO3', 'ARMC2', 'SESN1', 'AK9', 'METTL24', 'SLC16A10', 'AL360227.1', 'TRAF3IP2-AS1', 'FAM229B', 'MARCKS', 'NT5DC1', 'COL10A1', 'CALHM6', 'AL137009.1', 'FAM184A', 'MAN1A1', 'TBC1D32', 'PKIB', 'NKAIN2', 'RNF217-AS1', 'TRMT11', 'SOGA3', 'THEMIS', 'PTPRK', 'AL590006.1', 'AL356124.1', 'AL355581.1', 'SAMD3', 'TMEM200A', 'EPB41L2', 'AKAP7', 'ARG1', 'LINC01013', 'MOXD1', 'STX7', 'VNN1', 'VNN3', 'SGK1', 'ALDH8A1', 'MYB', 'AHI1', 'PDE7B', 'AL360178.1', 'AL138828.1', 'MAP7', 'PEX7', 'WAKMAR2', 'TNFAIP3', 'NHSL1', 'TXLNB', 'AL592429.2', 'AL035446.2', 'LINC01277', 'AIG1', 'PHACTR2', 'PLAGL1', 'STX11', 'UTRN', 'EPM2A', 'AL356599.1', 'STXBP5-AS1', 'SASH1', 'UST', 'ZC3H12D', 'PLEKHG1', 'MTHFD1L', 'AL138733.1', 'RMND1', 'CCDC170', 'ESR1', 'SYNE1', 'RGS17', 'OPRM1', 'AL589693.1', 'AL512658.2', 'ZDHHC14', 'SYNJ2', 'AL139330.1', 'SYTL3', 'RSPH3', 'AL078604.4', 'SOD2', 'AGPAT4', 'PRKN', 'PACRG', 'AL078602.1', 'RPS6KA2', 'Z94721.1', 'FGFR1OP', 'CCR6', 'AFDN', 'WDR27', 'PRKAR1B', 'CARD11', 'AC024028.1', 'SDK1', 'CYTH3', 'AC006042.2', 'GLCCI1', 'ICA1', 'SCIN', 'AC011287.1', 'CRPPA', 'AC073333.1', 'TSPAN13', 'AHR', 'AC073332.1', 'HDAC9', 'MACC1', 'DNAH11', 'CDCA7L', 'RAPGEF5', 'STEAP1B', 'AC002480.2', 'IL6', 'KLHL7', 'CCDC126', 'MPP6', 'OSBPL3', 'C7orf31', 'AC005165.1', 'SKAP2', 'HIBADH', 'JAZF1', 'CREB5', 'CPVL', 'PLEKHA8', 'MINDY4', 'RP9', 'BBS9', 'EEPD1', 'AOAH', 'ELMO1-AS1', 'EPDR1', 'TRGC2', 'TRGC1', 'TRGV9', 'TRG-AS1', 'AMPH', 'POU6F2', 'YAE1', 'SUGCT', 'URGCP', 'IGFBP3', 'AC073115.1', 'TNS3', 'PKD1L1', 'ZPBP', 'GRB10', 'LANCL2', 'ZNF713', 'PSPH', 'LINC02848', 'ZNF138', 'AC073349.4', 'VKORC1L1', 'AC068533.3', 'TPST1', 'LINC00174', 'AC006001.2', 'RABGEF1', 'AUTS2', 'TYW1B', 'CLIP2', 'NCF1', 'CASTOR2', 'SPDYE5', 'HIP1', 'SSC4D', 'FGL2', 'PTPN12', 'MAGI2', 'GNAI1', 'CD36', 'SEMA3C', 'SEMA3A', 'KIAA1324L', 'CROT', 'ABCB4', 'ABCB1', 'ADAM22', 'CLDN12', 'CDK14', 'MTERF1', 'CD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:343,performance,error,error,343,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:350,performance,Error,Error,350,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:343,safety,error,error,343,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:350,safety,Error,Error,350,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:938,safety,input,input-,938,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:965,safety,modul,module,965,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:2796,safety,valid,valid,2796,"_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 98 if np.any(positions < 0):. 99 not_found = indexer[positions < 0]. --> 100 raise KeyError(. 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". KeyError: ""Values ['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ', 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11', 'PER3', 'ERRFI1', 'CA6', 'SLC2A5', 'GPR157', 'PIK3CD-AS2', 'CTNNBIP1', 'KIF1B', 'PEX14', 'AL109811.1', 'FBXO2', 'TNFRSF1B', 'DHRS3', 'KAZN', 'FHAD1', 'PADI4', 'EIF4G3', 'NBPF3', 'CDC42-AS1', 'C1QA', 'C1QB', 'AL031428.1', 'ZNF436', 'TCEA3', 'ID3', 'MDS2', 'ELOA-AS1', 'CNR2', 'IFNLR1', 'STPG1', 'NIPAL3', 'RCAN3AS', 'RCAN3', 'NCMAP-DT', 'CLIC4', 'RHD', 'MAN1C1', 'AL031280.1', 'STMN1', 'PAFAH2', 'CRYBG2', 'ZNF683', 'AHDC1', 'SMPDL3B', 'PTAFR', 'LINC01715', 'MECR', 'LINC01226', 'YARS', 'AZIN2', 'TRIM62', 'CSF3R', 'LINC01137', 'INPP5B', 'AL139260.1', 'RHBDL2', 'HPCAL4', 'LINC02811', 'AL603839.3', 'SLFNL1-AS1', 'SCMH1', 'HIVEP3', 'CCDC30', 'TMEM269', 'AL139289.2', 'KDM4A-AS1', 'ST3GAL3', 'AL451062.1', 'KIF2C', 'EIF2B3', 'ZSWIM5', 'TESK2', 'MAST2', 'PIK3R3', 'STIL', 'LINC01389', 'AGBL4', 'AC104170.1', 'ZFYVE9', 'SHISAL2A', 'COA7', 'AL60",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:79186,safety,valid,valid,79186,"B', 'HDHD5-AS1', 'MICAL3', 'TUBA8', 'AC008079.1', 'CLTCL1', 'YPEL1', 'IGLV10-54', 'AC245060.5', 'IGLC1', 'IGLC2', 'IGLC3', 'RSPH14', 'VPREB3', 'DERL3', 'DDTL', 'UPB1', 'SGSM1', 'KIAA1671', 'GRK3', 'SEZ6L', 'MIAT', 'LINC01422', 'TTC28-AS1', 'TTC28', 'CHEK2', 'HSCB', 'Z93930.2', 'ZNRF3', 'NIPSNAP1', 'RNF215', 'SEC14L2', 'OSBP2', 'SELENOM', 'LIMK2', 'PIK3IP1', 'PIK3IP1-AS1', 'EIF4ENIF1', 'SLC5A4-AS1', 'SYN3', 'LARGE1', 'LARGE-AS1', 'HMOX1', 'IL2RB', 'LGALS1', 'KDELR3', 'CBY1', 'AL021707.2', 'NPTXR', 'APOBEC3A', 'APOBEC3G', 'PDGFB', 'AL031590.1', 'MGAT3', 'CACNA1I', 'GRAP2', 'AL022238.2', 'SLC25A17', 'XPNPEP3', 'CCDC134', 'TNFRSF13C', 'WBP2NL', 'NDUFA6-DT', 'TTLL1', 'EFCAB6', 'PRR5', 'SMC1B', 'PPARA', 'TTC38', 'CELSR1', 'LINC01644', 'TYMP', 'CHKB-DT', 'LINC00685', 'CSF2RA', 'AL683807.1', 'DHRSX', 'PUDP', 'STS', 'AC079142.1', 'CLCN4', 'MID1', 'AC073529.1', 'ARHGAP6', 'GPM6B', 'NHS', 'MAP7D2', 'CNKSR2', 'MBTPS2', 'PHEX', 'PTCHD1-AS', 'SAT1', 'APOO', 'CXorf58', 'POLA1', 'GK', 'GK-AS1', 'CYBB', 'AC108879.1', 'CASK', 'PPP1R2C', 'MIR222HG', 'LINC02595', 'SLC9A7', 'JADE3', 'ZNF81', 'AC231533.1', 'SYP', 'FOXP3', 'LINC01284', 'AC233976.1', 'AL158055.1', 'MAGED1', 'GPR173', 'TSPYL2', 'KANTR', 'AL050309.1', 'NBDY', 'FAAH2', 'LINC01278', 'MTMR8', 'ZC4H2', 'ZC3H12B', 'AL034397.3', 'OPHN1', 'EDA', 'GCNA', 'CXCR3', 'ERCC6L', 'HDAC8', 'NAP1L2', 'CHIC1', 'NEXMIF', 'ZDHHC15', 'MAGEE1', 'CYSLTR1', 'P2RY10', 'GPR174', 'ITM2A', 'HMGN5', 'HDX', 'APOOL', 'DIAPH2-AS1', 'ARMCX4', 'BEX5', 'ARMCX5-GPRASP2', 'AL035427.1', 'LINC00630', 'BEX2', 'BEX3', 'TMSB15B-AS1', 'RADX', 'MORC4', 'NUP62CL', 'FRMPD3', 'AL109946.1', 'VSIG1', 'TMEM164', 'AMMECR1', 'PAK3', 'TRPC5', 'XACT', 'DANT2', 'KLHL13', 'LONRF3', 'SLC25A43', 'TENM1', 'AL008633.1', 'SMARCA1', 'AL034405.1', 'ENOX2', 'FIRRE', 'RAP2C-AS1', 'MBNL3', 'GPC3', 'ZNF75D', 'FHL1', 'CD40LG', 'AL683813.1', 'FGF13', 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8', 'TMLHE-AS1', 'PRKY', 'UTY'], are not valid obs/ var names or indices."". ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:894,testability,Trace,Traceback,894,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:343,usability,error,error,343,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:350,usability,Error,Error,350,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:938,usability,input,input-,938,"Had same problem. It seems it tries to look for all gene names present in the anndata object,. rather than the cell cycle genes that are requested? I have previously checked that the s_genes and g2m_genes in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1201,usability,tool,tools,1201," in call. sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). are in the data. use_raw=False . makes it run without error. Error message below. Note that printout of gene names in data at the beginning matches the list of keyErrors it spits out. ```. Gene_names_in_data:. Index(['HES4', 'C1orf159', 'TNFRSF18', 'TNFRSF4', 'ATAD3C', 'PRKCZ',. 'AL365255.1', 'GPR153', 'TNFRSF25', 'DNAJC11',. ... 'MCF2', 'SPANXA2-OT1', 'AFF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve tw",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1643,usability,tool,tools,1643,"FF2', 'LINC00894', 'MAMLD1', 'PDZD4', 'F8',. 'TMLHE-AS1', 'PRKY', 'UTY'],. dtype='object', length=3658). calculating cell cycle phase. computing score 'S_score'. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-82-30815e6b6381> in <module>. 20 print(gdata.var.index). 21 . ---> 22 sc.tl.score_genes_cell_cycle(gdata, s_genes=s_found, g2m_genes=g2m_found). 23 . 24 gdata.obs[""cellcycle""] = gdata.obs[""phase""]. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes_cell_cycle(adata, s_genes, g2m_genes, copy, **kwargs). 247 ctrl_size = min(len(s_genes), len(g2m_genes)). 248 # add s-score. --> 249 score_genes(adata, gene_list=s_genes, score_name='S_score', ctrl_size=ctrl_size, **kwargs). 250 # add g2m-score. 251 score_genes(adata, gene_list=g2m_genes, score_name='G2M_score', ctrl_size=ctrl_size, **kwargs). ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/scanpy/tools/_score_genes.py in score_genes(adata, gene_list, ctrl_size, gene_pool, n_bins, score_name, random_state, copy, use_raw). 128 _adata = adata.raw if use_raw else adata. 129 . --> 130 _adata_subset = _adata[:, gene_pool] if len(gene_pool) < len(_adata.var_names) else _adata. 131 if issparse(_adata_subset.X):. 132 obs_avg = pd.Series(. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in __getitem__(self, index). 99 . 100 def __getitem__(self, index):. --> 101 oidx, vidx = self._normalize_indices(index). 102 . 103 # To preserve two dimensional shape. ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/raw.py in _normalize_indices(self, packed_index). 159 obs, var = unpack_index(packed_index). 160 obs = _normalize_index(obs, self._adata.obs_names). --> 161 var = _normalize_index(var, self.var_names). 162 return obs, var. 163 . ~/.pyenv/versions/6_tes/lib/python3.8/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 98 if np.any(p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:31650,usability,TIP,TIPIN,31650,", 'BCL11B', 'AL109767.1', 'AL160313.1', 'CYP46A1', 'AL157871.4', 'WARS', 'WDR25', 'LINC02320', 'TECPR2', 'TNFAIP2', 'PPP1R13B', 'TDRD9', 'PLD4', 'CLBA1', 'CRIP1', 'IGHA2', 'IGHG4', 'IGHG2', 'IGHGP', 'IGHA1', 'IGHG1', 'IGHG3', 'IGHD', 'IGHM', 'FAM30A', 'NIPA1', 'MKRN3', 'ATP10A', 'AC023449.2', 'APBA2', 'AC127522.1', 'ARHGAP11B', 'FAN1', 'OTUD7A', 'FMN1', 'RYR3', 'AVEN', 'CHRM5', 'AC021231.1', 'DPH6', 'DPH6-DT', 'C15orf41', 'AC013640.1', 'LINC02345', 'SPRED1', 'RASGRP1', 'LINC02694', 'AC013652.1', 'THBS1', 'FSIP1', 'AC091045.1', 'KNL1', 'AC020661.3', 'OIP5', 'MAPKBP1', 'AC018362.1', 'STARD9', 'ADAL', 'PPIP5K1', 'CATSPER2', 'WDR76', 'FRMD5', 'CASC4', 'PATL2', 'DUOX1', 'AC051619.5', 'SLC30A4', 'MYEF2', 'FAM227B', 'ATP8B4', 'GABPB1-AS1', 'AC021752.1', 'AC066613.2', 'DMXL2', 'GNB5', 'CERNA1', 'NEDD4', 'TEX9', 'AC090518.1', 'AC090517.5', 'LINC00926', 'ALDH1A2', 'AQP9', 'LIPC', 'MYO1E', 'RORA-AS1', 'RORA', 'TLN2', 'DAPK2', 'SNX22', 'ZNF609', 'MTFMT', 'PARP16', 'HACD3', 'SLC24A1', 'DIS3L', 'TIPIN', 'IQCH', 'IQCH-AS1', 'CORO2B', 'SPESP1', 'GLCE', 'MYO9A', 'AC022872.1', 'AC009690.2', 'BBS4', 'NEO1', 'UBL7-AS1', 'SCAMP5', 'NEIL1', 'FBXO22', 'AC027243.3', 'PEAK1', 'ABHD17C', 'CEMIP', 'CFAP161', 'TMC3-AS1', 'AC104041.1', 'EFL1', 'CPEB1', 'AC105339.3', 'FSD2', 'HOMER2', 'AC012291.3', 'KLHL25', 'ANPEP', 'AP3S2', 'CRTC3-AS1', 'BLM', 'AC104035.1', 'SLCO3A1', 'AC091078.1', 'MCTP2', 'AC009432.2', 'ARRDC4', 'IGF1R', 'LRRC28', 'AC090825.1', 'ADAMTS17', 'CERS3', 'LRRK1', 'TARSL2', 'HBA1', 'RAB11FIP3', 'AL023882.1', 'AC009041.2', 'IFT140', 'AL133297.2', 'JPT2', 'ABCA3', 'AC004034.1', 'MMP25-AS1', 'IL32', 'AC025283.2', 'CLUAP1', 'ADCY9', 'DNAJA3', 'C16orf71', 'PPL', 'ABAT', 'ATF7IP2', 'AC133065.1', 'CIITA', 'AC007220.1', 'AC099489.1', 'AC007216.4', 'TNFRSF17', 'CPPED1', 'MRTFB', 'AC130650.1', 'BMERB1', 'MYH11', 'XYLT1', 'SYT17', 'IQCK', 'ACSM3', 'DNAH3', 'AC008551.1', 'MOSMO', 'EEF2K', 'AC092338.3', 'CDR2', 'USP31', 'COG7', 'PALB2', 'AC008731.1', 'SLC5A11', 'LCMT1-AS1', 'NSMCE1-DT', 'IL4R',",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:69817,usability,TIP,TIPIN,69817,", 'BCL11B', 'AL109767.1', 'AL160313.1', 'CYP46A1', 'AL157871.4', 'WARS', 'WDR25', 'LINC02320', 'TECPR2', 'TNFAIP2', 'PPP1R13B', 'TDRD9', 'PLD4', 'CLBA1', 'CRIP1', 'IGHA2', 'IGHG4', 'IGHG2', 'IGHGP', 'IGHA1', 'IGHG1', 'IGHG3', 'IGHD', 'IGHM', 'FAM30A', 'NIPA1', 'MKRN3', 'ATP10A', 'AC023449.2', 'APBA2', 'AC127522.1', 'ARHGAP11B', 'FAN1', 'OTUD7A', 'FMN1', 'RYR3', 'AVEN', 'CHRM5', 'AC021231.1', 'DPH6', 'DPH6-DT', 'C15orf41', 'AC013640.1', 'LINC02345', 'SPRED1', 'RASGRP1', 'LINC02694', 'AC013652.1', 'THBS1', 'FSIP1', 'AC091045.1', 'KNL1', 'AC020661.3', 'OIP5', 'MAPKBP1', 'AC018362.1', 'STARD9', 'ADAL', 'PPIP5K1', 'CATSPER2', 'WDR76', 'FRMD5', 'CASC4', 'PATL2', 'DUOX1', 'AC051619.5', 'SLC30A4', 'MYEF2', 'FAM227B', 'ATP8B4', 'GABPB1-AS1', 'AC021752.1', 'AC066613.2', 'DMXL2', 'GNB5', 'CERNA1', 'NEDD4', 'TEX9', 'AC090518.1', 'AC090517.5', 'LINC00926', 'ALDH1A2', 'AQP9', 'LIPC', 'MYO1E', 'RORA-AS1', 'RORA', 'TLN2', 'DAPK2', 'SNX22', 'ZNF609', 'MTFMT', 'PARP16', 'HACD3', 'SLC24A1', 'DIS3L', 'TIPIN', 'IQCH', 'IQCH-AS1', 'CORO2B', 'SPESP1', 'GLCE', 'MYO9A', 'AC022872.1', 'AC009690.2', 'BBS4', 'NEO1', 'UBL7-AS1', 'SCAMP5', 'NEIL1', 'FBXO22', 'AC027243.3', 'PEAK1', 'ABHD17C', 'CEMIP', 'CFAP161', 'TMC3-AS1', 'AC104041.1', 'EFL1', 'CPEB1', 'AC105339.3', 'FSD2', 'HOMER2', 'AC012291.3', 'KLHL25', 'ANPEP', 'AP3S2', 'CRTC3-AS1', 'BLM', 'AC104035.1', 'SLCO3A1', 'AC091078.1', 'MCTP2', 'AC009432.2', 'ARRDC4', 'IGF1R', 'LRRC28', 'AC090825.1', 'ADAMTS17', 'CERS3', 'LRRK1', 'TARSL2', 'HBA1', 'RAB11FIP3', 'AL023882.1', 'AC009041.2', 'IFT140', 'AL133297.2', 'JPT2', 'ABCA3', 'AC004034.1', 'MMP25-AS1', 'IL32', 'AC025283.2', 'CLUAP1', 'ADCY9', 'DNAJA3', 'C16orf71', 'PPL', 'ABAT', 'ATF7IP2', 'AC133065.1', 'CIITA', 'AC007220.1', 'AC099489.1', 'AC007216.4', 'TNFRSF17', 'CPPED1', 'MRTFB', 'AC130650.1', 'BMERB1', 'MYH11', 'XYLT1', 'SYT17', 'IQCK', 'ACSM3', 'DNAH3', 'AC008551.1', 'MOSMO', 'EEF2K', 'AC092338.3', 'CDR2', 'USP31', 'COG7', 'PALB2', 'AC008731.1', 'SLC5A11', 'LCMT1-AS1', 'NSMCE1-DT', 'IL4R',",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:89,deployability,version,version,89,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:348,deployability,log,log-transformed,348,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:616,deployability,Log,Log-transformation,616,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:756,deployability,log,logged,756,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:81,energy efficiency,current,current,81,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:260,energy efficiency,estimat,estimate,260,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:89,integrability,version,version,89,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:352,integrability,transform,transformed,352,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:620,integrability,transform,transformation,620,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:352,interoperability,transform,transformed,352,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:620,interoperability,transform,transformation,620,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:928,interoperability,specif,specific,928,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:89,modifiability,version,version,89,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:647,modifiability,scal,scaling,647,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:672,performance,perform,performed,672,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:69,safety,risk,risk,69,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:348,safety,log,log-transformed,348,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:616,safety,Log,Log-transformation,616,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:756,safety,log,logged,756,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:69,security,risk,risk,69,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:348,security,log,log-transformed,348,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:616,security,Log,Log-transformation,616,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:756,security,log,logged,756,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:348,testability,log,log-transformed,348,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:616,testability,Log,Log-transformation,616,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:756,testability,log,logged,756,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:672,usability,perform,performed,672,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:984,usability,behavi,behaviour,984,"Hello,. For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. - `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). - However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1155,availability,state,states,1155,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1479,availability,operat,operates,1479,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1495,availability,slo,slot,1495,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1629,availability,operat,operating,1629,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:90,deployability,version,version,90,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:355,deployability,log,log-transformed,355,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:625,deployability,Log,Log-transformation,625,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:771,deployability,log,logged,771,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1541,deployability,log,logNormalize,1541,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1659,deployability,scale,scale,1659,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:82,energy efficiency,current,current,82,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:267,energy efficiency,estimat,estimate,267,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1659,energy efficiency,scale,scale,1659,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:90,integrability,version,version,90,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:359,integrability,transform,transformed,359,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:629,integrability,transform,transformation,629,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1155,integrability,state,states,1155,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:359,interoperability,transform,transformed,359,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:629,interoperability,transform,transformation,629,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:943,interoperability,specif,specific,943,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:90,modifiability,version,version,90,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:656,modifiability,scal,scaling,656,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1659,modifiability,scal,scale,1659,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:681,performance,perform,performed,681,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1659,performance,scale,scale,1659,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1495,reliability,slo,slot,1495,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:70,safety,risk,risk,70,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:355,safety,log,log-transformed,355,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:625,safety,Log,Log-transformation,625,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:771,safety,log,logged,771,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1541,safety,log,logNormalize,1541,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:70,security,risk,risk,70,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:355,security,log,log-transformed,355,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:625,security,Log,Log-transformation,625,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:771,security,log,logged,771,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1541,security,log,logNormalize,1541,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:355,testability,log,log-transformed,355,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:625,testability,Log,Log-transformation,625,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:771,testability,log,logged,771,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1541,testability,log,logNormalize,1541,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1598,testability,understand,understand,1598,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:681,usability,perform,performed,681,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:999,usability,behavi,behaviour,999,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1141,usability,document,documentation,1141,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:1590,usability,help,help,1590,"> Hello, For information: if I understood correctly, there could be a risk on the current version of `score_genes_cell_cycle` method when the `adata.raw` is present:. > . > * `score_genes_cell_cycle` is based on `score_genes` method which seems to use `adata.raw` to estimate gene score when it is present by default. As far as I know, people often store log-transformed counts to `adata.raw` (an example could be found [here](https://scanpy-tutorials.readthedocs.io/en/latest/pbmc3k.html)). > * However, according to [here](https://nbviewer.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb), ""Log-transformation of data and scaling should always be performed before scoring."". > . > In this situation, when people use `adata.raw` to store logged values, and apply `score_genes_cell_cycle` method to the object without explicitly setting `use_raw = False`, the results could be problematic, unless there is some specific processing overwritting `score_genes`' initial behaviour that I was not aware of. Hi @LuckyMD , I also have a few uncertainties regarding the `score_genes` function in scanpy. Although the documentation states that it behaves similarly to seurat, I came across some references ([here](https://github.com/satijalab/seurat/blob/763259d05991d40721dee99c9919ec6d4491d15e/R/utilities.R#L273) and [here](https://github.com/mojaveazure/seurat-object/blob/3c9e3df0b44a7f6e31e8e0af5d04d398b2b1f004/R/assay.R#L1040)) that suggest seurat operates on the slot of data corresponding to the value after logNormalize. I would appreciate it if you could help me understand why scanpy suggests operating on the matrix after scale instead. Please forgive me if I missed something obvious.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:150,integrability,messag,message,150,You have already replied [here](https://github.com/theislab/single-cell-tutorial/issues/103#issuecomment-1313299316). Many apologies for missing this message.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/issues/1599:150,interoperability,messag,message,150,You have already replied [here](https://github.com/theislab/single-cell-tutorial/issues/103#issuecomment-1313299316). Many apologies for missing this message.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1599
https://github.com/scverse/scanpy/pull/1601:123,deployability,version,versions,123,Would making pynndescent a requirement make the code here a bit more simple? I'm wondering if all the branches around umap versions and whether pynndescent is present could be removed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:123,integrability,version,versions,123,Would making pynndescent a requirement make the code here a bit more simple? I'm wondering if all the branches around umap versions and whether pynndescent is present could be removed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:123,modifiability,version,versions,123,Would making pynndescent a requirement make the code here a bit more simple? I'm wondering if all the branches around umap versions and whether pynndescent is present could be removed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:69,testability,simpl,simple,69,Would making pynndescent a requirement make the code here a bit more simple? I'm wondering if all the branches around umap versions and whether pynndescent is present could be removed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:69,usability,simpl,simple,69,Would making pynndescent a requirement make the code here a bit more simple? I'm wondering if all the branches around umap versions and whether pynndescent is present could be removed.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:126,deployability,version,versions,126,"For umap 0.5 pynndescent is in requirements, so it is used here also. For backwards compatibility i left the code for the old versions. I think i will remove the old code sometime in the future and just use pynndescent and umap >= 0.5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:126,integrability,version,versions,126,"For umap 0.5 pynndescent is in requirements, so it is used here also. For backwards compatibility i left the code for the old versions. I think i will remove the old code sometime in the future and just use pynndescent and umap >= 0.5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:84,interoperability,compatib,compatibility,84,"For umap 0.5 pynndescent is in requirements, so it is used here also. For backwards compatibility i left the code for the old versions. I think i will remove the old code sometime in the future and just use pynndescent and umap >= 0.5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:126,modifiability,version,versions,126,"For umap 0.5 pynndescent is in requirements, so it is used here also. For backwards compatibility i left the code for the old versions. I think i will remove the old code sometime in the future and just use pynndescent and umap >= 0.5.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:164,deployability,fail,failing,164,"> rp_forest storage is still broken though. Is this something you are planning on fixing in this pr? Also, what's broken about it? I would have thought we'd have a failing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:164,reliability,fail,failing,164,"> rp_forest storage is still broken though. Is this something you are planning on fixing in this pr? Also, what's broken about it? I would have thought we'd have a failing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:172,safety,test,test,172,"> rp_forest storage is still broken though. Is this something you are planning on fixing in this pr? Also, what's broken about it? I would have thought we'd have a failing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:70,testability,plan,planning,70,"> rp_forest storage is still broken though. Is this something you are planning on fixing in this pr? Also, what's broken about it? I would have thought we'd have a failing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:172,testability,test,test,172,"> rp_forest storage is still broken though. Is this something you are planning on fixing in this pr? Also, what's broken about it? I would have thought we'd have a failing test.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:83,deployability,fail,failing,83,"I am checking, it seems that the format for `rp_forest `changed significantly. No, failing forest conversion won't cause any problems because it was deliberately coded to avoid problems. The code is in neighbors, and neighbors is critical functuionality and can work without `rp_forest`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:33,interoperability,format,format,33,"I am checking, it seems that the format for `rp_forest `changed significantly. No, failing forest conversion won't cause any problems because it was deliberately coded to avoid problems. The code is in neighbors, and neighbors is critical functuionality and can work without `rp_forest`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:98,interoperability,convers,conversion,98,"I am checking, it seems that the format for `rp_forest `changed significantly. No, failing forest conversion won't cause any problems because it was deliberately coded to avoid problems. The code is in neighbors, and neighbors is critical functuionality and can work without `rp_forest`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:83,reliability,fail,failing,83,"I am checking, it seems that the format for `rp_forest `changed significantly. No, failing forest conversion won't cause any problems because it was deliberately coded to avoid problems. The code is in neighbors, and neighbors is critical functuionality and can work without `rp_forest`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:171,safety,avoid,avoid,171,"I am checking, it seems that the format for `rp_forest `changed significantly. No, failing forest conversion won't cause any problems because it was deliberately coded to avoid problems. The code is in neighbors, and neighbors is critical functuionality and can work without `rp_forest`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:64,security,sign,significantly,64,"I am checking, it seems that the format for `rp_forest `changed significantly. No, failing forest conversion won't cause any problems because it was deliberately coded to avoid problems. The code is in neighbors, and neighbors is critical functuionality and can work without `rp_forest`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:35,interoperability,format,format,35,"> I am checking, it seems that the format for rp_forest changed significantly. Can we validate the format? If so, can we just recalculate the forest if the format isn't what we expect? Also, can you point me to the code where this is handled in `ingest`? I feel like something user visible should happen if there's malformed data in the object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:99,interoperability,format,format,99,"> I am checking, it seems that the format for rp_forest changed significantly. Can we validate the format? If so, can we just recalculate the forest if the format isn't what we expect? Also, can you point me to the code where this is handled in `ingest`? I feel like something user visible should happen if there's malformed data in the object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:156,interoperability,format,format,156,"> I am checking, it seems that the format for rp_forest changed significantly. Can we validate the format? If so, can we just recalculate the forest if the format isn't what we expect? Also, can you point me to the code where this is handled in `ingest`? I feel like something user visible should happen if there's malformed data in the object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:86,safety,valid,validate,86,"> I am checking, it seems that the format for rp_forest changed significantly. Can we validate the format? If so, can we just recalculate the forest if the format isn't what we expect? Also, can you point me to the code where this is handled in `ingest`? I feel like something user visible should happen if there's malformed data in the object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:64,security,sign,significantly,64,"> I am checking, it seems that the format for rp_forest changed significantly. Can we validate the format? If so, can we just recalculate the forest if the format isn't what we expect? Also, can you point me to the code where this is handled in `ingest`? I feel like something user visible should happen if there's malformed data in the object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:86,security,validat,validate,86,"> I am checking, it seems that the format for rp_forest changed significantly. Can we validate the format? If so, can we just recalculate the forest if the format isn't what we expect? Also, can you point me to the code where this is handled in `ingest`? I feel like something user visible should happen if there's malformed data in the object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:277,usability,user,user,277,"> I am checking, it seems that the format for rp_forest changed significantly. Can we validate the format? If so, can we just recalculate the forest if the format isn't what we expect? Also, can you point me to the code where this is handled in `ingest`? I feel like something user visible should happen if there's malformed data in the object.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:151,deployability,version,version,151,"Alright, I was more wondering how you were handling the forest right now, but I see that you're just calculating it. I suspect there will be a minimum version bound needed for the `init_graph` parameter. Let me know when you think this is ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:151,integrability,version,version,151,"Alright, I was more wondering how you were handling the forest right now, but I see that you're just calculating it. I suspect there will be a minimum version bound needed for the `init_graph` parameter. Let me know when you think this is ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:151,modifiability,version,version,151,"Alright, I was more wondering how you were handling the forest right now, but I see that you're just calculating it. I suspect there will be a minimum version bound needed for the `init_graph` parameter. Let me know when you think this is ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:193,modifiability,paramet,parameter,193,"Alright, I was more wondering how you were handling the forest right now, but I see that you're just calculating it. I suspect there will be a minimum version bound needed for the `init_graph` parameter. Let me know when you think this is ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1601:143,usability,minim,minimum,143,"Alright, I was more wondering how you were handling the forest right now, but I see that you're just calculating it. I suspect there will be a minimum version bound needed for the `init_graph` parameter. Let me know when you think this is ready to merge.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1601
https://github.com/scverse/scanpy/pull/1602:39,deployability,version,versions,39,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:168,deployability,depend,dependencies,168,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:242,deployability,version,versions,242,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:267,deployability,version,versions,267,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:465,deployability,build,builds,465,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:708,deployability,build,builds,708,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:505,energy efficiency,current,currently,505,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:39,integrability,version,versions,39,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:168,integrability,depend,dependencies,168,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:242,integrability,version,versions,242,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:267,integrability,version,versions,267,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:39,modifiability,version,versions,39,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:168,modifiability,depend,dependencies,168,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:242,modifiability,version,versions,242,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:267,modifiability,version,versions,267,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:421,performance,time,time,421,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:168,safety,depend,dependencies,168,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:621,safety,Test,Test,621,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:640,safety,Test,Test,640,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:689,safety,Test,Test,689,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:168,testability,depend,dependencies,168,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:621,testability,Test,Test,621,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:640,testability,Test,Test,640,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:689,testability,Test,Test,689,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:252,usability,Minim,Minimum,252,":+1: to twine check :-1: to the python versions. Ultimately, we do have a limited amount of CI, so I think it's important to be a bit cautious adding many jobs. Of the dependencies I'm worried about being an issue: generally not newer python versions. Minimum python versions are important for catching us using newer features. . I am up for swapping python 3.7 with 3.8. I don't think 3.9 is going to work for now. Last time I tried to use 3.9 (a month ago) numpy builds weren't working. I believe numba currently isn't working: https://github.com/numba/numba/issues/6345. Higher priorities to me (roughly in order):. * Test on windows. * Test against lower bounds of our requirements. * Test on Mac. * Dev builds.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:62,safety,test,testing,62,@ivirshup perfectly fine with me. Removed 3.6 . So we are now testing against 3.7 and 3.8.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:62,testability,test,testing,62,@ivirshup perfectly fine with me. Removed 3.6 . So we are now testing against 3.7 and 3.8.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:41,energy efficiency,current,currently,41,@ivirshup is there any reason why we are currently not additionally using Github Actions?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:913,availability,recov,recover,913,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:388,deployability,Depend,Depends,388,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:414,deployability,depend,depends,414,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:549,deployability,integr,integrate,549,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:571,deployability,resourc,resources,571,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:832,deployability,artifact,artifact,832,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:913,deployability,recov,recover,913,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:339,energy efficiency,current,currently,339,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:571,energy efficiency,resourc,resources,571,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:388,integrability,Depend,Depends,388,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:414,integrability,depend,depends,414,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:549,integrability,integr,integrate,549,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:549,interoperability,integr,integrate,549,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:630,interoperability,standard,standard,630,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:388,modifiability,Depend,Depends,388,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:414,modifiability,depend,depends,414,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:549,modifiability,integr,integrate,549,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:658,modifiability,pac,packages,658,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:571,performance,resourc,resources,571,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:674,performance,time,time,674,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:172,reliability,doe,does,172,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:299,reliability,doe,does,299,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:549,reliability,integr,integrate,549,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:913,reliability,recov,recover,913,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:73,safety,accid,accidentally,73,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:388,safety,Depend,Depends,388,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:414,safety,depend,depends,414,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:508,safety,test,testing,508,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:571,safety,resourc,resources,571,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:913,safety,recov,recover,913,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:549,security,integr,integrate,549,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:913,security,recov,recover,913,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:388,testability,Depend,Depends,388,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:414,testability,depend,depends,414,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:508,testability,test,testing,508,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:549,testability,integr,integrate,549,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:571,testability,resourc,resources,571,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:48,usability,support,support,48,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:1028,usability,clear,clear,1028,"> Removed 3.6. We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow [NEP 29](https://numpy.org/neps/nep-0029-deprecation_policy.html) as soon as the ecosystem does). > is there any reason why we are currently not additionally using Github Actions? Depends on the task. Also depends on the definition of github actions I think? We aren't using any of their runners for testing because we'd like the ability to integrate with hosted resources on azure. Also, azure seemed like much more of a standard for numeric python packages at the time we chose it. I'd be happy to have github actions for other things, like `precommit`. `twine check` could be another one, but I haven't looked in to how ""artifact"" type things are handled with github actions to know if we'd be able to recover the built objects. We'd talked about using codecov too, which I'd like to add a check for. I'm not totally clear on the distinction between checks and actions yet.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:159,reliability,doe,does,159,> We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow NEP 29 as soon as the ecosystem does). All right. So do you want. 3.6 + 3.7 + 3.8. Or. 3.6 + 3.7. ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:227,reliability,doe,does,227,> We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow NEP 29 as soon as the ecosystem does). All right. So do you want. 3.6 + 3.7 + 3.8. Or. 3.6 + 3.7. ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:60,safety,accid,accidentally,60,> We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow NEP 29 as soon as the ecosystem does). All right. So do you want. 3.6 + 3.7 + 3.8. Or. 3.6 + 3.7. ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:35,usability,support,support,35,> We should keep 3.6 as long as we support it. It's easy to accidentally add features which only work with 3.7+ otherwise. I'd be happy to drop 3.6 once numpy does (and in general roughly follow NEP 29 as soon as the ecosystem does). All right. So do you want. 3.6 + 3.7 + 3.8. Or. 3.6 + 3.7. ?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:278,availability,Sla,Slack,278,Regarding Github Actions. It is in my opinion the fastest option out there and jobs spin up really fast. We could certainly move some of the fast and easy checks to Github Actions and leave the more heavy jobs for Azure. But let's keep this for a discussion in another issue or Slack. I'll add a sibling PR to Anndata as soon as we merged this one.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:278,reliability,Sla,Slack,278,Regarding Github Actions. It is in my opinion the fastest option out there and jobs spin up really fast. We could certainly move some of the fast and easy checks to Github Actions and leave the more heavy jobs for Azure. But let's keep this for a discussion in another issue or Slack. I'll add a sibling PR to Anndata as soon as we merged this one.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:194,interoperability,incompatib,incompatible,194,10 simultaneous jobs is the supposed limit (though I'm not sure how much this is enforced). I think upper and lower bounds are generally a good way to go:. * lower bounds make sure we don't add incompatible features. * upper bounds give us deprecation warnings.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1602:3,testability,simul,simultaneous,3,10 simultaneous jobs is the supposed limit (though I'm not sure how much this is enforced). I think upper and lower bounds are generally a good way to go:. * lower bounds make sure we don't add incompatible features. * upper bounds give us deprecation warnings.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1602
https://github.com/scverse/scanpy/pull/1603:193,availability,cluster,cluster,193,"@fidelram following a similar question discussed in https://github.com/theislab/squidpy/pull/236 and also in #1288 . the [scipy docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) expliclty say that it should be either a condensed distance matrix (from `pdist`) or the original feature space (e.g. PCA). . Here, the correlation matrix is passed. https://github.com/theislab/scanpy/blob/099be48b860388cf6b8b0aa6006311f6f39de712/scanpy/tools/_dendrogram.py#L140. curious to hear your thought on this, I see that correlation matrix can be used a distance surrogate (but then 1-corr should be used), but here either way not optimal?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:193,deployability,cluster,cluster,193,"@fidelram following a similar question discussed in https://github.com/theislab/squidpy/pull/236 and also in #1288 . the [scipy docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) expliclty say that it should be either a condensed distance matrix (from `pdist`) or the original feature space (e.g. PCA). . Here, the correlation matrix is passed. https://github.com/theislab/scanpy/blob/099be48b860388cf6b8b0aa6006311f6f39de712/scanpy/tools/_dendrogram.py#L140. curious to hear your thought on this, I see that correlation matrix can be used a distance surrogate (but then 1-corr should be used), but here either way not optimal?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:665,energy efficiency,optim,optimal,665,"@fidelram following a similar question discussed in https://github.com/theislab/squidpy/pull/236 and also in #1288 . the [scipy docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) expliclty say that it should be either a condensed distance matrix (from `pdist`) or the original feature space (e.g. PCA). . Here, the correlation matrix is passed. https://github.com/theislab/scanpy/blob/099be48b860388cf6b8b0aa6006311f6f39de712/scanpy/tools/_dendrogram.py#L140. curious to hear your thought on this, I see that correlation matrix can be used a distance surrogate (but then 1-corr should be used), but here either way not optimal?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:479,usability,tool,tools,479,"@fidelram following a similar question discussed in https://github.com/theislab/squidpy/pull/236 and also in #1288 . the [scipy docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) expliclty say that it should be either a condensed distance matrix (from `pdist`) or the original feature space (e.g. PCA). . Here, the correlation matrix is passed. https://github.com/theislab/scanpy/blob/099be48b860388cf6b8b0aa6006311f6f39de712/scanpy/tools/_dendrogram.py#L140. curious to hear your thought on this, I see that correlation matrix can be used a distance surrogate (but then 1-corr should be used), but here either way not optimal?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:128,deployability,releas,release,128,"@giovp, that does seem bad, but we can consider that a separate issue, right? Is the dendrogram bug important for the `squidgy` release?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:13,reliability,doe,does,13,"@giovp, that does seem bad, but we can consider that a separate issue, right? Is the dendrogram bug important for the `squidgy` release?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:932,deployability,build,building,932,"@fidelram, is this ready to go? It looks like most of the issues raised in #1591 are addressed (maybe @michalk8 could take a look too?). ## Minor lingering issues. To some extent, the horizontal line alignment issue is still present:. <img width=""547"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105947228-e0689300-60bc-11eb-8007-4ea31091ef42.png"">. But this seems to go away when plots are larger, as opposed to very zoomed in. It does look like it's a pixel or two too low. ## Order changes with dendrogram. I added the line. ```python. a.X[:, 0] = np.arange(a.shape[0]) / a.shape[0] * (a.X.max() - a.X.min()) - np.abs(a.X.min()). ```. to the example code to check if the within group ordering was still the same. It should appear as a smooth gradient if the ordering does not change. However, if there is any reordering of the groups, the order of the samples within the groups changes. Here's an example (building on the example from the issue):. ```python. b = a.copy(). b.obs['foo'].iloc[:16, :] = 2. b.obs['foo'].iloc[16:32, :] = 0. b.obs['foo'].iloc[32:50, :] = 1. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). sc.pl.heatmap(b, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). ```. <img width=""789"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105949911-98983a80-60c1-11eb-98a6-f0dc1115e562.png"">. You can keep the order the same if you make the sorting stable, e.g. `obs_tidy = obs_tidy.sort_index(kind=""mergesort"") ` here: https://github.com/theislab/scanpy/blob/a64a1e4a7cdb28baf88a0afa3b6064e77f6e32ee/scanpy/plotting/_anndata.py#L1045. I'm not sure if this is actually an issue though. We don't make any promises about this, do we?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:1102,energy efficiency,heat,heatmap,1102,"@fidelram, is this ready to go? It looks like most of the issues raised in #1591 are addressed (maybe @michalk8 could take a look too?). ## Minor lingering issues. To some extent, the horizontal line alignment issue is still present:. <img width=""547"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105947228-e0689300-60bc-11eb-8007-4ea31091ef42.png"">. But this seems to go away when plots are larger, as opposed to very zoomed in. It does look like it's a pixel or two too low. ## Order changes with dendrogram. I added the line. ```python. a.X[:, 0] = np.arange(a.shape[0]) / a.shape[0] * (a.X.max() - a.X.min()) - np.abs(a.X.min()). ```. to the example code to check if the within group ordering was still the same. It should appear as a smooth gradient if the ordering does not change. However, if there is any reordering of the groups, the order of the samples within the groups changes. Here's an example (building on the example from the issue):. ```python. b = a.copy(). b.obs['foo'].iloc[:16, :] = 2. b.obs['foo'].iloc[16:32, :] = 0. b.obs['foo'].iloc[32:50, :] = 1. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). sc.pl.heatmap(b, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). ```. <img width=""789"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105949911-98983a80-60c1-11eb-98a6-f0dc1115e562.png"">. You can keep the order the same if you make the sorting stable, e.g. `obs_tidy = obs_tidy.sort_index(kind=""mergesort"") ` here: https://github.com/theislab/scanpy/blob/a64a1e4a7cdb28baf88a0afa3b6064e77f6e32ee/scanpy/plotting/_anndata.py#L1045. I'm not sure if this is actually an issue though. We don't make any promises about this, do we?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:1198,energy efficiency,heat,heatmap,1198,"@fidelram, is this ready to go? It looks like most of the issues raised in #1591 are addressed (maybe @michalk8 could take a look too?). ## Minor lingering issues. To some extent, the horizontal line alignment issue is still present:. <img width=""547"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105947228-e0689300-60bc-11eb-8007-4ea31091ef42.png"">. But this seems to go away when plots are larger, as opposed to very zoomed in. It does look like it's a pixel or two too low. ## Order changes with dendrogram. I added the line. ```python. a.X[:, 0] = np.arange(a.shape[0]) / a.shape[0] * (a.X.max() - a.X.min()) - np.abs(a.X.min()). ```. to the example code to check if the within group ordering was still the same. It should appear as a smooth gradient if the ordering does not change. However, if there is any reordering of the groups, the order of the samples within the groups changes. Here's an example (building on the example from the issue):. ```python. b = a.copy(). b.obs['foo'].iloc[:16, :] = 2. b.obs['foo'].iloc[16:32, :] = 0. b.obs['foo'].iloc[32:50, :] = 1. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). sc.pl.heatmap(b, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). ```. <img width=""789"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105949911-98983a80-60c1-11eb-98a6-f0dc1115e562.png"">. You can keep the order the same if you make the sorting stable, e.g. `obs_tidy = obs_tidy.sort_index(kind=""mergesort"") ` here: https://github.com/theislab/scanpy/blob/a64a1e4a7cdb28baf88a0afa3b6064e77f6e32ee/scanpy/plotting/_anndata.py#L1045. I'm not sure if this is actually an issue though. We don't make any promises about this, do we?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:172,modifiability,exten,extent,172,"@fidelram, is this ready to go? It looks like most of the issues raised in #1591 are addressed (maybe @michalk8 could take a look too?). ## Minor lingering issues. To some extent, the horizontal line alignment issue is still present:. <img width=""547"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105947228-e0689300-60bc-11eb-8007-4ea31091ef42.png"">. But this seems to go away when plots are larger, as opposed to very zoomed in. It does look like it's a pixel or two too low. ## Order changes with dendrogram. I added the line. ```python. a.X[:, 0] = np.arange(a.shape[0]) / a.shape[0] * (a.X.max() - a.X.min()) - np.abs(a.X.min()). ```. to the example code to check if the within group ordering was still the same. It should appear as a smooth gradient if the ordering does not change. However, if there is any reordering of the groups, the order of the samples within the groups changes. Here's an example (building on the example from the issue):. ```python. b = a.copy(). b.obs['foo'].iloc[:16, :] = 2. b.obs['foo'].iloc[16:32, :] = 0. b.obs['foo'].iloc[32:50, :] = 1. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). sc.pl.heatmap(b, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). ```. <img width=""789"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105949911-98983a80-60c1-11eb-98a6-f0dc1115e562.png"">. You can keep the order the same if you make the sorting stable, e.g. `obs_tidy = obs_tidy.sort_index(kind=""mergesort"") ` here: https://github.com/theislab/scanpy/blob/a64a1e4a7cdb28baf88a0afa3b6064e77f6e32ee/scanpy/plotting/_anndata.py#L1045. I'm not sure if this is actually an issue though. We don't make any promises about this, do we?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:455,reliability,doe,does,455,"@fidelram, is this ready to go? It looks like most of the issues raised in #1591 are addressed (maybe @michalk8 could take a look too?). ## Minor lingering issues. To some extent, the horizontal line alignment issue is still present:. <img width=""547"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105947228-e0689300-60bc-11eb-8007-4ea31091ef42.png"">. But this seems to go away when plots are larger, as opposed to very zoomed in. It does look like it's a pixel or two too low. ## Order changes with dendrogram. I added the line. ```python. a.X[:, 0] = np.arange(a.shape[0]) / a.shape[0] * (a.X.max() - a.X.min()) - np.abs(a.X.min()). ```. to the example code to check if the within group ordering was still the same. It should appear as a smooth gradient if the ordering does not change. However, if there is any reordering of the groups, the order of the samples within the groups changes. Here's an example (building on the example from the issue):. ```python. b = a.copy(). b.obs['foo'].iloc[:16, :] = 2. b.obs['foo'].iloc[16:32, :] = 0. b.obs['foo'].iloc[32:50, :] = 1. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). sc.pl.heatmap(b, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). ```. <img width=""789"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105949911-98983a80-60c1-11eb-98a6-f0dc1115e562.png"">. You can keep the order the same if you make the sorting stable, e.g. `obs_tidy = obs_tidy.sort_index(kind=""mergesort"") ` here: https://github.com/theislab/scanpy/blob/a64a1e4a7cdb28baf88a0afa3b6064e77f6e32ee/scanpy/plotting/_anndata.py#L1045. I'm not sure if this is actually an issue though. We don't make any promises about this, do we?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:793,reliability,doe,does,793,"@fidelram, is this ready to go? It looks like most of the issues raised in #1591 are addressed (maybe @michalk8 could take a look too?). ## Minor lingering issues. To some extent, the horizontal line alignment issue is still present:. <img width=""547"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105947228-e0689300-60bc-11eb-8007-4ea31091ef42.png"">. But this seems to go away when plots are larger, as opposed to very zoomed in. It does look like it's a pixel or two too low. ## Order changes with dendrogram. I added the line. ```python. a.X[:, 0] = np.arange(a.shape[0]) / a.shape[0] * (a.X.max() - a.X.min()) - np.abs(a.X.min()). ```. to the example code to check if the within group ordering was still the same. It should appear as a smooth gradient if the ordering does not change. However, if there is any reordering of the groups, the order of the samples within the groups changes. Here's an example (building on the example from the issue):. ```python. b = a.copy(). b.obs['foo'].iloc[:16, :] = 2. b.obs['foo'].iloc[16:32, :] = 0. b.obs['foo'].iloc[32:50, :] = 1. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). sc.pl.heatmap(b, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). ```. <img width=""789"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105949911-98983a80-60c1-11eb-98a6-f0dc1115e562.png"">. You can keep the order the same if you make the sorting stable, e.g. `obs_tidy = obs_tidy.sort_index(kind=""mergesort"") ` here: https://github.com/theislab/scanpy/blob/a64a1e4a7cdb28baf88a0afa3b6064e77f6e32ee/scanpy/plotting/_anndata.py#L1045. I'm not sure if this is actually an issue though. We don't make any promises about this, do we?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:277,usability,user,user-images,277,"@fidelram, is this ready to go? It looks like most of the issues raised in #1591 are addressed (maybe @michalk8 could take a look too?). ## Minor lingering issues. To some extent, the horizontal line alignment issue is still present:. <img width=""547"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105947228-e0689300-60bc-11eb-8007-4ea31091ef42.png"">. But this seems to go away when plots are larger, as opposed to very zoomed in. It does look like it's a pixel or two too low. ## Order changes with dendrogram. I added the line. ```python. a.X[:, 0] = np.arange(a.shape[0]) / a.shape[0] * (a.X.max() - a.X.min()) - np.abs(a.X.min()). ```. to the example code to check if the within group ordering was still the same. It should appear as a smooth gradient if the ordering does not change. However, if there is any reordering of the groups, the order of the samples within the groups changes. Here's an example (building on the example from the issue):. ```python. b = a.copy(). b.obs['foo'].iloc[:16, :] = 2. b.obs['foo'].iloc[16:32, :] = 0. b.obs['foo'].iloc[32:50, :] = 1. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). sc.pl.heatmap(b, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). ```. <img width=""789"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105949911-98983a80-60c1-11eb-98a6-f0dc1115e562.png"">. You can keep the order the same if you make the sorting stable, e.g. `obs_tidy = obs_tidy.sort_index(kind=""mergesort"") ` here: https://github.com/theislab/scanpy/blob/a64a1e4a7cdb28baf88a0afa3b6064e77f6e32ee/scanpy/plotting/_anndata.py#L1045. I'm not sure if this is actually an issue though. We don't make any promises about this, do we?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:1335,usability,user,user-images,1335,"@fidelram, is this ready to go? It looks like most of the issues raised in #1591 are addressed (maybe @michalk8 could take a look too?). ## Minor lingering issues. To some extent, the horizontal line alignment issue is still present:. <img width=""547"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105947228-e0689300-60bc-11eb-8007-4ea31091ef42.png"">. But this seems to go away when plots are larger, as opposed to very zoomed in. It does look like it's a pixel or two too low. ## Order changes with dendrogram. I added the line. ```python. a.X[:, 0] = np.arange(a.shape[0]) / a.shape[0] * (a.X.max() - a.X.min()) - np.abs(a.X.min()). ```. to the example code to check if the within group ordering was still the same. It should appear as a smooth gradient if the ordering does not change. However, if there is any reordering of the groups, the order of the samples within the groups changes. Here's an example (building on the example from the issue):. ```python. b = a.copy(). b.obs['foo'].iloc[:16, :] = 2. b.obs['foo'].iloc[16:32, :] = 0. b.obs['foo'].iloc[32:50, :] = 1. sc.pl.heatmap(a, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). sc.pl.heatmap(b, var_names=a.var_names[:50], groupby=""foo"", swap_axes=False, dendrogram=False). ```. <img width=""789"" alt=""image"" src=""https://user-images.githubusercontent.com/8238804/105949911-98983a80-60c1-11eb-98a6-f0dc1115e562.png"">. You can keep the order the same if you make the sorting stable, e.g. `obs_tidy = obs_tidy.sort_index(kind=""mergesort"") ` here: https://github.com/theislab/scanpy/blob/a64a1e4a7cdb28baf88a0afa3b6064e77f6e32ee/scanpy/plotting/_anndata.py#L1045. I'm not sure if this is actually an issue though. We don't make any promises about this, do we?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:100,availability,sla,slack,100,"@giovp As suggested by Isaac, I think the dendrogram problem is a separate issue. As discussed over slack the solution is to use the same prescription as seaborn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/pull/1603:100,reliability,sla,slack,100,"@giovp As suggested by Isaac, I think the dendrogram problem is a separate issue. As discussed over slack the solution is to use the same prescription as seaborn.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1603
https://github.com/scverse/scanpy/issues/1604:691,availability,robust,robust,691,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:361,deployability,pipelin,pipeline,361,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:632,deployability,log,logic,632,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:584,energy efficiency,cool,cool,584,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:296,integrability,pub,public,296,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:361,integrability,pipelin,pipeline,361,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:705,modifiability,maintain,maintainer,705,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:428,performance,time,time,428,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:691,reliability,robust,robust,691,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:632,safety,log,logic,632,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:691,safety,robust,robust,691,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:705,safety,maintain,maintainer,705,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:632,security,log,logic,632,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:632,testability,log,logic,632,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:673,usability,user,user,673,"@fidelram that's a really great point and something I'd like to discuss at next meeting (already put it in the agenda). Another great example of such examples 😅 is the way @michalk8 set it up for [cellrank](https://cellrank.readthedocs.io/en/latest/auto_examples/index.html) and squidpy [not yet public]. The even nicer thing is that @michalk8 implemented a CI pipeline for the tutorials/examples part of the repo so that every time there is a change in master of the original repo, the examples are refreshed in the notebooks repo, so to have them always up to date. Would be really cool to concentrate efforts and try to get this logic also in scanpy (makes it both very user friendly and robust from a maintainer perspective)",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:430,deployability,resourc,resources,430,"I would love to have this! I've been talking about this with @falexwolf and a bit with @flying-sheep. Seaborn uses a sphinx extension from `matplotlib` to generate the plots ([plot directive](https://matplotlib.org/3.1.3/devel/plot_directive.html)). I would definitely like to get tutorials running on CI as well, but think this is a little separate (some previous discussion: #302). 1. Tutorials can use a lot more computational resources than plotting. 2. These examples are written in the docstrings, while tutorials are currently in notebooks. It'll probably be a different set of tools to render these.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:430,energy efficiency,resourc,resources,430,"I would love to have this! I've been talking about this with @falexwolf and a bit with @flying-sheep. Seaborn uses a sphinx extension from `matplotlib` to generate the plots ([plot directive](https://matplotlib.org/3.1.3/devel/plot_directive.html)). I would definitely like to get tutorials running on CI as well, but think this is a little separate (some previous discussion: #302). 1. Tutorials can use a lot more computational resources than plotting. 2. These examples are written in the docstrings, while tutorials are currently in notebooks. It'll probably be a different set of tools to render these.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:524,energy efficiency,current,currently,524,"I would love to have this! I've been talking about this with @falexwolf and a bit with @flying-sheep. Seaborn uses a sphinx extension from `matplotlib` to generate the plots ([plot directive](https://matplotlib.org/3.1.3/devel/plot_directive.html)). I would definitely like to get tutorials running on CI as well, but think this is a little separate (some previous discussion: #302). 1. Tutorials can use a lot more computational resources than plotting. 2. These examples are written in the docstrings, while tutorials are currently in notebooks. It'll probably be a different set of tools to render these.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:124,modifiability,extens,extension,124,"I would love to have this! I've been talking about this with @falexwolf and a bit with @flying-sheep. Seaborn uses a sphinx extension from `matplotlib` to generate the plots ([plot directive](https://matplotlib.org/3.1.3/devel/plot_directive.html)). I would definitely like to get tutorials running on CI as well, but think this is a little separate (some previous discussion: #302). 1. Tutorials can use a lot more computational resources than plotting. 2. These examples are written in the docstrings, while tutorials are currently in notebooks. It'll probably be a different set of tools to render these.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:416,performance,computational resourc,computational resources,416,"I would love to have this! I've been talking about this with @falexwolf and a bit with @flying-sheep. Seaborn uses a sphinx extension from `matplotlib` to generate the plots ([plot directive](https://matplotlib.org/3.1.3/devel/plot_directive.html)). I would definitely like to get tutorials running on CI as well, but think this is a little separate (some previous discussion: #302). 1. Tutorials can use a lot more computational resources than plotting. 2. These examples are written in the docstrings, while tutorials are currently in notebooks. It'll probably be a different set of tools to render these.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:430,safety,resourc,resources,430,"I would love to have this! I've been talking about this with @falexwolf and a bit with @flying-sheep. Seaborn uses a sphinx extension from `matplotlib` to generate the plots ([plot directive](https://matplotlib.org/3.1.3/devel/plot_directive.html)). I would definitely like to get tutorials running on CI as well, but think this is a little separate (some previous discussion: #302). 1. Tutorials can use a lot more computational resources than plotting. 2. These examples are written in the docstrings, while tutorials are currently in notebooks. It'll probably be a different set of tools to render these.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:430,testability,resourc,resources,430,"I would love to have this! I've been talking about this with @falexwolf and a bit with @flying-sheep. Seaborn uses a sphinx extension from `matplotlib` to generate the plots ([plot directive](https://matplotlib.org/3.1.3/devel/plot_directive.html)). I would definitely like to get tutorials running on CI as well, but think this is a little separate (some previous discussion: #302). 1. Tutorials can use a lot more computational resources than plotting. 2. These examples are written in the docstrings, while tutorials are currently in notebooks. It'll probably be a different set of tools to render these.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:585,usability,tool,tools,585,"I would love to have this! I've been talking about this with @falexwolf and a bit with @flying-sheep. Seaborn uses a sphinx extension from `matplotlib` to generate the plots ([plot directive](https://matplotlib.org/3.1.3/devel/plot_directive.html)). I would definitely like to get tutorials running on CI as well, but think this is a little separate (some previous discussion: #302). 1. Tutorials can use a lot more computational resources than plotting. 2. These examples are written in the docstrings, while tutorials are currently in notebooks. It'll probably be a different set of tools to render these.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:19,modifiability,extens,extension,19,I tried the sphinx extension but could not get any result. Can somebody take over this?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1604:0,usability,Close,Closed,0,"Closed, as the initial issue is done, and being tracked elsewhere.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1604
https://github.com/scverse/scanpy/issues/1605:65,availability,replic,replicating,65,Can you provide an example where this occurs? I'm having trouble replicating this.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1605
https://github.com/scverse/scanpy/issues/1605:79,modifiability,pac,packages,79,"Hi @ivirshup , I figured out that it seems the function that I used from other packages which called scanpy set the zero-centered as False!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1605
https://github.com/scverse/scanpy/issues/1605:19,deployability,updat,update,19,"Ah, thanks for the update!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1605
https://github.com/scverse/scanpy/issues/1605:19,safety,updat,update,19,"Ah, thanks for the update!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1605
https://github.com/scverse/scanpy/issues/1605:19,security,updat,update,19,"Ah, thanks for the update!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1605
https://github.com/scverse/scanpy/issues/1606:782,deployability,modul,module,782,"It's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1011,deployability,log,logg,1011,"o be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:996,energy efficiency,estimat,estimator,996,"t's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1127,energy efficiency,model,model,1127,"his issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Sing",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1276,energy efficiency,model,model,1276,"`python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. </details>. Does this occur in your data? You",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1337,energy efficiency,estimat,estimator,1337,"atasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. </details>. Does this occur in your data? You can check with:. ```python. pd.crosstab(adata.obs[""384plate""],",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:180,integrability,batch,batch,180,"It's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:947,integrability,standardiz,standardize,947,"It's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1022,integrability,Standardiz,Standardizing,1022,"t a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_sing",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1188,integrability,standardiz,standardized,1188,"riates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, fla",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1894,integrability,wrap,wrap,1894,"lobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. </details>. Does this occur in your data? You can check with:. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:947,interoperability,standard,standardize,947,"It's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1022,interoperability,Standard,Standardizing,1022,"t a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_sing",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1188,interoperability,standard,standardized,1188,"riates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, fla",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:782,modifiability,modul,module,782,"It's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1170,modifiability,paramet,parameters,1170,"your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1644,modifiability,pac,packages,1644,"lobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. </details>. Does this occur in your data? You can check with:. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1971,modifiability,pac,packages,1971,"lobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. </details>. Does this occur in your data? You can check with:. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:180,performance,batch,batch,180,"It's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:2246,reliability,Doe,Does,2246,"lobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. </details>. Does this occur in your data? You can check with:. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:205,safety,compl,completely,205,"It's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:755,safety,input,input-,755,"It's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:782,safety,modul,module,782,"It's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1011,safety,log,logg,1011,"o be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:205,security,compl,completely,205,"It's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1011,security,log,logg,1011,"o be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1127,security,model,model,1127,"his issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Sing",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1276,security,model,model,1276,"`python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. </details>. Does this occur in your data? You",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1691,security,sign,signature,1691,"lobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. </details>. Does this occur in your data? You can check with:. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1846,security,sign,signature,1846,"lobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. </details>. Does this occur in your data? You can check with:. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1856,security,sign,signature,1856,"lobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag). 86 . 87 def _raise_linalgerror_singular(err, flag):. ---> 88 raise LinAlgError(""Singular matrix""). 89 . 90 def _raise_linalgerror_nonposdef(err, flag):. LinAlgError: Singular matrix. ```. </details>. Does this occur in your data? You can check with:. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:591,testability,trace,traceback,591,"It's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:711,testability,Trace,Traceback,711,"It's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:1011,testability,log,logg,1011,"o be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in _raise_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:755,usability,input,input-,755,"It's a hard to be sure without a reproducible example (e.g., we'd need to be able to make an `AnnData` object that could trigger this issue), but I suspect some categories in your batch and covariates are completely confounded. Here's an example that would also trigger this:. ```python. import scanpy as sc. import pandas as pd. blobs = sc.datasets.blobs(). blobs.obs[""blobs""] = pd.Categorical(blobs.obs[""blobs""]). blobs.obs[""cov""] = pd.Categorical(blobs.obs[""blobs""] == ""0""). sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). # LinAlgError: Singular matrix. ```. <details>. <summary> Full traceback </summary>. ```pytb. ---------------------------------------------------------------------------. LinAlgError Traceback (most recent call last). <ipython-input-13-5685c001369c> in <module>. ----> 1 sc.pp.combat(blobs, ""blobs"", covariates=[""cov""]). ~/github/scanpy/scanpy/preprocessing/_combat.py in combat(adata, key, covariates, inplace). 204 # standardize across genes using a pooled variance estimator. 205 logg.info(""Standardizing Data across genes.\n""). --> 206 s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key). 207 . 208 # fitting the parameters on the standardized data. ~/github/scanpy/scanpy/preprocessing/_combat.py in _standardize_data(model, data, batch_key). 102 . 103 # compute pooled variance estimator. --> 104 B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T). 105 grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :]). 106 var_pooled = (data - np.dot(design, B_hat).T) ** 2. <__array_function__ internals> in inv(*args, **kwargs). /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.py in inv(a). 544 signature = 'D->D' if isComplexType(t) else 'd->d'. 545 extobj = get_linalg_error_extobj(_raise_linalgerror_singular). --> 546 ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj). 547 return wrap(ainv.astype(result_t, copy=False)). 548 . /usr/local/lib/python3.8/site-packages/numpy/linalg/linalg.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:33,availability,Error,Error,33,"Thank you so much for the reply. Error posted above is happening in two independent datasets possibly due to confounding factors as you mentioned. Though I don't quite comprehend what is a confounding factor in my specific case. . Here is the output of the batches and covariates as requested. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]). ```. ```pytb. age_group Old YoungAdult Pediatric Fetal NewBorn. 384plate . 27YOMP1 0 368 0 0 0. 27YOMP2 0 383 0 0 0. 27YOMP3 0 184 0 0 0. BM01152P1 0 384 0 0 0. BM01152P2 0 384 0 0 0. BM01158P1 0 0 382 0 0. BM01158P2 0 0 384 0 0. BM8182P1 0 55 0 0 0. FBFLP5L2 0 0 0 382 0. FBML1 0 0 0 378 0. FBML2 0 0 0 322 0. FBP5L1 0 0 0 377 0. FLP5L1 0 0 0 381 0. FLP5L3 0 0 0 338 0. FLP6L1 0 0 0 376 0. FLP6L2 0 0 0 383 0. FLP6L3 0 0 0 53 0. UCB250P1 0 0 0 0 382. UCB250P2 0 0 0 0 384. UCB250P3 0 0 0 0 150. UCB259P1 0 0 0 0 373. UCB259P2 0 0 0 0 376. UCB270P1 0 0 0 0 382. UCB270P2 0 0 0 0 128. UCBBMP4 0 41 272 0 63. UCBBMP5 0 0 0 0 325. hHSCP1B1 379 0 0 0 0. hHSCP1B2 359 0 0 0 0. hHSCP1B3 377 0 0 0 0. hHSCP2B1 376 0 0 0 0. hHSCP2B2 350 0 0 0 0. hHSCP3B1 371 0 0 0 0. hHSCP3B2 340 0 0 0 0. hHSCP4B1 290 0 0 0 0. ```. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""sex""]). ```. ```pytb. sex F M U. 384plate . 27YOMP1 0 368 0. 27YOMP2 0 383 0. 27YOMP3 0 184 0. BM01152P1 0 384 0. BM01152P2 0 384 0. BM01158P1 0 382 0. BM01158P2 0 384 0. BM8182P1 0 55 0. FBFLP5L2 0 55 327. FBML1 0 0 378. FBML2 0 0 322. FBP5L1 0 0 377. FLP5L1 0 0 381. FLP5L3 0 338 0. FLP6L1 0 376 0. FLP6L2 0 71 312. FLP6L3 0 0 53. UCB250P1 0 382 0. UCB250P2 0 384 0. UCB250P3 0 150 0. UCB259P1 0 373 0. UCB259P2 0 376 0. UCB270P1 0 382 0. UCB270P2 0 128 0. UCBBMP4 0 376 0. UCBBMP5 0 325 0. hHSCP1B1 0 379 0. hHSCP1B2 173 186 0. hHSCP1B3 216 161 0. hHSCP2B1 0 376 0. hHSCP2B2 350 0 0. hHSCP3B1 0 371 0. hHSCP3B2 158 182 0. hHSCP4B1 0 290 0. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:257,integrability,batch,batches,257,"Thank you so much for the reply. Error posted above is happening in two independent datasets possibly due to confounding factors as you mentioned. Though I don't quite comprehend what is a confounding factor in my specific case. . Here is the output of the batches and covariates as requested. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]). ```. ```pytb. age_group Old YoungAdult Pediatric Fetal NewBorn. 384plate . 27YOMP1 0 368 0 0 0. 27YOMP2 0 383 0 0 0. 27YOMP3 0 184 0 0 0. BM01152P1 0 384 0 0 0. BM01152P2 0 384 0 0 0. BM01158P1 0 0 382 0 0. BM01158P2 0 0 384 0 0. BM8182P1 0 55 0 0 0. FBFLP5L2 0 0 0 382 0. FBML1 0 0 0 378 0. FBML2 0 0 0 322 0. FBP5L1 0 0 0 377 0. FLP5L1 0 0 0 381 0. FLP5L3 0 0 0 338 0. FLP6L1 0 0 0 376 0. FLP6L2 0 0 0 383 0. FLP6L3 0 0 0 53 0. UCB250P1 0 0 0 0 382. UCB250P2 0 0 0 0 384. UCB250P3 0 0 0 0 150. UCB259P1 0 0 0 0 373. UCB259P2 0 0 0 0 376. UCB270P1 0 0 0 0 382. UCB270P2 0 0 0 0 128. UCBBMP4 0 41 272 0 63. UCBBMP5 0 0 0 0 325. hHSCP1B1 379 0 0 0 0. hHSCP1B2 359 0 0 0 0. hHSCP1B3 377 0 0 0 0. hHSCP2B1 376 0 0 0 0. hHSCP2B2 350 0 0 0 0. hHSCP3B1 371 0 0 0 0. hHSCP3B2 340 0 0 0 0. hHSCP4B1 290 0 0 0 0. ```. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""sex""]). ```. ```pytb. sex F M U. 384plate . 27YOMP1 0 368 0. 27YOMP2 0 383 0. 27YOMP3 0 184 0. BM01152P1 0 384 0. BM01152P2 0 384 0. BM01158P1 0 382 0. BM01158P2 0 384 0. BM8182P1 0 55 0. FBFLP5L2 0 55 327. FBML1 0 0 378. FBML2 0 0 322. FBP5L1 0 0 377. FLP5L1 0 0 381. FLP5L3 0 338 0. FLP6L1 0 376 0. FLP6L2 0 71 312. FLP6L3 0 0 53. UCB250P1 0 382 0. UCB250P2 0 384 0. UCB250P3 0 150 0. UCB259P1 0 373 0. UCB259P2 0 376 0. UCB270P1 0 382 0. UCB270P2 0 128 0. UCBBMP4 0 376 0. UCBBMP5 0 325 0. hHSCP1B1 0 379 0. hHSCP1B2 173 186 0. hHSCP1B3 216 161 0. hHSCP2B1 0 376 0. hHSCP2B2 350 0 0. hHSCP3B1 0 371 0. hHSCP3B2 158 182 0. hHSCP4B1 0 290 0. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:214,interoperability,specif,specific,214,"Thank you so much for the reply. Error posted above is happening in two independent datasets possibly due to confounding factors as you mentioned. Though I don't quite comprehend what is a confounding factor in my specific case. . Here is the output of the batches and covariates as requested. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]). ```. ```pytb. age_group Old YoungAdult Pediatric Fetal NewBorn. 384plate . 27YOMP1 0 368 0 0 0. 27YOMP2 0 383 0 0 0. 27YOMP3 0 184 0 0 0. BM01152P1 0 384 0 0 0. BM01152P2 0 384 0 0 0. BM01158P1 0 0 382 0 0. BM01158P2 0 0 384 0 0. BM8182P1 0 55 0 0 0. FBFLP5L2 0 0 0 382 0. FBML1 0 0 0 378 0. FBML2 0 0 0 322 0. FBP5L1 0 0 0 377 0. FLP5L1 0 0 0 381 0. FLP5L3 0 0 0 338 0. FLP6L1 0 0 0 376 0. FLP6L2 0 0 0 383 0. FLP6L3 0 0 0 53 0. UCB250P1 0 0 0 0 382. UCB250P2 0 0 0 0 384. UCB250P3 0 0 0 0 150. UCB259P1 0 0 0 0 373. UCB259P2 0 0 0 0 376. UCB270P1 0 0 0 0 382. UCB270P2 0 0 0 0 128. UCBBMP4 0 41 272 0 63. UCBBMP5 0 0 0 0 325. hHSCP1B1 379 0 0 0 0. hHSCP1B2 359 0 0 0 0. hHSCP1B3 377 0 0 0 0. hHSCP2B1 376 0 0 0 0. hHSCP2B2 350 0 0 0 0. hHSCP3B1 371 0 0 0 0. hHSCP3B2 340 0 0 0 0. hHSCP4B1 290 0 0 0 0. ```. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""sex""]). ```. ```pytb. sex F M U. 384plate . 27YOMP1 0 368 0. 27YOMP2 0 383 0. 27YOMP3 0 184 0. BM01152P1 0 384 0. BM01152P2 0 384 0. BM01158P1 0 382 0. BM01158P2 0 384 0. BM8182P1 0 55 0. FBFLP5L2 0 55 327. FBML1 0 0 378. FBML2 0 0 322. FBP5L1 0 0 377. FLP5L1 0 0 381. FLP5L3 0 338 0. FLP6L1 0 376 0. FLP6L2 0 71 312. FLP6L3 0 0 53. UCB250P1 0 382 0. UCB250P2 0 384 0. UCB250P3 0 150 0. UCB259P1 0 373 0. UCB259P2 0 376 0. UCB270P1 0 382 0. UCB270P2 0 128 0. UCBBMP4 0 376 0. UCBBMP5 0 325 0. hHSCP1B1 0 379 0. hHSCP1B2 173 186 0. hHSCP1B3 216 161 0. hHSCP2B1 0 376 0. hHSCP2B2 350 0 0. hHSCP3B1 0 371 0. hHSCP3B2 158 182 0. hHSCP4B1 0 290 0. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:33,performance,Error,Error,33,"Thank you so much for the reply. Error posted above is happening in two independent datasets possibly due to confounding factors as you mentioned. Though I don't quite comprehend what is a confounding factor in my specific case. . Here is the output of the batches and covariates as requested. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]). ```. ```pytb. age_group Old YoungAdult Pediatric Fetal NewBorn. 384plate . 27YOMP1 0 368 0 0 0. 27YOMP2 0 383 0 0 0. 27YOMP3 0 184 0 0 0. BM01152P1 0 384 0 0 0. BM01152P2 0 384 0 0 0. BM01158P1 0 0 382 0 0. BM01158P2 0 0 384 0 0. BM8182P1 0 55 0 0 0. FBFLP5L2 0 0 0 382 0. FBML1 0 0 0 378 0. FBML2 0 0 0 322 0. FBP5L1 0 0 0 377 0. FLP5L1 0 0 0 381 0. FLP5L3 0 0 0 338 0. FLP6L1 0 0 0 376 0. FLP6L2 0 0 0 383 0. FLP6L3 0 0 0 53 0. UCB250P1 0 0 0 0 382. UCB250P2 0 0 0 0 384. UCB250P3 0 0 0 0 150. UCB259P1 0 0 0 0 373. UCB259P2 0 0 0 0 376. UCB270P1 0 0 0 0 382. UCB270P2 0 0 0 0 128. UCBBMP4 0 41 272 0 63. UCBBMP5 0 0 0 0 325. hHSCP1B1 379 0 0 0 0. hHSCP1B2 359 0 0 0 0. hHSCP1B3 377 0 0 0 0. hHSCP2B1 376 0 0 0 0. hHSCP2B2 350 0 0 0 0. hHSCP3B1 371 0 0 0 0. hHSCP3B2 340 0 0 0 0. hHSCP4B1 290 0 0 0 0. ```. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""sex""]). ```. ```pytb. sex F M U. 384plate . 27YOMP1 0 368 0. 27YOMP2 0 383 0. 27YOMP3 0 184 0. BM01152P1 0 384 0. BM01152P2 0 384 0. BM01158P1 0 382 0. BM01158P2 0 384 0. BM8182P1 0 55 0. FBFLP5L2 0 55 327. FBML1 0 0 378. FBML2 0 0 322. FBP5L1 0 0 377. FLP5L1 0 0 381. FLP5L3 0 338 0. FLP6L1 0 376 0. FLP6L2 0 71 312. FLP6L3 0 0 53. UCB250P1 0 382 0. UCB250P2 0 384 0. UCB250P3 0 150 0. UCB259P1 0 373 0. UCB259P2 0 376 0. UCB270P1 0 382 0. UCB270P2 0 128 0. UCBBMP4 0 376 0. UCBBMP5 0 325 0. hHSCP1B1 0 379 0. hHSCP1B2 173 186 0. hHSCP1B3 216 161 0. hHSCP2B1 0 376 0. hHSCP2B2 350 0 0. hHSCP3B1 0 371 0. hHSCP3B2 158 182 0. hHSCP4B1 0 290 0. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:257,performance,batch,batches,257,"Thank you so much for the reply. Error posted above is happening in two independent datasets possibly due to confounding factors as you mentioned. Though I don't quite comprehend what is a confounding factor in my specific case. . Here is the output of the batches and covariates as requested. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]). ```. ```pytb. age_group Old YoungAdult Pediatric Fetal NewBorn. 384plate . 27YOMP1 0 368 0 0 0. 27YOMP2 0 383 0 0 0. 27YOMP3 0 184 0 0 0. BM01152P1 0 384 0 0 0. BM01152P2 0 384 0 0 0. BM01158P1 0 0 382 0 0. BM01158P2 0 0 384 0 0. BM8182P1 0 55 0 0 0. FBFLP5L2 0 0 0 382 0. FBML1 0 0 0 378 0. FBML2 0 0 0 322 0. FBP5L1 0 0 0 377 0. FLP5L1 0 0 0 381 0. FLP5L3 0 0 0 338 0. FLP6L1 0 0 0 376 0. FLP6L2 0 0 0 383 0. FLP6L3 0 0 0 53 0. UCB250P1 0 0 0 0 382. UCB250P2 0 0 0 0 384. UCB250P3 0 0 0 0 150. UCB259P1 0 0 0 0 373. UCB259P2 0 0 0 0 376. UCB270P1 0 0 0 0 382. UCB270P2 0 0 0 0 128. UCBBMP4 0 41 272 0 63. UCBBMP5 0 0 0 0 325. hHSCP1B1 379 0 0 0 0. hHSCP1B2 359 0 0 0 0. hHSCP1B3 377 0 0 0 0. hHSCP2B1 376 0 0 0 0. hHSCP2B2 350 0 0 0 0. hHSCP3B1 371 0 0 0 0. hHSCP3B2 340 0 0 0 0. hHSCP4B1 290 0 0 0 0. ```. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""sex""]). ```. ```pytb. sex F M U. 384plate . 27YOMP1 0 368 0. 27YOMP2 0 383 0. 27YOMP3 0 184 0. BM01152P1 0 384 0. BM01152P2 0 384 0. BM01158P1 0 382 0. BM01158P2 0 384 0. BM8182P1 0 55 0. FBFLP5L2 0 55 327. FBML1 0 0 378. FBML2 0 0 322. FBP5L1 0 0 377. FLP5L1 0 0 381. FLP5L3 0 338 0. FLP6L1 0 376 0. FLP6L2 0 71 312. FLP6L3 0 0 53. UCB250P1 0 382 0. UCB250P2 0 384 0. UCB250P3 0 150 0. UCB259P1 0 373 0. UCB259P2 0 376 0. UCB270P1 0 382 0. UCB270P2 0 128 0. UCBBMP4 0 376 0. UCBBMP5 0 325 0. hHSCP1B1 0 379 0. hHSCP1B2 173 186 0. hHSCP1B3 216 161 0. hHSCP2B1 0 376 0. hHSCP2B2 350 0 0. hHSCP3B1 0 371 0. hHSCP3B2 158 182 0. hHSCP4B1 0 290 0. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:33,safety,Error,Error,33,"Thank you so much for the reply. Error posted above is happening in two independent datasets possibly due to confounding factors as you mentioned. Though I don't quite comprehend what is a confounding factor in my specific case. . Here is the output of the batches and covariates as requested. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]). ```. ```pytb. age_group Old YoungAdult Pediatric Fetal NewBorn. 384plate . 27YOMP1 0 368 0 0 0. 27YOMP2 0 383 0 0 0. 27YOMP3 0 184 0 0 0. BM01152P1 0 384 0 0 0. BM01152P2 0 384 0 0 0. BM01158P1 0 0 382 0 0. BM01158P2 0 0 384 0 0. BM8182P1 0 55 0 0 0. FBFLP5L2 0 0 0 382 0. FBML1 0 0 0 378 0. FBML2 0 0 0 322 0. FBP5L1 0 0 0 377 0. FLP5L1 0 0 0 381 0. FLP5L3 0 0 0 338 0. FLP6L1 0 0 0 376 0. FLP6L2 0 0 0 383 0. FLP6L3 0 0 0 53 0. UCB250P1 0 0 0 0 382. UCB250P2 0 0 0 0 384. UCB250P3 0 0 0 0 150. UCB259P1 0 0 0 0 373. UCB259P2 0 0 0 0 376. UCB270P1 0 0 0 0 382. UCB270P2 0 0 0 0 128. UCBBMP4 0 41 272 0 63. UCBBMP5 0 0 0 0 325. hHSCP1B1 379 0 0 0 0. hHSCP1B2 359 0 0 0 0. hHSCP1B3 377 0 0 0 0. hHSCP2B1 376 0 0 0 0. hHSCP2B2 350 0 0 0 0. hHSCP3B1 371 0 0 0 0. hHSCP3B2 340 0 0 0 0. hHSCP4B1 290 0 0 0 0. ```. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""sex""]). ```. ```pytb. sex F M U. 384plate . 27YOMP1 0 368 0. 27YOMP2 0 383 0. 27YOMP3 0 184 0. BM01152P1 0 384 0. BM01152P2 0 384 0. BM01158P1 0 382 0. BM01158P2 0 384 0. BM8182P1 0 55 0. FBFLP5L2 0 55 327. FBML1 0 0 378. FBML2 0 0 322. FBP5L1 0 0 377. FLP5L1 0 0 381. FLP5L3 0 338 0. FLP6L1 0 376 0. FLP6L2 0 71 312. FLP6L3 0 0 53. UCB250P1 0 382 0. UCB250P2 0 384 0. UCB250P3 0 150 0. UCB259P1 0 373 0. UCB259P2 0 376 0. UCB270P1 0 382 0. UCB270P2 0 128 0. UCBBMP4 0 376 0. UCBBMP5 0 325 0. hHSCP1B1 0 379 0. hHSCP1B2 173 186 0. hHSCP1B3 216 161 0. hHSCP2B1 0 376 0. hHSCP2B2 350 0 0. hHSCP3B1 0 371 0. hHSCP3B2 158 182 0. hHSCP4B1 0 290 0. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:33,usability,Error,Error,33,"Thank you so much for the reply. Error posted above is happening in two independent datasets possibly due to confounding factors as you mentioned. Though I don't quite comprehend what is a confounding factor in my specific case. . Here is the output of the batches and covariates as requested. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""age_group""]). ```. ```pytb. age_group Old YoungAdult Pediatric Fetal NewBorn. 384plate . 27YOMP1 0 368 0 0 0. 27YOMP2 0 383 0 0 0. 27YOMP3 0 184 0 0 0. BM01152P1 0 384 0 0 0. BM01152P2 0 384 0 0 0. BM01158P1 0 0 382 0 0. BM01158P2 0 0 384 0 0. BM8182P1 0 55 0 0 0. FBFLP5L2 0 0 0 382 0. FBML1 0 0 0 378 0. FBML2 0 0 0 322 0. FBP5L1 0 0 0 377 0. FLP5L1 0 0 0 381 0. FLP5L3 0 0 0 338 0. FLP6L1 0 0 0 376 0. FLP6L2 0 0 0 383 0. FLP6L3 0 0 0 53 0. UCB250P1 0 0 0 0 382. UCB250P2 0 0 0 0 384. UCB250P3 0 0 0 0 150. UCB259P1 0 0 0 0 373. UCB259P2 0 0 0 0 376. UCB270P1 0 0 0 0 382. UCB270P2 0 0 0 0 128. UCBBMP4 0 41 272 0 63. UCBBMP5 0 0 0 0 325. hHSCP1B1 379 0 0 0 0. hHSCP1B2 359 0 0 0 0. hHSCP1B3 377 0 0 0 0. hHSCP2B1 376 0 0 0 0. hHSCP2B2 350 0 0 0 0. hHSCP3B1 371 0 0 0 0. hHSCP3B2 340 0 0 0 0. hHSCP4B1 290 0 0 0 0. ```. ```python. pd.crosstab(adata.obs[""384plate""], adata.obs[""sex""]). ```. ```pytb. sex F M U. 384plate . 27YOMP1 0 368 0. 27YOMP2 0 383 0. 27YOMP3 0 184 0. BM01152P1 0 384 0. BM01152P2 0 384 0. BM01158P1 0 382 0. BM01158P2 0 384 0. BM8182P1 0 55 0. FBFLP5L2 0 55 327. FBML1 0 0 378. FBML2 0 0 322. FBP5L1 0 0 377. FLP5L1 0 0 381. FLP5L3 0 338 0. FLP6L1 0 376 0. FLP6L2 0 71 312. FLP6L3 0 0 53. UCB250P1 0 382 0. UCB250P2 0 384 0. UCB250P3 0 150 0. UCB259P1 0 373 0. UCB259P2 0 376 0. UCB270P1 0 382 0. UCB270P2 0 128 0. UCBBMP4 0 376 0. UCBBMP5 0 325 0. hHSCP1B1 0 379 0. hHSCP1B2 173 186 0. hHSCP1B3 216 161 0. hHSCP2B1 0 376 0. hHSCP2B2 350 0 0. hHSCP3B1 0 371 0. hHSCP3B2 158 182 0. hHSCP4B1 0 290 0. ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:54,deployability,contain,contain,54,"I believe the issue is that there are no plates which contain ""Old""/ ""Fetal"" samples and any other kind. So these categories are completely confounded with plate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:129,safety,compl,completely,129,"I believe the issue is that there are no plates which contain ""Old""/ ""Fetal"" samples and any other kind. So these categories are completely confounded with plate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:129,security,compl,completely,129,"I believe the issue is that there are no plates which contain ""Old""/ ""Fetal"" samples and any other kind. So these categories are completely confounded with plate.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:304,energy efficiency,model,model,304,"Hi @transcriptomics, . If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:393,energy efficiency,model,model,393,"Hi @transcriptomics, . If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:319,interoperability,specif,specify,319,"Hi @transcriptomics, . If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:432,interoperability,distribut,distribution,432,"Hi @transcriptomics, . If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:337,modifiability,paramet,parameters,337,"Hi @transcriptomics, . If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:170,safety,test,testing,170,"Hi @transcriptomics, . If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:304,security,model,model,304,"Hi @transcriptomics, . If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:393,security,model,model,393,"Hi @transcriptomics, . If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:170,testability,test,testing,170,"Hi @transcriptomics, . If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:38,usability,learn,learn,38,"Hi @transcriptomics, . If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:106,usability,guid,guide,106,"Hi @transcriptomics, . If you want to learn a little more about confounding, there's a pretty nice recent guide to setting up design matrices for differential expression testing [here](https://f1000research.com/articles/9-1444/v1). This is a very related issue as ComBat essentially fits the statistical model that you specify with your parameters in a similar manner than you would with a DE model. In brief, the issue is that the distribution of covariates makes it impossible for the statistical fit to prioritize whether to assign the variation in cells that are e.g., on plates starting with ""F"" to variation from being fetal or variation due to being on those plates.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:185,integrability,batch,batch,185,Thanks so much for explanations. This would mean that this is not a bug as I had thought rather a design issue. For this to work one would need somewhat non-overlapping distribution of batch key and covariates. Does not seem there are easy workarounds in this case!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:169,interoperability,distribut,distribution,169,Thanks so much for explanations. This would mean that this is not a bug as I had thought rather a design issue. For this to work one would need somewhat non-overlapping distribution of batch key and covariates. Does not seem there are easy workarounds in this case!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:185,performance,batch,batch,185,Thanks so much for explanations. This would mean that this is not a bug as I had thought rather a design issue. For this to work one would need somewhat non-overlapping distribution of batch key and covariates. Does not seem there are easy workarounds in this case!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:211,reliability,Doe,Does,211,Thanks so much for explanations. This would mean that this is not a bug as I had thought rather a design issue. For this to work one would need somewhat non-overlapping distribution of batch key and covariates. Does not seem there are easy workarounds in this case!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:134,integrability,batch,batch,134,"Actually, you need a very overlapping distribution of all covariates you want to fit. So that every covariate is represented in every batch ideally. There is indeed no workaround. It's an experimental design issue that now dictates what you can correct for an what you cannot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:38,interoperability,distribut,distribution,38,"Actually, you need a very overlapping distribution of all covariates you want to fit. So that every covariate is represented in every batch ideally. There is indeed no workaround. It's an experimental design issue that now dictates what you can correct for an what you cannot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1606:134,performance,batch,batch,134,"Actually, you need a very overlapping distribution of all covariates you want to fit. So that every covariate is represented in every batch ideally. There is indeed no workaround. It's an experimental design issue that now dictates what you can correct for an what you cannot.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1606
https://github.com/scverse/scanpy/issues/1611:90,energy efficiency,heat,heat,90,"This would definitely be useful. @fidelram, you were thinking of including more `seaborn` heat map features like this, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:217,availability,cluster,clustermaps-with-multiple-legends-in-seaborn,217,"here are some examples, may they help. https://stackoverflow.com/questions/27988846/how-to-express-classes-on-the-axis-of-a-heatmap-in-seaborn. https://stackoverflow.com/questions/60177861/plotting-annotated-heatmaps-clustermaps-with-multiple-legends-in-seaborn. https://stackoverflow.com/questions/61816216/seaborn-clustermap-with-two-row-colors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:316,availability,cluster,clustermap-with-two-row-colors,316,"here are some examples, may they help. https://stackoverflow.com/questions/27988846/how-to-express-classes-on-the-axis-of-a-heatmap-in-seaborn. https://stackoverflow.com/questions/60177861/plotting-annotated-heatmaps-clustermaps-with-multiple-legends-in-seaborn. https://stackoverflow.com/questions/61816216/seaborn-clustermap-with-two-row-colors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:47,deployability,stack,stackoverflow,47,"here are some examples, may they help. https://stackoverflow.com/questions/27988846/how-to-express-classes-on-the-axis-of-a-heatmap-in-seaborn. https://stackoverflow.com/questions/60177861/plotting-annotated-heatmaps-clustermaps-with-multiple-legends-in-seaborn. https://stackoverflow.com/questions/61816216/seaborn-clustermap-with-two-row-colors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:152,deployability,stack,stackoverflow,152,"here are some examples, may they help. https://stackoverflow.com/questions/27988846/how-to-express-classes-on-the-axis-of-a-heatmap-in-seaborn. https://stackoverflow.com/questions/60177861/plotting-annotated-heatmaps-clustermaps-with-multiple-legends-in-seaborn. https://stackoverflow.com/questions/61816216/seaborn-clustermap-with-two-row-colors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:217,deployability,cluster,clustermaps-with-multiple-legends-in-seaborn,217,"here are some examples, may they help. https://stackoverflow.com/questions/27988846/how-to-express-classes-on-the-axis-of-a-heatmap-in-seaborn. https://stackoverflow.com/questions/60177861/plotting-annotated-heatmaps-clustermaps-with-multiple-legends-in-seaborn. https://stackoverflow.com/questions/61816216/seaborn-clustermap-with-two-row-colors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:271,deployability,stack,stackoverflow,271,"here are some examples, may they help. https://stackoverflow.com/questions/27988846/how-to-express-classes-on-the-axis-of-a-heatmap-in-seaborn. https://stackoverflow.com/questions/60177861/plotting-annotated-heatmaps-clustermaps-with-multiple-legends-in-seaborn. https://stackoverflow.com/questions/61816216/seaborn-clustermap-with-two-row-colors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:316,deployability,cluster,clustermap-with-two-row-colors,316,"here are some examples, may they help. https://stackoverflow.com/questions/27988846/how-to-express-classes-on-the-axis-of-a-heatmap-in-seaborn. https://stackoverflow.com/questions/60177861/plotting-annotated-heatmaps-clustermaps-with-multiple-legends-in-seaborn. https://stackoverflow.com/questions/61816216/seaborn-clustermap-with-two-row-colors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:124,energy efficiency,heat,heatmap-in-seaborn,124,"here are some examples, may they help. https://stackoverflow.com/questions/27988846/how-to-express-classes-on-the-axis-of-a-heatmap-in-seaborn. https://stackoverflow.com/questions/60177861/plotting-annotated-heatmaps-clustermaps-with-multiple-legends-in-seaborn. https://stackoverflow.com/questions/61816216/seaborn-clustermap-with-two-row-colors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:208,energy efficiency,heat,heatmaps-clustermaps-with-multiple-legends-in-seaborn,208,"here are some examples, may they help. https://stackoverflow.com/questions/27988846/how-to-express-classes-on-the-axis-of-a-heatmap-in-seaborn. https://stackoverflow.com/questions/60177861/plotting-annotated-heatmaps-clustermaps-with-multiple-legends-in-seaborn. https://stackoverflow.com/questions/61816216/seaborn-clustermap-with-two-row-colors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:33,usability,help,help,33,"here are some examples, may they help. https://stackoverflow.com/questions/27988846/how-to-express-classes-on-the-axis-of-a-heatmap-in-seaborn. https://stackoverflow.com/questions/60177861/plotting-annotated-heatmaps-clustermaps-with-multiple-legends-in-seaborn. https://stackoverflow.com/questions/61816216/seaborn-clustermap-with-two-row-colors.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:5,availability,cluster,clustermap,5,"The `clustermap` ([docs](https://seaborn.pydata.org/generated/seaborn.clustermap.html)) function does this. It doesn't do legends though, so that's something we would need to add ourselves.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:70,availability,cluster,clustermap,70,"The `clustermap` ([docs](https://seaborn.pydata.org/generated/seaborn.clustermap.html)) function does this. It doesn't do legends though, so that's something we would need to add ourselves.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:5,deployability,cluster,clustermap,5,"The `clustermap` ([docs](https://seaborn.pydata.org/generated/seaborn.clustermap.html)) function does this. It doesn't do legends though, so that's something we would need to add ourselves.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:70,deployability,cluster,clustermap,70,"The `clustermap` ([docs](https://seaborn.pydata.org/generated/seaborn.clustermap.html)) function does this. It doesn't do legends though, so that's something we would need to add ourselves.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:97,reliability,doe,does,97,"The `clustermap` ([docs](https://seaborn.pydata.org/generated/seaborn.clustermap.html)) function does this. It doesn't do legends though, so that's something we would need to add ourselves.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:111,reliability,doe,doesn,111,"The `clustermap` ([docs](https://seaborn.pydata.org/generated/seaborn.clustermap.html)) function does this. It doesn't do legends though, so that's something we would need to add ourselves.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1611:56,energy efficiency,heat,heatmaps,56,"Hello, was capability of adding multiple annotations to heatmaps implemented in scanpy?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1611
https://github.com/scverse/scanpy/issues/1612:405,interoperability,distribut,distribution,405,"Do your input objects have different `dtype` values in `X`? I suspect that is what's causing this. If so, are the results very different? I would expect normalizing 64 bit vs 32 bit values to not be exactly the same (which is what `np.array_equal` is testing), but it's not good if the function is returning very different values. You can check this with `np.all_close`/ `np.isclose` or by looking at the distribution of the differences of the results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:8,safety,input,input,8,"Do your input objects have different `dtype` values in `X`? I suspect that is what's causing this. If so, are the results very different? I would expect normalizing 64 bit vs 32 bit values to not be exactly the same (which is what `np.array_equal` is testing), but it's not good if the function is returning very different values. You can check this with `np.all_close`/ `np.isclose` or by looking at the distribution of the differences of the results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:251,safety,test,testing,251,"Do your input objects have different `dtype` values in `X`? I suspect that is what's causing this. If so, are the results very different? I would expect normalizing 64 bit vs 32 bit values to not be exactly the same (which is what `np.array_equal` is testing), but it's not good if the function is returning very different values. You can check this with `np.all_close`/ `np.isclose` or by looking at the distribution of the differences of the results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:251,testability,test,testing,251,"Do your input objects have different `dtype` values in `X`? I suspect that is what's causing this. If so, are the results very different? I would expect normalizing 64 bit vs 32 bit values to not be exactly the same (which is what `np.array_equal` is testing), but it's not good if the function is returning very different values. You can check this with `np.all_close`/ `np.isclose` or by looking at the distribution of the differences of the results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:8,usability,input,input,8,"Do your input objects have different `dtype` values in `X`? I suspect that is what's causing this. If so, are the results very different? I would expect normalizing 64 bit vs 32 bit values to not be exactly the same (which is what `np.array_equal` is testing), but it's not good if the function is returning very different values. You can check this with `np.all_close`/ `np.isclose` or by looking at the distribution of the differences of the results.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:142,availability,slo,slot,142,"Hi @ivirshup,. The in-house dataset is build from 2 `AnnData` objects, both have sparse matrix of type `'<class 'numpy.float32'>'` in the `X` slot. I just ran (after the normalisation step):. ```. np.allclose(adata_run1_concat.X.todense(), adata_run2_concat.X.todense()). ```. which gives me:. ```. True. ```. As a lambda user, I would still find it confusing to have different (altough very similar) results (e.g.: embeddings) for 2 identical runs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:39,deployability,build,build,39,"Hi @ivirshup,. The in-house dataset is build from 2 `AnnData` objects, both have sparse matrix of type `'<class 'numpy.float32'>'` in the `X` slot. I just ran (after the normalisation step):. ```. np.allclose(adata_run1_concat.X.todense(), adata_run2_concat.X.todense()). ```. which gives me:. ```. True. ```. As a lambda user, I would still find it confusing to have different (altough very similar) results (e.g.: embeddings) for 2 identical runs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:142,reliability,slo,slot,142,"Hi @ivirshup,. The in-house dataset is build from 2 `AnnData` objects, both have sparse matrix of type `'<class 'numpy.float32'>'` in the `X` slot. I just ran (after the normalisation step):. ```. np.allclose(adata_run1_concat.X.todense(), adata_run2_concat.X.todense()). ```. which gives me:. ```. True. ```. As a lambda user, I would still find it confusing to have different (altough very similar) results (e.g.: embeddings) for 2 identical runs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:434,security,ident,identical,434,"Hi @ivirshup,. The in-house dataset is build from 2 `AnnData` objects, both have sparse matrix of type `'<class 'numpy.float32'>'` in the `X` slot. I just ran (after the normalisation step):. ```. np.allclose(adata_run1_concat.X.todense(), adata_run2_concat.X.todense()). ```. which gives me:. ```. True. ```. As a lambda user, I would still find it confusing to have different (altough very similar) results (e.g.: embeddings) for 2 identical runs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:322,usability,user,user,322,"Hi @ivirshup,. The in-house dataset is build from 2 `AnnData` objects, both have sparse matrix of type `'<class 'numpy.float32'>'` in the `X` slot. I just ran (after the normalisation step):. ```. np.allclose(adata_run1_concat.X.todense(), adata_run2_concat.X.todense()). ```. which gives me:. ```. True. ```. As a lambda user, I would still find it confusing to have different (altough very similar) results (e.g.: embeddings) for 2 identical runs",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:176,integrability,sub,subset,176,"If both inputs had equal data of the same dtype, then they should definitely be the same after normalization. Would you be able to share the data (or at least some reproducing subset) so this can be investigated further?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:131,interoperability,share,share,131,"If both inputs had equal data of the same dtype, then they should definitely be the same after normalization. Would you be able to share the data (or at least some reproducing subset) so this can be investigated further?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:8,safety,input,inputs,8,"If both inputs had equal data of the same dtype, then they should definitely be the same after normalization. Would you be able to share the data (or at least some reproducing subset) so this can be investigated further?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1612:8,usability,input,inputs,8,"If both inputs had equal data of the same dtype, then they should definitely be the same after normalization. Would you be able to share the data (or at least some reproducing subset) so this can be investigated further?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1612
https://github.com/scverse/scanpy/issues/1613:43,deployability,log,logging,43,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:107,deployability,depend,dependencies,107,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:212,deployability,version,version,212,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:107,integrability,depend,dependencies,107,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:212,integrability,version,version,212,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:107,modifiability,depend,dependencies,107,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:212,modifiability,version,version,212,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:127,reliability,doe,does,127,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:43,safety,log,logging,43,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:87,safety,compl,complete,87,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:107,safety,depend,dependencies,107,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:43,security,log,logging,43,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:87,security,compl,complete,87,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:43,testability,log,logging,43,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/issues/1613:107,testability,depend,dependencies,107,"I would definitely recommend using the `sc.logging.print_versions` function for a more complete listing of dependencies, which does include `pynndescent`. That said, I'm not against adding it to the more compact version.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1613
https://github.com/scverse/scanpy/pull/1614:80,integrability,transform,transforming,80,"Hi @fidelram ! . looks good yes, with `distance.squareform` you are essentially transforming the corr in a condensed distance matrix right? and that's what `pdist` also passes. So I think it looks good now! Another option would have been to pass `mean_df.T` directly to linkage, but I understand wanting to keep correlation as distances.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1614:80,interoperability,transform,transforming,80,"Hi @fidelram ! . looks good yes, with `distance.squareform` you are essentially transforming the corr in a condensed distance matrix right? and that's what `pdist` also passes. So I think it looks good now! Another option would have been to pass `mean_df.T` directly to linkage, but I understand wanting to keep correlation as distances.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1614:285,testability,understand,understand,285,"Hi @fidelram ! . looks good yes, with `distance.squareform` you are essentially transforming the corr in a condensed distance matrix right? and that's what `pdist` also passes. So I think it looks good now! Another option would have been to pass `mean_df.T` directly to linkage, but I understand wanting to keep correlation as distances.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1614:25,usability,clear,clear,25,Makes sense! Thanks it's clear now why the corr matrix,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1614:22,usability,close,closed,22,Issues that should be closed:. * https://github.com/theislab/scanpy/issues/1288,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1614:122,deployability,automat,automatically,122,"@fidelram, in future, if you add a line like: . ```. Fixes #x, #y. ```. To the initial comment in a PR, those issues will automatically be closed when the PR is merged. I think you can also link PRs to issues with the ""Linked issues"" element in the righthand sidebar. Using these makes it easier to tell which issues have been addressed already.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1614:122,testability,automat,automatically,122,"@fidelram, in future, if you add a line like: . ```. Fixes #x, #y. ```. To the initial comment in a PR, those issues will automatically be closed when the PR is merged. I think you can also link PRs to issues with the ""Linked issues"" element in the righthand sidebar. Using these makes it easier to tell which issues have been addressed already.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/pull/1614:139,usability,close,closed,139,"@fidelram, in future, if you add a line like: . ```. Fixes #x, #y. ```. To the initial comment in a PR, those issues will automatically be closed when the PR is merged. I think you can also link PRs to issues with the ""Linked issues"" element in the righthand sidebar. Using these makes it easier to tell which issues have been addressed already.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1614
https://github.com/scverse/scanpy/issues/1619:60,usability,help,help,60,I would be keen to see densmap added to scanpy. I can maybe help with a PR but not sure whether it's better/prefered to add a new `sc.tl.densmap()` function or add arguments to the existing `sc.tl.umap()`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:108,usability,prefer,prefered,108,I would be keen to see densmap added to scanpy. I can maybe help with a PR but not sure whether it's better/prefered to add a new `sc.tl.densmap()` function or add arguments to the existing `sc.tl.umap()`?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:361,integrability,transform,transform,361,"> I can maybe help with a PR but not sure whether it's better/prefered to add a new sc.tl.densmap(). Good question. In UMAP, I believe it's just a key word argument. My instinct would be separate function, since it takes a few new arguments and has a few new values associated. Also, could potentially add a different key in `obsm`, since I don't think you can transform data into the existing space and this may conflict with `ingest` (any thoughts @Koncopd?). I would also be interested in any thoughts you had on the applicability/ usefulness of the method in general. One question that comes up for me, is PCA space a reasonable original space to be using here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:361,interoperability,transform,transform,361,"> I can maybe help with a PR but not sure whether it's better/prefered to add a new sc.tl.densmap(). Good question. In UMAP, I believe it's just a key word argument. My instinct would be separate function, since it takes a few new arguments and has a few new values associated. Also, could potentially add a different key in `obsm`, since I don't think you can transform data into the existing space and this may conflict with `ingest` (any thoughts @Koncopd?). I would also be interested in any thoughts you had on the applicability/ usefulness of the method in general. One question that comes up for me, is PCA space a reasonable original space to be using here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:413,interoperability,conflict,conflict,413,"> I can maybe help with a PR but not sure whether it's better/prefered to add a new sc.tl.densmap(). Good question. In UMAP, I believe it's just a key word argument. My instinct would be separate function, since it takes a few new arguments and has a few new values associated. Also, could potentially add a different key in `obsm`, since I don't think you can transform data into the existing space and this may conflict with `ingest` (any thoughts @Koncopd?). I would also be interested in any thoughts you had on the applicability/ usefulness of the method in general. One question that comes up for me, is PCA space a reasonable original space to be using here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:14,usability,help,help,14,"> I can maybe help with a PR but not sure whether it's better/prefered to add a new sc.tl.densmap(). Good question. In UMAP, I believe it's just a key word argument. My instinct would be separate function, since it takes a few new arguments and has a few new values associated. Also, could potentially add a different key in `obsm`, since I don't think you can transform data into the existing space and this may conflict with `ingest` (any thoughts @Koncopd?). I would also be interested in any thoughts you had on the applicability/ usefulness of the method in general. One question that comes up for me, is PCA space a reasonable original space to be using here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:62,usability,prefer,prefered,62,"> I can maybe help with a PR but not sure whether it's better/prefered to add a new sc.tl.densmap(). Good question. In UMAP, I believe it's just a key word argument. My instinct would be separate function, since it takes a few new arguments and has a few new values associated. Also, could potentially add a different key in `obsm`, since I don't think you can transform data into the existing space and this may conflict with `ingest` (any thoughts @Koncopd?). I would also be interested in any thoughts you had on the applicability/ usefulness of the method in general. One question that comes up for me, is PCA space a reasonable original space to be using here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:82,integrability,coupl,couple,82,I think being able to see density could be really useful but I've only tried it a couple of times so not sure how it goes in practice. A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. The umap developers adding it to the main package seems like a good argument that it's useful though. I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? The function would let the user choose the input though right (with some sensible default)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:82,modifiability,coupl,couple,82,I think being able to see density could be really useful but I've only tried it a couple of times so not sure how it goes in practice. A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. The umap developers adding it to the main package seems like a good argument that it's useful though. I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? The function would let the user choose the input though right (with some sensible default)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:261,modifiability,pac,package,261,I think being able to see density could be really useful but I've only tried it a couple of times so not sure how it goes in practice. A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. The umap developers adding it to the main package seems like a good argument that it's useful though. I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? The function would let the user choose the input though right (with some sensible default)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:92,performance,time,times,92,I think being able to see density could be really useful but I've only tried it a couple of times so not sure how it goes in practice. A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. The umap developers adding it to the main package seems like a good argument that it's useful though. I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? The function would let the user choose the input though right (with some sensible default)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:125,reliability,pra,practice,125,I think being able to see density could be really useful but I've only tried it a couple of times so not sure how it goes in practice. A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. The umap developers adding it to the main package seems like a good argument that it's useful though. I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? The function would let the user choose the input though right (with some sensible default)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:344,safety,input,inputs,344,I think being able to see density could be really useful but I've only tried it a couple of times so not sure how it goes in practice. A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. The umap developers adding it to the main package seems like a good argument that it's useful though. I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? The function would let the user choose the input though right (with some sensible default)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:478,safety,input,input,478,I think being able to see density could be really useful but I've only tried it a couple of times so not sure how it goes in practice. A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. The umap developers adding it to the main package seems like a good argument that it's useful though. I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? The function would let the user choose the input though right (with some sensible default)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:82,testability,coupl,couple,82,I think being able to see density could be really useful but I've only tried it a couple of times so not sure how it goes in practice. A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. The umap developers adding it to the main package seems like a good argument that it's useful though. I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? The function would let the user choose the input though right (with some sensible default)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:344,usability,input,inputs,344,I think being able to see density could be really useful but I've only tried it a couple of times so not sure how it goes in practice. A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. The umap developers adding it to the main package seems like a good argument that it's useful though. I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? The function would let the user choose the input though right (with some sensible default)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:462,usability,user,user,462,I think being able to see density could be really useful but I've only tried it a couple of times so not sure how it goes in practice. A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. The umap developers adding it to the main package seems like a good argument that it's useful though. I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? The function would let the user choose the input though right (with some sensible default)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:478,usability,input,input,478,I think being able to see density could be really useful but I've only tried it a couple of times so not sure how it goes in practice. A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. The umap developers adding it to the main package seems like a good argument that it's useful though. I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? The function would let the user choose the input though right (with some sensible default)?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:248,deployability,modul,module,248,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:657,deployability,depend,dependent,657,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:657,integrability,depend,dependent,657,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:701,integrability,compon,components,701,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:701,interoperability,compon,components,701,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:756,interoperability,share,shared,756,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:248,modifiability,modul,module,248,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:300,modifiability,pac,package,300,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:657,modifiability,depend,dependent,657,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:701,modifiability,compon,components,701,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:248,safety,modul,module,248,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:539,safety,input,inputs,539,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:657,safety,depend,dependent,657,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:657,testability,depend,dependent,657,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:178,usability,user,users,178,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:539,usability,input,inputs,539,"> A lot of the motivation for wanting it in scanpy is to make it easier to try out 😸. We've definitely ended up with a number of features this way, but more so when we had fewer users... I think this would be a good candidate for an `experimental` module. > The umap developers adding it to the main package seems like a good argument that it's useful though. I should look through the paper on this more carefully, but from my initial skimming I wasn't particularly convinced the relative density was meaningful. > I hadn't thought about inputs but first guess is that PCA (or equivalent) should capture density relatively well? I'm suspicious it could be dependent on dataset make-up, e.g. what the components represent and whether they are likely to be shared.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:45,energy efficiency,current,current,45,"Ok, well I have half a function based on the current UMAP function which seems to work ok if that ends up being helpful for whatever implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:112,usability,help,helpful,112,"Ok, well I have half a function based on the current UMAP function which seems to work ok if that ends up being helpful for whatever implementation.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:45,deployability,modul,module,45,"Great, I think we may go for an experimental module as soon as next release (though, maybe the one after). If you form any strong opinions or have any cool use cases for the function, I'd definitely be interested in hearing about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:68,deployability,releas,release,68,"Great, I think we may go for an experimental module as soon as next release (though, maybe the one after). If you form any strong opinions or have any cool use cases for the function, I'd definitely be interested in hearing about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:151,energy efficiency,cool,cool,151,"Great, I think we may go for an experimental module as soon as next release (though, maybe the one after). If you form any strong opinions or have any cool use cases for the function, I'd definitely be interested in hearing about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:45,modifiability,modul,module,45,"Great, I think we may go for an experimental module as soon as next release (though, maybe the one after). If you form any strong opinions or have any cool use cases for the function, I'd definitely be interested in hearing about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:45,safety,modul,module,45,"Great, I think we may go for an experimental module as soon as next release (though, maybe the one after). If you form any strong opinions or have any cool use cases for the function, I'd definitely be interested in hearing about it.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:28,safety,test,testing,28,"Hi, did any one ever end up testing Densmap for scRNA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1619:28,testability,test,testing,28,"Hi, did any one ever end up testing Densmap for scRNA ?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1619
https://github.com/scverse/scanpy/issues/1625:42,deployability,version,versions,42,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:155,deployability,releas,released,155,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:164,deployability,version,version,164,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:325,deployability,updat,updated,325,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:42,integrability,version,versions,42,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:164,integrability,version,version,164,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:42,modifiability,version,versions,42,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:164,modifiability,version,version,164,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:270,modifiability,paramet,parameter,270,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:325,safety,updat,updated,325,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:325,security,updat,updated,325,"This might just be the case as the scanpy versions are different. I fixed `marker_genes_overlap` at some point, and that should be in 1.7.0 but not in any released version before that. To get the same results just add the `top_n_marker=100` (if that was the name of the parameter). This was caused by rank_genes_groups being updated to output not just the top 100 genes by default anymore.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:51,deployability,version,version,51,@LuckyMD I checked the commits. Between the Scanpy version on agando and the latest release the `marker_genes_overlap` was not changed. But maybe I am blind. I'll go for the empirical route and try it out. Will report back!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:84,deployability,releas,release,84,@LuckyMD I checked the commits. Between the Scanpy version on agando and the latest release the `marker_genes_overlap` was not changed. But maybe I am blind. I'll go for the empirical route and try it out. Will report back!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:51,integrability,version,version,51,@LuckyMD I checked the commits. Between the Scanpy version on agando and the latest release the `marker_genes_overlap` was not changed. But maybe I am blind. I'll go for the empirical route and try it out. Will report back!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:184,integrability,rout,route,184,@LuckyMD I checked the commits. Between the Scanpy version on agando and the latest release the `marker_genes_overlap` was not changed. But maybe I am blind. I'll go for the empirical route and try it out. Will report back!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:51,modifiability,version,version,51,@LuckyMD I checked the commits. Between the Scanpy version on agando and the latest release the `marker_genes_overlap` was not changed. But maybe I am blind. I'll go for the empirical route and try it out. Will report back!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1625:13,usability,help,help,13,Glad I could help :),MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1625
https://github.com/scverse/scanpy/issues/1626:7,energy efficiency,current,currently,7,"We are currently using `sklearn.utils.check_random_state` to validate the argument for `random_state` in most places. Sometime's (especially in external) we pass the argument directly to the wrapped tool. In sklearn `0.24.1`, this looks like `np.random.RandomState` if you pass an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626
https://github.com/scverse/scanpy/issues/1626:191,integrability,wrap,wrapped,191,"We are currently using `sklearn.utils.check_random_state` to validate the argument for `random_state` in most places. Sometime's (especially in external) we pass the argument directly to the wrapped tool. In sklearn `0.24.1`, this looks like `np.random.RandomState` if you pass an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626
https://github.com/scverse/scanpy/issues/1626:61,safety,valid,validate,61,"We are currently using `sklearn.utils.check_random_state` to validate the argument for `random_state` in most places. Sometime's (especially in external) we pass the argument directly to the wrapped tool. In sklearn `0.24.1`, this looks like `np.random.RandomState` if you pass an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626
https://github.com/scverse/scanpy/issues/1626:61,security,validat,validate,61,"We are currently using `sklearn.utils.check_random_state` to validate the argument for `random_state` in most places. Sometime's (especially in external) we pass the argument directly to the wrapped tool. In sklearn `0.24.1`, this looks like `np.random.RandomState` if you pass an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626
https://github.com/scverse/scanpy/issues/1626:199,usability,tool,tool,199,"We are currently using `sklearn.utils.check_random_state` to validate the argument for `random_state` in most places. Sometime's (especially in external) we pass the argument directly to the wrapped tool. In sklearn `0.24.1`, this looks like `np.random.RandomState` if you pass an integer.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1626
https://github.com/scverse/scanpy/issues/1629:937,availability,ping,pinging,937,"hey, thanks for the interest and very good questions, my 2 cents:. > Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think you could yes, maybe complementary to some standard approaches like hypergeometric test? > How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? since you have densitieis, it should be ok (?). you could also try subsampling the condition where you have more samples n times. > I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. thanks, very much appreciated! Actually I don't think we have really a class/example of density/line plots in scanpy. Not sure if it can be of broad use/scope. . pinging @dawe (original author of the function).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:592,integrability,sub,subsampling,592,"hey, thanks for the interest and very good questions, my 2 cents:. > Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think you could yes, maybe complementary to some standard approaches like hypergeometric test? > How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? since you have densitieis, it should be ok (?). you could also try subsampling the condition where you have more samples n times. > I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. thanks, very much appreciated! Actually I don't think we have really a class/example of density/line plots in scanpy. Not sure if it can be of broad use/scope. . pinging @dawe (original author of the function).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:328,interoperability,standard,standard,328,"hey, thanks for the interest and very good questions, my 2 cents:. > Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think you could yes, maybe complementary to some standard approaches like hypergeometric test? > How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? since you have densitieis, it should be ok (?). you could also try subsampling the condition where you have more samples n times. > I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. thanks, very much appreciated! Actually I don't think we have really a class/example of density/line plots in scanpy. Not sure if it can be of broad use/scope. . pinging @dawe (original author of the function).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:704,interoperability,distribut,distributions,704,"hey, thanks for the interest and very good questions, my 2 cents:. > Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think you could yes, maybe complementary to some standard approaches like hypergeometric test? > How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? since you have densitieis, it should be ok (?). you could also try subsampling the condition where you have more samples n times. > I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. thanks, very much appreciated! Actually I don't think we have really a class/example of density/line plots in scanpy. Not sure if it can be of broad use/scope. . pinging @dawe (original author of the function).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:648,performance,time,times,648,"hey, thanks for the interest and very good questions, my 2 cents:. > Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think you could yes, maybe complementary to some standard approaches like hypergeometric test? > How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? since you have densitieis, it should be ok (?). you could also try subsampling the condition where you have more samples n times. > I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. thanks, very much appreciated! Actually I don't think we have really a class/example of density/line plots in scanpy. Not sure if it can be of broad use/scope. . pinging @dawe (original author of the function).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:380,reliability,doe,does,380,"hey, thanks for the interest and very good questions, my 2 cents:. > Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think you could yes, maybe complementary to some standard approaches like hypergeometric test? > How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? since you have densitieis, it should be ok (?). you could also try subsampling the condition where you have more samples n times. > I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. thanks, very much appreciated! Actually I don't think we have really a class/example of density/line plots in scanpy. Not sure if it can be of broad use/scope. . pinging @dawe (original author of the function).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:306,safety,compl,complementary,306,"hey, thanks for the interest and very good questions, my 2 cents:. > Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think you could yes, maybe complementary to some standard approaches like hypergeometric test? > How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? since you have densitieis, it should be ok (?). you could also try subsampling the condition where you have more samples n times. > I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. thanks, very much appreciated! Actually I don't think we have really a class/example of density/line plots in scanpy. Not sure if it can be of broad use/scope. . pinging @dawe (original author of the function).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:368,safety,test,test,368,"hey, thanks for the interest and very good questions, my 2 cents:. > Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think you could yes, maybe complementary to some standard approaches like hypergeometric test? > How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? since you have densitieis, it should be ok (?). you could also try subsampling the condition where you have more samples n times. > I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. thanks, very much appreciated! Actually I don't think we have really a class/example of density/line plots in scanpy. Not sure if it can be of broad use/scope. . pinging @dawe (original author of the function).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:306,security,compl,complementary,306,"hey, thanks for the interest and very good questions, my 2 cents:. > Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think you could yes, maybe complementary to some standard approaches like hypergeometric test? > How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? since you have densitieis, it should be ok (?). you could also try subsampling the condition where you have more samples n times. > I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. thanks, very much appreciated! Actually I don't think we have really a class/example of density/line plots in scanpy. Not sure if it can be of broad use/scope. . pinging @dawe (original author of the function).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:961,security,auth,author,961,"hey, thanks for the interest and very good questions, my 2 cents:. > Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think you could yes, maybe complementary to some standard approaches like hypergeometric test? > How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? since you have densitieis, it should be ok (?). you could also try subsampling the condition where you have more samples n times. > I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. thanks, very much appreciated! Actually I don't think we have really a class/example of density/line plots in scanpy. Not sure if it can be of broad use/scope. . pinging @dawe (original author of the function).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:368,testability,test,test,368,"hey, thanks for the interest and very good questions, my 2 cents:. > Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think you could yes, maybe complementary to some standard approaches like hypergeometric test? > How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? since you have densitieis, it should be ok (?). you could also try subsampling the condition where you have more samples n times. > I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. thanks, very much appreciated! Actually I don't think we have really a class/example of density/line plots in scanpy. Not sure if it can be of broad use/scope. . pinging @dawe (original author of the function).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:721,usability,visual,visualise,721,"hey, thanks for the interest and very good questions, my 2 cents:. > Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think you could yes, maybe complementary to some standard approaches like hypergeometric test? > How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? since you have densitieis, it should be ok (?). you could also try subsampling the condition where you have more samples n times. > I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. thanks, very much appreciated! Actually I don't think we have really a class/example of density/line plots in scanpy. Not sure if it can be of broad use/scope. . pinging @dawe (original author of the function).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:382,availability,state,state,382,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:1067,energy efficiency,estimat,estimate,1067,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:382,integrability,state,state,382,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:373,interoperability,specif,specific,373,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:413,interoperability,distribut,distribution,413,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:700,interoperability,distribut,distributions,700,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:1080,interoperability,distribut,distributions,1080,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:1147,interoperability,distribut,distributions,1147,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:662,performance,perform,perform,662,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:846,reliability,doe,does,846,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:686,safety,test,test,686,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:429,security,sign,signatures,429,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:1280,security,sign,signatures,1280,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:686,testability,test,test,686,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:662,usability,perform,perform,662,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:1164,usability,visual,visualise,1164,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:1313,usability,visual,visualize,1313,"Hi, sorry for the late response... > 1. Can we infer from such an analysis how much a pathway is upregulated? (e.g. by calculating the FC of the mean?) . It would be great to conclude for example, that Pathway X is 30% more active, in condition Y. I think so. Historically, this function has been used to score cell cycle and, in that case, one can say that cells are in a specific state *because* of a different distribution of signatures. This is generally true. I have myself used the score to underline cells with activated/depleted pathways. Also, I have used gene lists from KEGG or Reactome to score single cells. IMHO, once you have those values you can perform any statistical test on their distributions to tell if there's a difference in activation of a certain pathway. There may be better ways to do this, but it's a start. > 2. How does in your opinion class-imbalance affect the analysis? For example, Condition A has 10 samples, while for Condition B,C.. I only have 3 each? As @giovp pointed out, it should be ok, as long as you have enough cells to estimate the distributions. > 3. I am happy to provide the code for the density distributions to visualise the results of the gene-set-score function. What I usually do is to calculate the `embedding_density` for signatures, so that it's easy to visualize them on my embeddings (I usually cut values into quartiles).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:49,reliability,doe,does,49,"that's great explanation, thanks @dawe ! @Elhl93 does this resolve the issue for you? If you'd still like to contribute to a PR for plotting let me know! I'll close this for the moment but please feel free to reopen for any issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1629:159,usability,close,close,159,"that's great explanation, thanks @dawe ! @Elhl93 does this resolve the issue for you? If you'd still like to contribute to a PR for plotting let me know! I'll close this for the moment but please feel free to reopen for any issue.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1629
https://github.com/scverse/scanpy/issues/1630:68,availability,slo,slot,68,"Yes, looking at the source code, the `""vst""` method uses the counts slot. https://github.com/satijalab/seurat/blob/69dbc46c82c5aaa5c2fd275a5d7bf28c04351c6a/R/preprocessing.R#L1817-L1825. In the unit tests, we test directly against the output of the Seurat method. The only known difference is how HVGs are aggregated across batches. I could not reproduce their results, as some of the R code was unclear to me. It would be helpful if someone fixed that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630
https://github.com/scverse/scanpy/issues/1630:324,integrability,batch,batches,324,"Yes, looking at the source code, the `""vst""` method uses the counts slot. https://github.com/satijalab/seurat/blob/69dbc46c82c5aaa5c2fd275a5d7bf28c04351c6a/R/preprocessing.R#L1817-L1825. In the unit tests, we test directly against the output of the Seurat method. The only known difference is how HVGs are aggregated across batches. I could not reproduce their results, as some of the R code was unclear to me. It would be helpful if someone fixed that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630
https://github.com/scverse/scanpy/issues/1630:324,performance,batch,batches,324,"Yes, looking at the source code, the `""vst""` method uses the counts slot. https://github.com/satijalab/seurat/blob/69dbc46c82c5aaa5c2fd275a5d7bf28c04351c6a/R/preprocessing.R#L1817-L1825. In the unit tests, we test directly against the output of the Seurat method. The only known difference is how HVGs are aggregated across batches. I could not reproduce their results, as some of the R code was unclear to me. It would be helpful if someone fixed that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630
https://github.com/scverse/scanpy/issues/1630:68,reliability,slo,slot,68,"Yes, looking at the source code, the `""vst""` method uses the counts slot. https://github.com/satijalab/seurat/blob/69dbc46c82c5aaa5c2fd275a5d7bf28c04351c6a/R/preprocessing.R#L1817-L1825. In the unit tests, we test directly against the output of the Seurat method. The only known difference is how HVGs are aggregated across batches. I could not reproduce their results, as some of the R code was unclear to me. It would be helpful if someone fixed that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630
https://github.com/scverse/scanpy/issues/1630:199,safety,test,tests,199,"Yes, looking at the source code, the `""vst""` method uses the counts slot. https://github.com/satijalab/seurat/blob/69dbc46c82c5aaa5c2fd275a5d7bf28c04351c6a/R/preprocessing.R#L1817-L1825. In the unit tests, we test directly against the output of the Seurat method. The only known difference is how HVGs are aggregated across batches. I could not reproduce their results, as some of the R code was unclear to me. It would be helpful if someone fixed that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630
https://github.com/scverse/scanpy/issues/1630:209,safety,test,test,209,"Yes, looking at the source code, the `""vst""` method uses the counts slot. https://github.com/satijalab/seurat/blob/69dbc46c82c5aaa5c2fd275a5d7bf28c04351c6a/R/preprocessing.R#L1817-L1825. In the unit tests, we test directly against the output of the Seurat method. The only known difference is how HVGs are aggregated across batches. I could not reproduce their results, as some of the R code was unclear to me. It would be helpful if someone fixed that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630
https://github.com/scverse/scanpy/issues/1630:194,testability,unit,unit,194,"Yes, looking at the source code, the `""vst""` method uses the counts slot. https://github.com/satijalab/seurat/blob/69dbc46c82c5aaa5c2fd275a5d7bf28c04351c6a/R/preprocessing.R#L1817-L1825. In the unit tests, we test directly against the output of the Seurat method. The only known difference is how HVGs are aggregated across batches. I could not reproduce their results, as some of the R code was unclear to me. It would be helpful if someone fixed that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630
https://github.com/scverse/scanpy/issues/1630:199,testability,test,tests,199,"Yes, looking at the source code, the `""vst""` method uses the counts slot. https://github.com/satijalab/seurat/blob/69dbc46c82c5aaa5c2fd275a5d7bf28c04351c6a/R/preprocessing.R#L1817-L1825. In the unit tests, we test directly against the output of the Seurat method. The only known difference is how HVGs are aggregated across batches. I could not reproduce their results, as some of the R code was unclear to me. It would be helpful if someone fixed that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630
https://github.com/scverse/scanpy/issues/1630:209,testability,test,test,209,"Yes, looking at the source code, the `""vst""` method uses the counts slot. https://github.com/satijalab/seurat/blob/69dbc46c82c5aaa5c2fd275a5d7bf28c04351c6a/R/preprocessing.R#L1817-L1825. In the unit tests, we test directly against the output of the Seurat method. The only known difference is how HVGs are aggregated across batches. I could not reproduce their results, as some of the R code was unclear to me. It would be helpful if someone fixed that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630
https://github.com/scverse/scanpy/issues/1630:423,usability,help,helpful,423,"Yes, looking at the source code, the `""vst""` method uses the counts slot. https://github.com/satijalab/seurat/blob/69dbc46c82c5aaa5c2fd275a5d7bf28c04351c6a/R/preprocessing.R#L1817-L1825. In the unit tests, we test directly against the output of the Seurat method. The only known difference is how HVGs are aggregated across batches. I could not reproduce their results, as some of the R code was unclear to me. It would be helpful if someone fixed that at some point.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630
https://github.com/scverse/scanpy/issues/1630:25,usability,clear,clear,25,"Thanks @adamgayoso, it's clear now.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1630
https://github.com/scverse/scanpy/issues/1631:61,usability,tool,tools,61,"Hi Matt,. No particular reason. We'd definitely like to have tools for this in our ecosystem!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1631
https://github.com/scverse/scanpy/pull/1632:190,reliability,Doe,Does,190,"Example of what the `dotplot` docs look like (locally) once this is working:. ![image](https://user-images.githubusercontent.com/8238804/107312688-122e2080-6ae5-11eb-8a7e-f61c51a8392c.png). Does make that `bbox_inches=""tight""` option seem important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:95,usability,user,user-images,95,"Example of what the `dotplot` docs look like (locally) once this is working:. ![image](https://user-images.githubusercontent.com/8238804/107312688-122e2080-6ae5-11eb-8a7e-f61c51a8392c.png). Does make that `bbox_inches=""tight""` option seem important.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:318,deployability,updat,updated,318,"@ivirshup This looks great! Thanks. The issue with the cropping of the labels is quite annoying and indeed `bbox_inches=""tight""` should help. However, I don't think is nice to add that line for each example, but, on the other hand, if we add this to the scanpy code, many figures will be affected and would need to be updated. The same issue also affects the test images which most are cropped. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:318,safety,updat,updated,318,"@ivirshup This looks great! Thanks. The issue with the cropping of the labels is quite annoying and indeed `bbox_inches=""tight""` should help. However, I don't think is nice to add that line for each example, but, on the other hand, if we add this to the scanpy code, many figures will be affected and would need to be updated. The same issue also affects the test images which most are cropped. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:359,safety,test,test,359,"@ivirshup This looks great! Thanks. The issue with the cropping of the labels is quite annoying and indeed `bbox_inches=""tight""` should help. However, I don't think is nice to add that line for each example, but, on the other hand, if we add this to the scanpy code, many figures will be affected and would need to be updated. The same issue also affects the test images which most are cropped. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:318,security,updat,updated,318,"@ivirshup This looks great! Thanks. The issue with the cropping of the labels is quite annoying and indeed `bbox_inches=""tight""` should help. However, I don't think is nice to add that line for each example, but, on the other hand, if we add this to the scanpy code, many figures will be affected and would need to be updated. The same issue also affects the test images which most are cropped. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:359,testability,test,test,359,"@ivirshup This looks great! Thanks. The issue with the cropping of the labels is quite annoying and indeed `bbox_inches=""tight""` should help. However, I don't think is nice to add that line for each example, but, on the other hand, if we add this to the scanpy code, many figures will be affected and would need to be updated. The same issue also affects the test images which most are cropped. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:136,usability,help,help,136,"@ivirshup This looks great! Thanks. The issue with the cropping of the labels is quite annoying and indeed `bbox_inches=""tight""` should help. However, I don't think is nice to add that line for each example, but, on the other hand, if we add this to the scanpy code, many figures will be affected and would need to be updated. The same issue also affects the test images which most are cropped. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:254,availability,error,error,254,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:457,availability,error,error,457,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:883,availability,error,error,883,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:0,deployability,Updat,Update,0,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:84,deployability,updat,updated,84,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:207,deployability,build,builds,207,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:218,deployability,fail,failing,218,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:691,deployability,build,builds,691,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:1040,modifiability,pac,packages,1040,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:254,performance,error,error,254,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:457,performance,error,error,457,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:883,performance,error,error,883,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:218,reliability,fail,failing,218,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:0,safety,Updat,Update,0,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:84,safety,updat,updated,84,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:254,safety,error,error,254,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:457,safety,error,error,457,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:883,safety,error,error,883,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:0,security,Updat,Update,0,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:84,security,updat,updated,84,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:533,testability,context,context,533,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:254,usability,error,error,254,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:457,usability,error,error,457,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:542,usability,close,close-figs,542,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:883,usability,error,error,883,"Update: Nvm, I figured this out. Tt had to do with `qualname_overrides`, which I've updated. <details>. <summary> Old problem </summary>. @flying-sheep, weird sphinx bug I'm running into:. * The readthedocs builds are failing after commit fc83ec3. * The error is:. ```pytb. scanpy/scanpy/external/pp/_bbknn.py:docstring of scanpy.external.pp.bbknn:24: WARNING: py:class reference target not found: sklearn.neighbors._dist_metrics.DistanceMetric. ```. * The error will still occur as long as this has been added:. ```rst. .. plot::. :context: close-figs. >>> import scanpy as sc. >>> adata = sc.datasets.pbmc68k_reduced(). >>> sc.tl.umap(adata). ```. * If I remove the `sc.tl.umap` line, the builds work fine, as `sklearn.neighbors.DistanceMetric` resolves and no warning is thrown. For now, I'm going to remove the type annotation from `sc.external.pp.bbknn`, since it's causing the error. Any ideas why calling `sc.tl.umap` means `sklearn.neighbors._dist_metrics.DistanceMetric` can no longer resolve? I assume it has something to do with packages being imported in an unexpected order, but also this is real weird. </details>",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:172,interoperability,specif,specifically,172,"@fidelram . I don't think making a broader change would be so bad right now, especially if it deals with this issue. However, if there was a way to just fix the issue here specifically that would also be good. Separate issue these examples raise:. It looks like the colormap is changing at some point when it shouldn't:. ![image](https://user-images.githubusercontent.com/8238804/107342280-1d4c7500-6b14-11eb-85dc-a0217c31c042.png). Any idea what's up with this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:338,usability,user,user-images,338,"@fidelram . I don't think making a broader change would be so bad right now, especially if it deals with this issue. However, if there was a way to just fix the issue here specifically that would also be good. Separate issue these examples raise:. It looks like the colormap is changing at some point when it shouldn't:. ![image](https://user-images.githubusercontent.com/8238804/107342280-1d4c7500-6b14-11eb-85dc-a0217c31c042.png). Any idea what's up with this?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:295,deployability,releas,release,295,"For this PR, I'm thinking I'm going to wait on a response from @flying-sheep to figure out what to do about the sklearn intersphinx problem, then clean it up for a merge. A new issue will be opened up with all the functions that need examples, and those can be added through separate PRs as the release progresses.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:303,usability,progress,progresses,303,"For this PR, I'm thinking I'm going to wait on a response from @flying-sheep to figure out what to do about the sklearn intersphinx problem, then clean it up for a merge. A new issue will be opened up with all the functions that need examples, and those can be added through separate PRs as the release progresses.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:120,usability,visual,visualisation,120,"@ivirshup the color change is caused because of the way the defaults are used. This is an issue that has to do with the visualisation of defaults in the documentation. . When you add `.style(....)` as in the second case you report, the colormap changes to `winter` although it should stay as `Reds`. This happens because the method default value of `cmap` is `winter`. Ideally, it should be `None` in which case it can be decided if the colormap was already set or if a default is needed. However, setting the default of the colormap to `None` removes the default information from the sphinx documentation. We had some discussions with @flying-sheep on how to address this that are still pending but ideas on how to address this are welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:153,usability,document,documentation,153,"@ivirshup the color change is caused because of the way the defaults are used. This is an issue that has to do with the visualisation of defaults in the documentation. . When you add `.style(....)` as in the second case you report, the colormap changes to `winter` although it should stay as `Reds`. This happens because the method default value of `cmap` is `winter`. Ideally, it should be `None` in which case it can be decided if the colormap was already set or if a default is needed. However, setting the default of the colormap to `None` removes the default information from the sphinx documentation. We had some discussions with @flying-sheep on how to address this that are still pending but ideas on how to address this are welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/pull/1632:592,usability,document,documentation,592,"@ivirshup the color change is caused because of the way the defaults are used. This is an issue that has to do with the visualisation of defaults in the documentation. . When you add `.style(....)` as in the second case you report, the colormap changes to `winter` although it should stay as `Reds`. This happens because the method default value of `cmap` is `winter`. Ideally, it should be `None` in which case it can be decided if the colormap was already set or if a default is needed. However, setting the default of the colormap to `None` removes the default information from the sphinx documentation. We had some discussions with @flying-sheep on how to address this that are still pending but ideas on how to address this are welcome.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1632
https://github.com/scverse/scanpy/issues/1633:33,modifiability,paramet,parameter,33,I think it would be fine for the parameter description to define the behavior. I don't think it particularly matters whether the default parameter is None or not. I think this is a pretty common solution to this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:137,modifiability,paramet,parameter,137,I think it would be fine for the parameter description to define the behavior. I don't think it particularly matters whether the default parameter is None or not. I think this is a pretty common solution to this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:69,usability,behavi,behavior,69,I think it would be fine for the parameter description to define the behavior. I don't think it particularly matters whether the default parameter is None or not. I think this is a pretty common solution to this issue.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:344,deployability,updat,update,344,"Changing the behavior of the `style()` methods to no longer reset everything to the default values means that the parameter defaults of that methods *shouldn’t* mention the instance defaults, because it will no longer have anything to do with them. Specifying defaults in text descriptions practically doesn’t work, because nobody remembers to update the defaults in 2 places. So unless there is a DRY solution, let’s not do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:249,interoperability,Specif,Specifying,249,"Changing the behavior of the `style()` methods to no longer reset everything to the default values means that the parameter defaults of that methods *shouldn’t* mention the instance defaults, because it will no longer have anything to do with them. Specifying defaults in text descriptions practically doesn’t work, because nobody remembers to update the defaults in 2 places. So unless there is a DRY solution, let’s not do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:114,modifiability,paramet,parameter,114,"Changing the behavior of the `style()` methods to no longer reset everything to the default values means that the parameter defaults of that methods *shouldn’t* mention the instance defaults, because it will no longer have anything to do with them. Specifying defaults in text descriptions practically doesn’t work, because nobody remembers to update the defaults in 2 places. So unless there is a DRY solution, let’s not do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:290,reliability,pra,practically,290,"Changing the behavior of the `style()` methods to no longer reset everything to the default values means that the parameter defaults of that methods *shouldn’t* mention the instance defaults, because it will no longer have anything to do with them. Specifying defaults in text descriptions practically doesn’t work, because nobody remembers to update the defaults in 2 places. So unless there is a DRY solution, let’s not do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:302,reliability,doe,doesn,302,"Changing the behavior of the `style()` methods to no longer reset everything to the default values means that the parameter defaults of that methods *shouldn’t* mention the instance defaults, because it will no longer have anything to do with them. Specifying defaults in text descriptions practically doesn’t work, because nobody remembers to update the defaults in 2 places. So unless there is a DRY solution, let’s not do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:331,safety,reme,remembers,331,"Changing the behavior of the `style()` methods to no longer reset everything to the default values means that the parameter defaults of that methods *shouldn’t* mention the instance defaults, because it will no longer have anything to do with them. Specifying defaults in text descriptions practically doesn’t work, because nobody remembers to update the defaults in 2 places. So unless there is a DRY solution, let’s not do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:344,safety,updat,update,344,"Changing the behavior of the `style()` methods to no longer reset everything to the default values means that the parameter defaults of that methods *shouldn’t* mention the instance defaults, because it will no longer have anything to do with them. Specifying defaults in text descriptions practically doesn’t work, because nobody remembers to update the defaults in 2 places. So unless there is a DRY solution, let’s not do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:344,security,updat,update,344,"Changing the behavior of the `style()` methods to no longer reset everything to the default values means that the parameter defaults of that methods *shouldn’t* mention the instance defaults, because it will no longer have anything to do with them. Specifying defaults in text descriptions practically doesn’t work, because nobody remembers to update the defaults in 2 places. So unless there is a DRY solution, let’s not do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1633:13,usability,behavi,behavior,13,"Changing the behavior of the `style()` methods to no longer reset everything to the default values means that the parameter defaults of that methods *shouldn’t* mention the instance defaults, because it will no longer have anything to do with them. Specifying defaults in text descriptions practically doesn’t work, because nobody remembers to update the defaults in 2 places. So unless there is a DRY solution, let’s not do that.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1633
https://github.com/scverse/scanpy/issues/1636:124,availability,error,error,124,"@galamm, thanks for the report! Also thanks for the example, very useful! I think I see what the issue is here, though your error is unexpected. Are you using the most recent version of scanpy (1.7.0)? The `gene_symbols` argument is supposed to refer to column in `var` that has more human readable gene names. The idea here is that you might have some unique identifier as `var_names` (like ensembl ids), but would have something more interpretable sorted in `adata.var[gene_symbols]`. On my machine, I get a `KeyError` when I run your example since there is no column `""TEST""` in `adata.var`. This is expected. It's strange to me that you get a `NameError`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:175,deployability,version,version,175,"@galamm, thanks for the report! Also thanks for the example, very useful! I think I see what the issue is here, though your error is unexpected. Are you using the most recent version of scanpy (1.7.0)? The `gene_symbols` argument is supposed to refer to column in `var` that has more human readable gene names. The idea here is that you might have some unique identifier as `var_names` (like ensembl ids), but would have something more interpretable sorted in `adata.var[gene_symbols]`. On my machine, I get a `KeyError` when I run your example since there is no column `""TEST""` in `adata.var`. This is expected. It's strange to me that you get a `NameError`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:175,integrability,version,version,175,"@galamm, thanks for the report! Also thanks for the example, very useful! I think I see what the issue is here, though your error is unexpected. Are you using the most recent version of scanpy (1.7.0)? The `gene_symbols` argument is supposed to refer to column in `var` that has more human readable gene names. The idea here is that you might have some unique identifier as `var_names` (like ensembl ids), but would have something more interpretable sorted in `adata.var[gene_symbols]`. On my machine, I get a `KeyError` when I run your example since there is no column `""TEST""` in `adata.var`. This is expected. It's strange to me that you get a `NameError`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:175,modifiability,version,version,175,"@galamm, thanks for the report! Also thanks for the example, very useful! I think I see what the issue is here, though your error is unexpected. Are you using the most recent version of scanpy (1.7.0)? The `gene_symbols` argument is supposed to refer to column in `var` that has more human readable gene names. The idea here is that you might have some unique identifier as `var_names` (like ensembl ids), but would have something more interpretable sorted in `adata.var[gene_symbols]`. On my machine, I get a `KeyError` when I run your example since there is no column `""TEST""` in `adata.var`. This is expected. It's strange to me that you get a `NameError`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:124,performance,error,error,124,"@galamm, thanks for the report! Also thanks for the example, very useful! I think I see what the issue is here, though your error is unexpected. Are you using the most recent version of scanpy (1.7.0)? The `gene_symbols` argument is supposed to refer to column in `var` that has more human readable gene names. The idea here is that you might have some unique identifier as `var_names` (like ensembl ids), but would have something more interpretable sorted in `adata.var[gene_symbols]`. On my machine, I get a `KeyError` when I run your example since there is no column `""TEST""` in `adata.var`. This is expected. It's strange to me that you get a `NameError`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:124,safety,error,error,124,"@galamm, thanks for the report! Also thanks for the example, very useful! I think I see what the issue is here, though your error is unexpected. Are you using the most recent version of scanpy (1.7.0)? The `gene_symbols` argument is supposed to refer to column in `var` that has more human readable gene names. The idea here is that you might have some unique identifier as `var_names` (like ensembl ids), but would have something more interpretable sorted in `adata.var[gene_symbols]`. On my machine, I get a `KeyError` when I run your example since there is no column `""TEST""` in `adata.var`. This is expected. It's strange to me that you get a `NameError`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:572,safety,TEST,TEST,572,"@galamm, thanks for the report! Also thanks for the example, very useful! I think I see what the issue is here, though your error is unexpected. Are you using the most recent version of scanpy (1.7.0)? The `gene_symbols` argument is supposed to refer to column in `var` that has more human readable gene names. The idea here is that you might have some unique identifier as `var_names` (like ensembl ids), but would have something more interpretable sorted in `adata.var[gene_symbols]`. On my machine, I get a `KeyError` when I run your example since there is no column `""TEST""` in `adata.var`. This is expected. It's strange to me that you get a `NameError`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:360,security,ident,identifier,360,"@galamm, thanks for the report! Also thanks for the example, very useful! I think I see what the issue is here, though your error is unexpected. Are you using the most recent version of scanpy (1.7.0)? The `gene_symbols` argument is supposed to refer to column in `var` that has more human readable gene names. The idea here is that you might have some unique identifier as `var_names` (like ensembl ids), but would have something more interpretable sorted in `adata.var[gene_symbols]`. On my machine, I get a `KeyError` when I run your example since there is no column `""TEST""` in `adata.var`. This is expected. It's strange to me that you get a `NameError`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:572,testability,TEST,TEST,572,"@galamm, thanks for the report! Also thanks for the example, very useful! I think I see what the issue is here, though your error is unexpected. Are you using the most recent version of scanpy (1.7.0)? The `gene_symbols` argument is supposed to refer to column in `var` that has more human readable gene names. The idea here is that you might have some unique identifier as `var_names` (like ensembl ids), but would have something more interpretable sorted in `adata.var[gene_symbols]`. On my machine, I get a `KeyError` when I run your example since there is no column `""TEST""` in `adata.var`. This is expected. It's strange to me that you get a `NameError`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:124,usability,error,error,124,"@galamm, thanks for the report! Also thanks for the example, very useful! I think I see what the issue is here, though your error is unexpected. Are you using the most recent version of scanpy (1.7.0)? The `gene_symbols` argument is supposed to refer to column in `var` that has more human readable gene names. The idea here is that you might have some unique identifier as `var_names` (like ensembl ids), but would have something more interpretable sorted in `adata.var[gene_symbols]`. On my machine, I get a `KeyError` when I run your example since there is no column `""TEST""` in `adata.var`. This is expected. It's strange to me that you get a `NameError`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:280,availability,cluster,clusters,280,"Hello! I also have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:853,availability,toler,tolerance,853,"Hello! I also have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1679,availability,error,error,1679,"o have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this parameter? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:280,deployability,cluster,clusters,280,"Hello! I also have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:790,energy efficiency,core,core,790,"Hello! I also have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:82,modifiability,paramet,parameter,82,"Hello! I also have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:774,modifiability,pac,packages,774,"Hello! I also have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1004,modifiability,pac,packages,1004," also have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this paramete",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1135,modifiability,pac,packages,1135,"o have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this parameter? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2000,modifiability,paramet,parameter,2000,"o have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this parameter? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1679,performance,error,error,1679,"o have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this parameter? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:853,reliability,toleran,tolerance,853,"Hello! I also have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:930,safety,except,except,930,"Hello! I also have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1679,safety,error,error,1679,"o have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this parameter? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1284,security,hash,hashtable,1284,"o have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this parameter? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1391,security,hash,hashtable,1391,"o have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this parameter? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:690,testability,Trace,Traceback,690,"Hello! I also have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1679,usability,error,error,1679,"o have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this parameter? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1982,usability,behavi,behavior,1982,"o have been running into issues when trying to use the `gene_symbols` parameter with the `sc.pl.dotplot()` function despite the column with the proper `gene_symbols` being in my `adata.var` Data Frame. . ```. $ adata.var.columns. $ sc.pl.dotplot(adata, marker_genes, 'clusters', dendrogram=True, gene_symbols='alternate_gene_symbols'). ==============================================================================. Index(['gene_symbols', 'feature_types', 'n_cells', 'highly_variable', 'means',. 'dispersions', 'dispersions_norm', 'mean', 'std',. 'alternate_gene_symbols'],. dtype='object'). ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621, in Index.get_loc(self, key, method, tolerance). 3620 try:. -> 3621 return self._engine.get_loc(casted_key). 3622 except KeyError as err:. File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:136, in pandas._libs.index.IndexEngine.get_loc(). File ~/miniconda3/envs/scanpy/lib/python3.9/site-packages/pandas/_libs/index.pyx:163, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:5198, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:5206, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'alternate_gene_symbols'. ... ```. When I tried setting `adata.var['gene_symbols'] = adata.var['alternate_gene_symbols']` and trying to generate a `dotplot` with a random gene present in `alternate_gene_symbols`, I ran into the following error: . ```. ... KeyError: ""Could not find keys '['KH.C1.159.']' in columns of `adata.obs` or in adata.raw.var['gene_symbols']."". ```. It seems that `sc.pl.dotplot()` is expecting `gene_symbols` that are present in the `adata.raw.var` Data Frame versus the `adata.var` Data Frame. Is this the expected behavior for this parameter? .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1346,availability,error,error,1346,"with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:26,deployability,version,version,26,"Hello! I'm running scanpy version 1.9.3 now and it seems that bug is still not fixed since it was found in scanpy 1.6.0. . The situation is the same as in previous comments. I created the new column in the adata.var with some names changed to the GenBank ID (I'm working with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:280,energy efficiency,model,model,280,"Hello! I'm running scanpy version 1.9.3 now and it seems that bug is still not fixed since it was found in scanpy 1.6.0. . The situation is the same as in previous comments. I created the new column in the adata.var with some names changed to the GenBank ID (I'm working with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1533,energy efficiency,core,core,1533,"with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:26,integrability,version,version,26,"Hello! I'm running scanpy version 1.9.3 now and it seems that bug is still not fixed since it was found in scanpy 1.6.0. . The situation is the same as in previous comments. I created the new column in the adata.var with some names changed to the GenBank ID (I'm working with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:26,modifiability,version,version,26,"Hello! I'm running scanpy version 1.9.3 now and it seems that bug is still not fixed since it was found in scanpy 1.6.0. . The situation is the same as in previous comments. I created the new column in the adata.var with some names changed to the GenBank ID (I'm working with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1517,modifiability,pac,packages,1517,"with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1346,performance,error,error,1346,"with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1346,safety,error,error,1346,"with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1654,safety,except,except,1654,"with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:280,security,model,model,280,"Hello! I'm running scanpy version 1.9.3 now and it seems that bug is still not fixed since it was found in scanpy 1.6.0. . The situation is the same as in previous comments. I created the new column in the adata.var with some names changed to the GenBank ID (I'm working with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:464,security,access,accessions,464,"Hello! I'm running scanpy version 1.9.3 now and it seems that bug is still not fixed since it was found in scanpy 1.6.0. . The situation is the same as in previous comments. I created the new column in the adata.var with some names changed to the GenBank ID (I'm working with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjec",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1876,security,hash,hashtable,1876,"with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1983,security,hash,hashtable,1983,"with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1441,testability,Trace,Traceback,1441,"with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2058,testability,understand,understand,2058,"with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:1346,usability,error,error,1346,"with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/issues/1636:2258,usability,help,help,2258,"with non-model species and the majority of gene names are non-informative like ""nbis-gene-11111"", but I am interested in some genes of actin that I deposited in GenBank. I would like to put GB accessions into the plot.) I created the column with following code: [""bob"" is the dataset name]. bob.var['GB_IDs'] = bob.var_names.copy(). ID_dict = {. ""nbis-gene-777"":""MT451954"",. ""nbis-gene-775"":""MT451955"",. ""nbis-gene-3785"":""MT451956"",. ""nbis-gene-3784"":""MT451957"",. ""nbis-gene-23114"":""MT451958"",. ""nbis-gene-25113"":""MT451959"",. ""nbis-gene-3783"":""MT518195"". }. bob.var['GB_IDs'].replace(ID_dict, inplace=True). After that GB_IDs column was present in the dataframe. And then I tried to plot the dotplot:. dict = {. ""Actin 1"": [""nbis-gene-777""],. ""Actin 2"": [""nbis-gene-775""],. ""Actin 3"": [""nbis-gene-3785""],. ""Actin 4"": [""nbis-gene-3784""],. ""Actin 5"": [""nbis-gene-23114""],. ""Actin 6"": [""nbis-gene-25113""],. ""Actin 7"": [""nbis-gene-3783""]. }. dp=sc.pl.dotplot(bob, dict, ""scGate_multi"", dendrogram=False, return_fig=True, cmap='YlGnBu', gene_symbols='GB_IDs'). This results in an error: . ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). File ~/software/SAMap/lib/python3.9/site-packages/pandas/core/indexes/base.py:3791, in Index.get_loc(self, key). 3790 try:. -> 3791 return self._engine.get_loc(casted_key). 3792 except KeyError as err:. File index.pyx:152, in pandas._libs.index.IndexEngine.get_loc(). File index.pyx:181, in pandas._libs.index.IndexEngine.get_loc(). File pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item(). File pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item(). KeyError: 'GB_IDs'. If I correctly understand the docs (https://scanpy.readthedocs.io/en/latest/generated/scanpy.pl.dotplot.html), this code should work. I tried also to create such additional column in adata.raw.var, but that did not help as well.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1636
https://github.com/scverse/scanpy/pull/1637:10,reliability,Doe,Does,10,"@ivirshup Does this PR also solve the issue when keys is a str? I thought somewhere in the code we needed to add something like. ```python. if isinstance(keys, str):. keys = [keys]. ````",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1637
https://github.com/scverse/scanpy/pull/1642:77,availability,error,error,77,"So, without any evidence, I think it should be fine. The reason I had put an error in the first place is that the typical behavior is to pass log normalized data to this HVG function, and I didn't want people to run this incorrectly. I think another solution would be to just throw a UserWarning, though in a way I like the idea of having an argument that disables the `check_nonnegative_integer()`. I think I would call it `enforce_counts_seurat_v3` though. You might also consider bypassing the check if the flag is set, because it can be slowish for large datasets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:541,availability,slo,slowish,541,"So, without any evidence, I think it should be fine. The reason I had put an error in the first place is that the typical behavior is to pass log normalized data to this HVG function, and I didn't want people to run this incorrectly. I think another solution would be to just throw a UserWarning, though in a way I like the idea of having an argument that disables the `check_nonnegative_integer()`. I think I would call it `enforce_counts_seurat_v3` though. You might also consider bypassing the check if the flag is set, because it can be slowish for large datasets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:142,deployability,log,log,142,"So, without any evidence, I think it should be fine. The reason I had put an error in the first place is that the typical behavior is to pass log normalized data to this HVG function, and I didn't want people to run this incorrectly. I think another solution would be to just throw a UserWarning, though in a way I like the idea of having an argument that disables the `check_nonnegative_integer()`. I think I would call it `enforce_counts_seurat_v3` though. You might also consider bypassing the check if the flag is set, because it can be slowish for large datasets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:77,performance,error,error,77,"So, without any evidence, I think it should be fine. The reason I had put an error in the first place is that the typical behavior is to pass log normalized data to this HVG function, and I didn't want people to run this incorrectly. I think another solution would be to just throw a UserWarning, though in a way I like the idea of having an argument that disables the `check_nonnegative_integer()`. I think I would call it `enforce_counts_seurat_v3` though. You might also consider bypassing the check if the flag is set, because it can be slowish for large datasets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:541,reliability,slo,slowish,541,"So, without any evidence, I think it should be fine. The reason I had put an error in the first place is that the typical behavior is to pass log normalized data to this HVG function, and I didn't want people to run this incorrectly. I think another solution would be to just throw a UserWarning, though in a way I like the idea of having an argument that disables the `check_nonnegative_integer()`. I think I would call it `enforce_counts_seurat_v3` though. You might also consider bypassing the check if the flag is set, because it can be slowish for large datasets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:77,safety,error,error,77,"So, without any evidence, I think it should be fine. The reason I had put an error in the first place is that the typical behavior is to pass log normalized data to this HVG function, and I didn't want people to run this incorrectly. I think another solution would be to just throw a UserWarning, though in a way I like the idea of having an argument that disables the `check_nonnegative_integer()`. I think I would call it `enforce_counts_seurat_v3` though. You might also consider bypassing the check if the flag is set, because it can be slowish for large datasets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:142,safety,log,log,142,"So, without any evidence, I think it should be fine. The reason I had put an error in the first place is that the typical behavior is to pass log normalized data to this HVG function, and I didn't want people to run this incorrectly. I think another solution would be to just throw a UserWarning, though in a way I like the idea of having an argument that disables the `check_nonnegative_integer()`. I think I would call it `enforce_counts_seurat_v3` though. You might also consider bypassing the check if the flag is set, because it can be slowish for large datasets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:142,security,log,log,142,"So, without any evidence, I think it should be fine. The reason I had put an error in the first place is that the typical behavior is to pass log normalized data to this HVG function, and I didn't want people to run this incorrectly. I think another solution would be to just throw a UserWarning, though in a way I like the idea of having an argument that disables the `check_nonnegative_integer()`. I think I would call it `enforce_counts_seurat_v3` though. You might also consider bypassing the check if the flag is set, because it can be slowish for large datasets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:142,testability,log,log,142,"So, without any evidence, I think it should be fine. The reason I had put an error in the first place is that the typical behavior is to pass log normalized data to this HVG function, and I didn't want people to run this incorrectly. I think another solution would be to just throw a UserWarning, though in a way I like the idea of having an argument that disables the `check_nonnegative_integer()`. I think I would call it `enforce_counts_seurat_v3` though. You might also consider bypassing the check if the flag is set, because it can be slowish for large datasets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:77,usability,error,error,77,"So, without any evidence, I think it should be fine. The reason I had put an error in the first place is that the typical behavior is to pass log normalized data to this HVG function, and I didn't want people to run this incorrectly. I think another solution would be to just throw a UserWarning, though in a way I like the idea of having an argument that disables the `check_nonnegative_integer()`. I think I would call it `enforce_counts_seurat_v3` though. You might also consider bypassing the check if the flag is set, because it can be slowish for large datasets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:122,usability,behavi,behavior,122,"So, without any evidence, I think it should be fine. The reason I had put an error in the first place is that the typical behavior is to pass log normalized data to this HVG function, and I didn't want people to run this incorrectly. I think another solution would be to just throw a UserWarning, though in a way I like the idea of having an argument that disables the `check_nonnegative_integer()`. I think I would call it `enforce_counts_seurat_v3` though. You might also consider bypassing the check if the flag is set, because it can be slowish for large datasets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:284,usability,User,UserWarning,284,"So, without any evidence, I think it should be fine. The reason I had put an error in the first place is that the typical behavior is to pass log normalized data to this HVG function, and I didn't want people to run this incorrectly. I think another solution would be to just throw a UserWarning, though in a way I like the idea of having an argument that disables the `check_nonnegative_integer()`. I think I would call it `enforce_counts_seurat_v3` though. You might also consider bypassing the check if the flag is set, because it can be slowish for large datasets.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:80,availability,error,error,80,"How about a `check_values` argument which defaults to `True`, and replacing the error with a warning like:. ""Your data doesn't appear to be have integer values, but this method expects raw count data. Proceed with caution."". `check_values` is nice and generic, and could be used in other functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:80,performance,error,error,80,"How about a `check_values` argument which defaults to `True`, and replacing the error with a warning like:. ""Your data doesn't appear to be have integer values, but this method expects raw count data. Proceed with caution."". `check_values` is nice and generic, and could be used in other functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:119,reliability,doe,doesn,119,"How about a `check_values` argument which defaults to `True`, and replacing the error with a warning like:. ""Your data doesn't appear to be have integer values, but this method expects raw count data. Proceed with caution."". `check_values` is nice and generic, and could be used in other functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:80,safety,error,error,80,"How about a `check_values` argument which defaults to `True`, and replacing the error with a warning like:. ""Your data doesn't appear to be have integer values, but this method expects raw count data. Proceed with caution."". `check_values` is nice and generic, and could be used in other functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:80,usability,error,error,80,"How about a `check_values` argument which defaults to `True`, and replacing the error with a warning like:. ""Your data doesn't appear to be have integer values, but this method expects raw count data. Proceed with caution."". `check_values` is nice and generic, and could be used in other functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:372,availability,error,error,372,"> Though if check_values is False, it shouldn't even run the check, correct? Yes. > So the warning would be more like,. >. > ""'seurat_v3' flavor expects raw count data. Proceed with caution."". I was thinking the warning would only occur if `check_values=True` and non integer values were found. ""partial counts"" are common enough I'd agree it's not reasonable for this to error if the results are still meaningful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:372,performance,error,error,372,"> Though if check_values is False, it shouldn't even run the check, correct? Yes. > So the warning would be more like,. >. > ""'seurat_v3' flavor expects raw count data. Proceed with caution."". I was thinking the warning would only occur if `check_values=True` and non integer values were found. ""partial counts"" are common enough I'd agree it's not reasonable for this to error if the results are still meaningful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:372,safety,error,error,372,"> Though if check_values is False, it shouldn't even run the check, correct? Yes. > So the warning would be more like,. >. > ""'seurat_v3' flavor expects raw count data. Proceed with caution."". I was thinking the warning would only occur if `check_values=True` and non integer values were found. ""partial counts"" are common enough I'd agree it's not reasonable for this to error if the results are still meaningful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:372,usability,error,error,372,"> Though if check_values is False, it shouldn't even run the check, correct? Yes. > So the warning would be more like,. >. > ""'seurat_v3' flavor expects raw count data. Proceed with caution."". I was thinking the warning would only occur if `check_values=True` and non integer values were found. ""partial counts"" are common enough I'd agree it's not reasonable for this to error if the results are still meaningful.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:182,deployability,manag,manage,182,"thanks guys, added the `check_values` argument and triggred the warning. I tried to add a test for the warning following: https://docs.pytest.org/en/stable/warnings.html. but didn't manage (couldn't find a way to test for `logging` ). not sure if useful anyway. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:223,deployability,log,logging,223,"thanks guys, added the `check_values` argument and triggred the warning. I tried to add a test for the warning following: https://docs.pytest.org/en/stable/warnings.html. but didn't manage (couldn't find a way to test for `logging` ). not sure if useful anyway. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:182,energy efficiency,manag,manage,182,"thanks guys, added the `check_values` argument and triggred the warning. I tried to add a test for the warning following: https://docs.pytest.org/en/stable/warnings.html. but didn't manage (couldn't find a way to test for `logging` ). not sure if useful anyway. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:90,safety,test,test,90,"thanks guys, added the `check_values` argument and triggred the warning. I tried to add a test for the warning following: https://docs.pytest.org/en/stable/warnings.html. but didn't manage (couldn't find a way to test for `logging` ). not sure if useful anyway. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:182,safety,manag,manage,182,"thanks guys, added the `check_values` argument and triggred the warning. I tried to add a test for the warning following: https://docs.pytest.org/en/stable/warnings.html. but didn't manage (couldn't find a way to test for `logging` ). not sure if useful anyway. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:213,safety,test,test,213,"thanks guys, added the `check_values` argument and triggred the warning. I tried to add a test for the warning following: https://docs.pytest.org/en/stable/warnings.html. but didn't manage (couldn't find a way to test for `logging` ). not sure if useful anyway. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:223,safety,log,logging,223,"thanks guys, added the `check_values` argument and triggred the warning. I tried to add a test for the warning following: https://docs.pytest.org/en/stable/warnings.html. but didn't manage (couldn't find a way to test for `logging` ). not sure if useful anyway. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:223,security,log,logging,223,"thanks guys, added the `check_values` argument and triggred the warning. I tried to add a test for the warning following: https://docs.pytest.org/en/stable/warnings.html. but didn't manage (couldn't find a way to test for `logging` ). not sure if useful anyway. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:90,testability,test,test,90,"thanks guys, added the `check_values` argument and triggred the warning. I tried to add a test for the warning following: https://docs.pytest.org/en/stable/warnings.html. but didn't manage (couldn't find a way to test for `logging` ). not sure if useful anyway. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:213,testability,test,test,213,"thanks guys, added the `check_values` argument and triggred the warning. I tried to add a test for the warning following: https://docs.pytest.org/en/stable/warnings.html. but didn't manage (couldn't find a way to test for `logging` ). not sure if useful anyway. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/pull/1642:223,testability,log,logging,223,"thanks guys, added the `check_values` argument and triggred the warning. I tried to add a test for the warning following: https://docs.pytest.org/en/stable/warnings.html. but didn't manage (couldn't find a way to test for `logging` ). not sure if useful anyway. Thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1642
https://github.com/scverse/scanpy/issues/1643:343,deployability,resourc,resources,343,"Definitely love to have more contributions! I believe @LuckyMD and @giovp are quite keen on having this in the library. Excited to see your benchmarks! Side note: I'd definitely recommend looking into using `joblib` instead of `multiprocessing` for parallelization. It's a bit more simple to use, is much better about not oversubscribing your resources, and copying less data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:343,energy efficiency,resourc,resources,343,"Definitely love to have more contributions! I believe @LuckyMD and @giovp are quite keen on having this in the library. Excited to see your benchmarks! Side note: I'd definitely recommend looking into using `joblib` instead of `multiprocessing` for parallelization. It's a bit more simple to use, is much better about not oversubscribing your resources, and copying less data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:249,performance,parallel,parallelization,249,"Definitely love to have more contributions! I believe @LuckyMD and @giovp are quite keen on having this in the library. Excited to see your benchmarks! Side note: I'd definitely recommend looking into using `joblib` instead of `multiprocessing` for parallelization. It's a bit more simple to use, is much better about not oversubscribing your resources, and copying less data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:343,performance,resourc,resources,343,"Definitely love to have more contributions! I believe @LuckyMD and @giovp are quite keen on having this in the library. Excited to see your benchmarks! Side note: I'd definitely recommend looking into using `joblib` instead of `multiprocessing` for parallelization. It's a bit more simple to use, is much better about not oversubscribing your resources, and copying less data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:343,safety,resourc,resources,343,"Definitely love to have more contributions! I believe @LuckyMD and @giovp are quite keen on having this in the library. Excited to see your benchmarks! Side note: I'd definitely recommend looking into using `joblib` instead of `multiprocessing` for parallelization. It's a bit more simple to use, is much better about not oversubscribing your resources, and copying less data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:282,testability,simpl,simple,282,"Definitely love to have more contributions! I believe @LuckyMD and @giovp are quite keen on having this in the library. Excited to see your benchmarks! Side note: I'd definitely recommend looking into using `joblib` instead of `multiprocessing` for parallelization. It's a bit more simple to use, is much better about not oversubscribing your resources, and copying less data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:343,testability,resourc,resources,343,"Definitely love to have more contributions! I believe @LuckyMD and @giovp are quite keen on having this in the library. Excited to see your benchmarks! Side note: I'd definitely recommend looking into using `joblib` instead of `multiprocessing` for parallelization. It's a bit more simple to use, is much better about not oversubscribing your resources, and copying less data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:282,usability,simpl,simple,282,"Definitely love to have more contributions! I believe @LuckyMD and @giovp are quite keen on having this in the library. Excited to see your benchmarks! Side note: I'd definitely recommend looking into using `joblib` instead of `multiprocessing` for parallelization. It's a bit more simple to use, is much better about not oversubscribing your resources, and copying less data.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:90,deployability,API,API,90,"I think there's a pretty strong desire on the team to see something like this in the main API, if you'd like to contribute this there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:90,integrability,API,API,90,"I think there's a pretty strong desire on the team to see something like this in the main API, if you'd like to contribute this there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:90,interoperability,API,API,90,"I think there's a pretty strong desire on the team to see something like this in the main API, if you'd like to contribute this there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:46,security,team,team,46,"I think there's a pretty strong desire on the team to see something like this in the main API, if you'd like to contribute this there.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:74,usability,guidanc,guidance,74,Gotcha! I'll prioritize getting the benchmarks up and then I'll need some guidance on how to organize it to fit in scanpy's codebase. Thanks!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:28,energy efficiency,cool,cool,28,"agreed, this would be super cool!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:31,energy efficiency,cool,cool,31,totally agree! would be really cool to have!,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:66,deployability,API,API,66,"I agree that this should not belong to 'external' but to the main API. . Also, I would not be initially concerned about performance. Having the tool first is more important at the moment. We can later see how can be optimized.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:216,energy efficiency,optim,optimized,216,"I agree that this should not belong to 'external' but to the main API. . Also, I would not be initially concerned about performance. Having the tool first is more important at the moment. We can later see how can be optimized.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:66,integrability,API,API,66,"I agree that this should not belong to 'external' but to the main API. . Also, I would not be initially concerned about performance. Having the tool first is more important at the moment. We can later see how can be optimized.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:66,interoperability,API,API,66,"I agree that this should not belong to 'external' but to the main API. . Also, I would not be initially concerned about performance. Having the tool first is more important at the moment. We can later see how can be optimized.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:104,modifiability,concern,concerned,104,"I agree that this should not belong to 'external' but to the main API. . Also, I would not be initially concerned about performance. Having the tool first is more important at the moment. We can later see how can be optimized.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:120,performance,perform,performance,120,"I agree that this should not belong to 'external' but to the main API. . Also, I would not be initially concerned about performance. Having the tool first is more important at the moment. We can later see how can be optimized.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:216,performance,optimiz,optimized,216,"I agree that this should not belong to 'external' but to the main API. . Also, I would not be initially concerned about performance. Having the tool first is more important at the moment. We can later see how can be optimized.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:104,testability,concern,concerned,104,"I agree that this should not belong to 'external' but to the main API. . Also, I would not be initially concerned about performance. Having the tool first is more important at the moment. We can later see how can be optimized.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:120,usability,perform,performance,120,"I agree that this should not belong to 'external' but to the main API. . Also, I would not be initially concerned about performance. Having the tool first is more important at the moment. We can later see how can be optimized.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:144,usability,tool,tool,144,"I agree that this should not belong to 'external' but to the main API. . Also, I would not be initially concerned about performance. Having the tool first is more important at the moment. We can later see how can be optimized.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:189,energy efficiency,current,currently,189,I looked at a few genes and it looks like I'm getting pretty similar residuals compared to the R implementation. Something weird is going on with the genes in the last couple images so I'm currently trying to figure that before generating more thorough benchmarks. (I clip negative values to zero to preserve sparsity structure). ![image](https://user-images.githubusercontent.com/16548075/107794002-c4dfc800-6d0b-11eb-8c69-eca5963a2cc4.png). ![image](https://user-images.githubusercontent.com/16548075/107794217-02445580-6d0c-11eb-9c12-a1fce473a69a.png). ![image](https://user-images.githubusercontent.com/16548075/107794044-d1642080-6d0b-11eb-9659-bd72e7a9aa99.png). ![image](https://user-images.githubusercontent.com/16548075/107794308-23a54180-6d0c-11eb-8b2e-59bb479b09af.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:168,integrability,coupl,couple,168,I looked at a few genes and it looks like I'm getting pretty similar residuals compared to the R implementation. Something weird is going on with the genes in the last couple images so I'm currently trying to figure that before generating more thorough benchmarks. (I clip negative values to zero to preserve sparsity structure). ![image](https://user-images.githubusercontent.com/16548075/107794002-c4dfc800-6d0b-11eb-8c69-eca5963a2cc4.png). ![image](https://user-images.githubusercontent.com/16548075/107794217-02445580-6d0c-11eb-9c12-a1fce473a69a.png). ![image](https://user-images.githubusercontent.com/16548075/107794044-d1642080-6d0b-11eb-9659-bd72e7a9aa99.png). ![image](https://user-images.githubusercontent.com/16548075/107794308-23a54180-6d0c-11eb-8b2e-59bb479b09af.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:168,modifiability,coupl,couple,168,I looked at a few genes and it looks like I'm getting pretty similar residuals compared to the R implementation. Something weird is going on with the genes in the last couple images so I'm currently trying to figure that before generating more thorough benchmarks. (I clip negative values to zero to preserve sparsity structure). ![image](https://user-images.githubusercontent.com/16548075/107794002-c4dfc800-6d0b-11eb-8c69-eca5963a2cc4.png). ![image](https://user-images.githubusercontent.com/16548075/107794217-02445580-6d0c-11eb-9c12-a1fce473a69a.png). ![image](https://user-images.githubusercontent.com/16548075/107794044-d1642080-6d0b-11eb-9659-bd72e7a9aa99.png). ![image](https://user-images.githubusercontent.com/16548075/107794308-23a54180-6d0c-11eb-8b2e-59bb479b09af.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:168,testability,coupl,couple,168,I looked at a few genes and it looks like I'm getting pretty similar residuals compared to the R implementation. Something weird is going on with the genes in the last couple images so I'm currently trying to figure that before generating more thorough benchmarks. (I clip negative values to zero to preserve sparsity structure). ![image](https://user-images.githubusercontent.com/16548075/107794002-c4dfc800-6d0b-11eb-8c69-eca5963a2cc4.png). ![image](https://user-images.githubusercontent.com/16548075/107794217-02445580-6d0c-11eb-9c12-a1fce473a69a.png). ![image](https://user-images.githubusercontent.com/16548075/107794044-d1642080-6d0b-11eb-9659-bd72e7a9aa99.png). ![image](https://user-images.githubusercontent.com/16548075/107794308-23a54180-6d0c-11eb-8b2e-59bb479b09af.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:347,usability,user,user-images,347,I looked at a few genes and it looks like I'm getting pretty similar residuals compared to the R implementation. Something weird is going on with the genes in the last couple images so I'm currently trying to figure that before generating more thorough benchmarks. (I clip negative values to zero to preserve sparsity structure). ![image](https://user-images.githubusercontent.com/16548075/107794002-c4dfc800-6d0b-11eb-8c69-eca5963a2cc4.png). ![image](https://user-images.githubusercontent.com/16548075/107794217-02445580-6d0c-11eb-9c12-a1fce473a69a.png). ![image](https://user-images.githubusercontent.com/16548075/107794044-d1642080-6d0b-11eb-9659-bd72e7a9aa99.png). ![image](https://user-images.githubusercontent.com/16548075/107794308-23a54180-6d0c-11eb-8b2e-59bb479b09af.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:460,usability,user,user-images,460,I looked at a few genes and it looks like I'm getting pretty similar residuals compared to the R implementation. Something weird is going on with the genes in the last couple images so I'm currently trying to figure that before generating more thorough benchmarks. (I clip negative values to zero to preserve sparsity structure). ![image](https://user-images.githubusercontent.com/16548075/107794002-c4dfc800-6d0b-11eb-8c69-eca5963a2cc4.png). ![image](https://user-images.githubusercontent.com/16548075/107794217-02445580-6d0c-11eb-9c12-a1fce473a69a.png). ![image](https://user-images.githubusercontent.com/16548075/107794044-d1642080-6d0b-11eb-9659-bd72e7a9aa99.png). ![image](https://user-images.githubusercontent.com/16548075/107794308-23a54180-6d0c-11eb-8b2e-59bb479b09af.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:573,usability,user,user-images,573,I looked at a few genes and it looks like I'm getting pretty similar residuals compared to the R implementation. Something weird is going on with the genes in the last couple images so I'm currently trying to figure that before generating more thorough benchmarks. (I clip negative values to zero to preserve sparsity structure). ![image](https://user-images.githubusercontent.com/16548075/107794002-c4dfc800-6d0b-11eb-8c69-eca5963a2cc4.png). ![image](https://user-images.githubusercontent.com/16548075/107794217-02445580-6d0c-11eb-9c12-a1fce473a69a.png). ![image](https://user-images.githubusercontent.com/16548075/107794044-d1642080-6d0b-11eb-9659-bd72e7a9aa99.png). ![image](https://user-images.githubusercontent.com/16548075/107794308-23a54180-6d0c-11eb-8b2e-59bb479b09af.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:686,usability,user,user-images,686,I looked at a few genes and it looks like I'm getting pretty similar residuals compared to the R implementation. Something weird is going on with the genes in the last couple images so I'm currently trying to figure that before generating more thorough benchmarks. (I clip negative values to zero to preserve sparsity structure). ![image](https://user-images.githubusercontent.com/16548075/107794002-c4dfc800-6d0b-11eb-8c69-eca5963a2cc4.png). ![image](https://user-images.githubusercontent.com/16548075/107794217-02445580-6d0c-11eb-9c12-a1fce473a69a.png). ![image](https://user-images.githubusercontent.com/16548075/107794044-d1642080-6d0b-11eb-9659-bd72e7a9aa99.png). ![image](https://user-images.githubusercontent.com/16548075/107794308-23a54180-6d0c-11eb-8b2e-59bb479b09af.png).,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:260,availability,cluster,cluster,260,"Think I fixed it. . ![image](https://user-images.githubusercontent.com/16548075/107798294-c8298280-6d10-11eb-8117-74f61ead56f2.png). ![image](https://user-images.githubusercontent.com/16548075/107798329-ceb7fa00-6d10-11eb-97cd-b990b0e97b3e.png). Similarity of cluster assignments based on the R and python corrected expressions is 0.9 (`sklearn.metrics.v_measure_score`). Correlations between the top corrected gene expressions for the top 3000 varying genes between python and R:. ![image](https://user-images.githubusercontent.com/16548075/107798603-3b32f900-6d11-11eb-9b76-01adacd7937c.png). Although the correlations are all close to 1.0, if you look at the scatter plot of all non-zero gene expressions, there seems to be some systematic difference in the linear shifting/scaling. Is this worth figuring out? ![image](https://user-images.githubusercontent.com/16548075/107799107-dcba4a80-6d11-11eb-9a29-21157f4793bb.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:260,deployability,cluster,cluster,260,"Think I fixed it. . ![image](https://user-images.githubusercontent.com/16548075/107798294-c8298280-6d10-11eb-8117-74f61ead56f2.png). ![image](https://user-images.githubusercontent.com/16548075/107798329-ceb7fa00-6d10-11eb-97cd-b990b0e97b3e.png). Similarity of cluster assignments based on the R and python corrected expressions is 0.9 (`sklearn.metrics.v_measure_score`). Correlations between the top corrected gene expressions for the top 3000 varying genes between python and R:. ![image](https://user-images.githubusercontent.com/16548075/107798603-3b32f900-6d11-11eb-9b76-01adacd7937c.png). Although the correlations are all close to 1.0, if you look at the scatter plot of all non-zero gene expressions, there seems to be some systematic difference in the linear shifting/scaling. Is this worth figuring out? ![image](https://user-images.githubusercontent.com/16548075/107799107-dcba4a80-6d11-11eb-9a29-21157f4793bb.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:777,modifiability,scal,scaling,777,"Think I fixed it. . ![image](https://user-images.githubusercontent.com/16548075/107798294-c8298280-6d10-11eb-8117-74f61ead56f2.png). ![image](https://user-images.githubusercontent.com/16548075/107798329-ceb7fa00-6d10-11eb-97cd-b990b0e97b3e.png). Similarity of cluster assignments based on the R and python corrected expressions is 0.9 (`sklearn.metrics.v_measure_score`). Correlations between the top corrected gene expressions for the top 3000 varying genes between python and R:. ![image](https://user-images.githubusercontent.com/16548075/107798603-3b32f900-6d11-11eb-9b76-01adacd7937c.png). Although the correlations are all close to 1.0, if you look at the scatter plot of all non-zero gene expressions, there seems to be some systematic difference in the linear shifting/scaling. Is this worth figuring out? ![image](https://user-images.githubusercontent.com/16548075/107799107-dcba4a80-6d11-11eb-9a29-21157f4793bb.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:37,usability,user,user-images,37,"Think I fixed it. . ![image](https://user-images.githubusercontent.com/16548075/107798294-c8298280-6d10-11eb-8117-74f61ead56f2.png). ![image](https://user-images.githubusercontent.com/16548075/107798329-ceb7fa00-6d10-11eb-97cd-b990b0e97b3e.png). Similarity of cluster assignments based on the R and python corrected expressions is 0.9 (`sklearn.metrics.v_measure_score`). Correlations between the top corrected gene expressions for the top 3000 varying genes between python and R:. ![image](https://user-images.githubusercontent.com/16548075/107798603-3b32f900-6d11-11eb-9b76-01adacd7937c.png). Although the correlations are all close to 1.0, if you look at the scatter plot of all non-zero gene expressions, there seems to be some systematic difference in the linear shifting/scaling. Is this worth figuring out? ![image](https://user-images.githubusercontent.com/16548075/107799107-dcba4a80-6d11-11eb-9a29-21157f4793bb.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:150,usability,user,user-images,150,"Think I fixed it. . ![image](https://user-images.githubusercontent.com/16548075/107798294-c8298280-6d10-11eb-8117-74f61ead56f2.png). ![image](https://user-images.githubusercontent.com/16548075/107798329-ceb7fa00-6d10-11eb-97cd-b990b0e97b3e.png). Similarity of cluster assignments based on the R and python corrected expressions is 0.9 (`sklearn.metrics.v_measure_score`). Correlations between the top corrected gene expressions for the top 3000 varying genes between python and R:. ![image](https://user-images.githubusercontent.com/16548075/107798603-3b32f900-6d11-11eb-9b76-01adacd7937c.png). Although the correlations are all close to 1.0, if you look at the scatter plot of all non-zero gene expressions, there seems to be some systematic difference in the linear shifting/scaling. Is this worth figuring out? ![image](https://user-images.githubusercontent.com/16548075/107799107-dcba4a80-6d11-11eb-9a29-21157f4793bb.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:499,usability,user,user-images,499,"Think I fixed it. . ![image](https://user-images.githubusercontent.com/16548075/107798294-c8298280-6d10-11eb-8117-74f61ead56f2.png). ![image](https://user-images.githubusercontent.com/16548075/107798329-ceb7fa00-6d10-11eb-97cd-b990b0e97b3e.png). Similarity of cluster assignments based on the R and python corrected expressions is 0.9 (`sklearn.metrics.v_measure_score`). Correlations between the top corrected gene expressions for the top 3000 varying genes between python and R:. ![image](https://user-images.githubusercontent.com/16548075/107798603-3b32f900-6d11-11eb-9b76-01adacd7937c.png). Although the correlations are all close to 1.0, if you look at the scatter plot of all non-zero gene expressions, there seems to be some systematic difference in the linear shifting/scaling. Is this worth figuring out? ![image](https://user-images.githubusercontent.com/16548075/107799107-dcba4a80-6d11-11eb-9a29-21157f4793bb.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:629,usability,close,close,629,"Think I fixed it. . ![image](https://user-images.githubusercontent.com/16548075/107798294-c8298280-6d10-11eb-8117-74f61ead56f2.png). ![image](https://user-images.githubusercontent.com/16548075/107798329-ceb7fa00-6d10-11eb-97cd-b990b0e97b3e.png). Similarity of cluster assignments based on the R and python corrected expressions is 0.9 (`sklearn.metrics.v_measure_score`). Correlations between the top corrected gene expressions for the top 3000 varying genes between python and R:. ![image](https://user-images.githubusercontent.com/16548075/107798603-3b32f900-6d11-11eb-9b76-01adacd7937c.png). Although the correlations are all close to 1.0, if you look at the scatter plot of all non-zero gene expressions, there seems to be some systematic difference in the linear shifting/scaling. Is this worth figuring out? ![image](https://user-images.githubusercontent.com/16548075/107799107-dcba4a80-6d11-11eb-9a29-21157f4793bb.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:831,usability,user,user-images,831,"Think I fixed it. . ![image](https://user-images.githubusercontent.com/16548075/107798294-c8298280-6d10-11eb-8117-74f61ead56f2.png). ![image](https://user-images.githubusercontent.com/16548075/107798329-ceb7fa00-6d10-11eb-97cd-b990b0e97b3e.png). Similarity of cluster assignments based on the R and python corrected expressions is 0.9 (`sklearn.metrics.v_measure_score`). Correlations between the top corrected gene expressions for the top 3000 varying genes between python and R:. ![image](https://user-images.githubusercontent.com/16548075/107798603-3b32f900-6d11-11eb-9b76-01adacd7937c.png). Although the correlations are all close to 1.0, if you look at the scatter plot of all non-zero gene expressions, there seems to be some systematic difference in the linear shifting/scaling. Is this worth figuring out? ![image](https://user-images.githubusercontent.com/16548075/107799107-dcba4a80-6d11-11eb-9a29-21157f4793bb.png).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:300,availability,slo,slope,300,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:320,energy efficiency,estimat,estimate,320,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:43,integrability,coupl,couple,43,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:114,integrability,pub,publication,114,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:149,integrability,coupl,couple,149,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:362,integrability,pub,pubblication,362,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:459,interoperability,share,share,459,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:43,modifiability,coupl,couple,43,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:149,modifiability,coupl,couple,149,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:621,modifiability,pac,packages,621,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:381,performance,parallel,parallelization,381,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:526,performance,parallel,parallelizing,526,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:300,reliability,slo,slope,300,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:310,security,intercept,intercept,310,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:43,testability,coupl,couple,43,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:149,testability,coupl,couple,149,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:260,usability,help,help,260,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:885,usability,tool,tools,885,"Hi @atarashansky , looks really good! Have couple of questions and comments:. - Is it the dataset in the original publication? Could you try it in a couple of other datasets (like the 3k ?). If you can make a notebook on how to run your implementation I could help with that. - I would also plot the slope and intercept estimate against mean exp, as in original pubblication. . Re parallelization, @ivirshup already mentioned joblib. For reference, I want to share here a framework developed by @michalk8 that uses joblib for parallelizing a function over a collection of elements. It is already used in 2 scanpy sisters packages: https://github.com/theislab/cellrank and https://github.com/theislab/scvelo , as well as a third one that is coming out next week. Implementation in theislab/scvelo. https://github.com/theislab/scvelo/blob/1659cc8e00a45fcf87cd80a7013aae5531744613/scvelo/tools/dynamical_model.py#L1172. Implementation in theislab/cellrank. https://github.com/theislab/cellrank/blob/master/cellrank/ul/_parallelize.py. I feel this settings is very similar to the one above, so maybe useful to check it out. Let me know what you think !",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:19,modifiability,paramet,parameters,19,- Here are the fit parameters vs the geometric mean expression applied to the 33k pbmc dataset. Looks pretty similar! ![image](https://user-images.githubusercontent.com/16548075/107860398-faf87700-6df3-11eb-8c55-5befa70b350a.png). - Thanks for pointing me towards the joblib parallelization. I’ll work on applying it here. - Notebook incoming! Stay tuned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:275,performance,parallel,parallelization,275,- Here are the fit parameters vs the geometric mean expression applied to the 33k pbmc dataset. Looks pretty similar! ![image](https://user-images.githubusercontent.com/16548075/107860398-faf87700-6df3-11eb-8c55-5befa70b350a.png). - Thanks for pointing me towards the joblib parallelization. I’ll work on applying it here. - Notebook incoming! Stay tuned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:349,performance,tune,tuned,349,- Here are the fit parameters vs the geometric mean expression applied to the 33k pbmc dataset. Looks pretty similar! ![image](https://user-images.githubusercontent.com/16548075/107860398-faf87700-6df3-11eb-8c55-5befa70b350a.png). - Thanks for pointing me towards the joblib parallelization. I’ll work on applying it here. - Notebook incoming! Stay tuned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:135,usability,user,user-images,135,- Here are the fit parameters vs the geometric mean expression applied to the 33k pbmc dataset. Looks pretty similar! ![image](https://user-images.githubusercontent.com/16548075/107860398-faf87700-6df3-11eb-8c55-5befa70b350a.png). - Thanks for pointing me towards the joblib parallelization. I’ll work on applying it here. - Notebook incoming! Stay tuned.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:64,performance,time,time,64,"@atarashansky thanks a lot, looks really good indeed! I'll have time to have a look on Friday and will get back to you then! and thanks for the notebooks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:294,energy efficiency,model,models,294,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks! few questions before opening the PR:. - do you think it's worth it to allow users to add other variables (beside log_umi) ? - do you think it would be useful to add other models other than poisson? - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them? - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:218,modifiability,variab,variables,218,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks! few questions before opening the PR:. - do you think it's worth it to allow users to add other variables (beside log_umi) ? - do you think it would be useful to add other models other than poisson? - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them? - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:652,modifiability,variab,variable,652,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks! few questions before opening the PR:. - do you think it's worth it to allow users to add other variables (beside log_umi) ? - do you think it would be useful to add other models other than poisson? - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them? - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:312,reliability,poisson,poisson,312,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks! few questions before opening the PR:. - do you think it's worth it to allow users to add other variables (beside log_umi) ? - do you think it would be useful to add other models other than poisson? - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them? - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:449,safety,test,testing,449,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks! few questions before opening the PR:. - do you think it's worth it to allow users to add other variables (beside log_umi) ? - do you think it would be useful to add other models other than poisson? - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them? - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:493,safety,test,tested,493,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks! few questions before opening the PR:. - do you think it's worth it to allow users to add other variables (beside log_umi) ? - do you think it would be useful to add other models other than poisson? - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them? - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:571,safety,test,test,571,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks! few questions before opening the PR:. - do you think it's worth it to allow users to add other variables (beside log_umi) ? - do you think it would be useful to add other models other than poisson? - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them? - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:294,security,model,models,294,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks! few questions before opening the PR:. - do you think it's worth it to allow users to add other variables (beside log_umi) ? - do you think it would be useful to add other models other than poisson? - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them? - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:449,testability,test,testing,449,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks! few questions before opening the PR:. - do you think it's worth it to allow users to add other variables (beside log_umi) ? - do you think it would be useful to add other models other than poisson? - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them? - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:493,testability,test,tested,493,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks! few questions before opening the PR:. - do you think it's worth it to allow users to add other variables (beside log_umi) ? - do you think it would be useful to add other models other than poisson? - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them? - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:571,testability,test,test,571,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks! few questions before opening the PR:. - do you think it's worth it to allow users to add other variables (beside log_umi) ? - do you think it would be useful to add other models other than poisson? - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them? - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:199,usability,user,users,199,"@atarashansky sorry for getting back to you late! I played around with the implementation, thanks a lot for the notebooks! few questions before opening the PR:. - do you think it's worth it to allow users to add other variables (beside log_umi) ? - do you think it would be useful to add other models other than poisson? - there are other outputs provided by R implementation other than pearson residuals. Do you think it's worth to include them? - testing: how do you think it should be best tested? we thought about saving results from original implementation in R and test against those (as it's done for others seurat re-implementation like highly variable genes). looking forward to hear what you think! thank you!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:17,energy efficiency,cool,cool,17,"This looks super cool! One minor thing is the bandwidth adjustment. Ideally this shouldn't require manual tweaking. Is this the case? . When I was playing around with Nebulosa (https://github.com/powellgenomicslab/Nebulosa) to reimplement it in python, I noticed that both bandwidth algorithms (`scott` and `silverman`) in scipy's gaussian_kde are really horrible. Bandwidths estimated by the `ks` package in R (https://cran.r-project.org/web/packages/ks/ks.pdf, which is what Nebulosa uses) were always superior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:376,energy efficiency,estimat,estimated,376,"This looks super cool! One minor thing is the bandwidth adjustment. Ideally this shouldn't require manual tweaking. Is this the case? . When I was playing around with Nebulosa (https://github.com/powellgenomicslab/Nebulosa) to reimplement it in python, I noticed that both bandwidth algorithms (`scott` and `silverman`) in scipy's gaussian_kde are really horrible. Bandwidths estimated by the `ks` package in R (https://cran.r-project.org/web/packages/ks/ks.pdf, which is what Nebulosa uses) were always superior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:398,modifiability,pac,package,398,"This looks super cool! One minor thing is the bandwidth adjustment. Ideally this shouldn't require manual tweaking. Is this the case? . When I was playing around with Nebulosa (https://github.com/powellgenomicslab/Nebulosa) to reimplement it in python, I noticed that both bandwidth algorithms (`scott` and `silverman`) in scipy's gaussian_kde are really horrible. Bandwidths estimated by the `ks` package in R (https://cran.r-project.org/web/packages/ks/ks.pdf, which is what Nebulosa uses) were always superior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:443,modifiability,pac,packages,443,"This looks super cool! One minor thing is the bandwidth adjustment. Ideally this shouldn't require manual tweaking. Is this the case? . When I was playing around with Nebulosa (https://github.com/powellgenomicslab/Nebulosa) to reimplement it in python, I noticed that both bandwidth algorithms (`scott` and `silverman`) in scipy's gaussian_kde are really horrible. Bandwidths estimated by the `ks` package in R (https://cran.r-project.org/web/packages/ks/ks.pdf, which is what Nebulosa uses) were always superior.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:863,availability,down,downstream,863,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:897,availability,cluster,clustering,897,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:897,deployability,cluster,clustering,897,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:306,energy efficiency,model,models,306,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:495,energy efficiency,estimat,estimation,495,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:516,energy efficiency,model,models,516,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:192,interoperability,specif,specify,192,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:636,interoperability,specif,specifically,636,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:133,modifiability,exten,extend,133,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:158,modifiability,variab,variables,158,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:485,modifiability,paramet,parameter,485,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:361,reliability,Poisson,Poisson,361,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:662,safety,test,testing,662,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:306,security,model,models,306,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:516,security,model,models,516,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1004,security,control,control,1004,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:274,testability,regress,regression,274,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:373,testability,simpl,simplest,373,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:465,testability,regress,regression,465,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:662,testability,test,testing,662,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1004,testability,control,control,1004,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:373,usability,simpl,simplest,373,"Sorry for the late reply, the notifications for this thread got sent to my spam folder. @giovp . - I think so! It’s not difficult to extend it to more latent variables. We could allow them to specify any column(s) in the `obs` DataFrame. - Hmm, I think `statsmodels` can do regression on lots of different models, but from the source paper it sounds like using Poisson was simplest/fastest and did not affect the results too much when compared to negative binomial regression. I think parameter estimation for other models might be a bit more involved. - I think that would be pretty straightforward. What outputs are you referring to, specifically? - I’ve been testing by computing correlations between the genes from the python and R implementations. You could also compare rank-ordering of cells by variance. Another approach might be to compare the output of downstream analysis methods (like clustering) to see if the results are similar, and compare to the output of unprocessed data as a negative control.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:34,deployability,automat,automatic,34,"@gokceneraslan . SCTransform does automatic bandwidth selection using the scott algorithm, which I am implementing with `gaussian_kde`. Although, I could change this to use bandwidth selection using `KDEpy`’s implementation, since I use their ISJ bandwidth algorithm for another step (the bandwidth algorithms used at different steps is following the methods write-up in the original paper).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:29,reliability,doe,does,29,"@gokceneraslan . SCTransform does automatic bandwidth selection using the scott algorithm, which I am implementing with `gaussian_kde`. Although, I could change this to use bandwidth selection using `KDEpy`’s implementation, since I use their ISJ bandwidth algorithm for another step (the bandwidth algorithms used at different steps is following the methods write-up in the original paper).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:34,testability,automat,automatic,34,"@gokceneraslan . SCTransform does automatic bandwidth selection using the scott algorithm, which I am implementing with `gaussian_kde`. Although, I could change this to use bandwidth selection using `KDEpy`’s implementation, since I use their ISJ bandwidth algorithm for another step (the bandwidth algorithms used at different steps is following the methods write-up in the original paper).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:36,energy efficiency,estimat,estimation,36,"I don't know how ""easy"" the density estimation problem here is, but I would avoid scott's whenever I can (e.g. https://aakinshin.net/posts/kde-bw/).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:76,safety,avoid,avoid,76,"I don't know how ""easy"" the density estimation problem here is, but I would avoid scott's whenever I can (e.g. https://aakinshin.net/posts/kde-bw/).",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:628,integrability,batch,batch,628,"hi @atarashansky ,. sorryf or late reply again. . I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right? For the rest, I think the two main points that could be addressed before PR are:. - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking. - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:628,performance,batch,batch,628,"hi @atarashansky ,. sorryf or late reply again. . I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right? For the rest, I think the two main points that could be addressed before PR are:. - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking. - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:769,safety,review,review,769,"hi @atarashansky ,. sorryf or late reply again. . I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right? For the rest, I think the two main points that could be addressed before PR are:. - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking. - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:804,safety,test,tests,804,"hi @atarashansky ,. sorryf or late reply again. . I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right? For the rest, I think the two main points that could be addressed before PR are:. - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking. - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:879,safety,test,test,879,"hi @atarashansky ,. sorryf or late reply again. . I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right? For the rest, I think the two main points that could be addressed before PR are:. - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking. - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:769,testability,review,review,769,"hi @atarashansky ,. sorryf or late reply again. . I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right? For the rest, I think the two main points that could be addressed before PR are:. - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking. - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:804,testability,test,tests,804,"hi @atarashansky ,. sorryf or late reply again. . I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right? For the rest, I think the two main points that could be addressed before PR are:. - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking. - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:879,testability,test,test,879,"hi @atarashansky ,. sorryf or late reply again. . I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right? For the rest, I think the two main points that could be addressed before PR are:. - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking. - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:474,usability,user,users,474,"hi @atarashansky ,. sorryf or late reply again. . I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right? For the rest, I think the two main points that could be addressed before PR are:. - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking. - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:588,usability,user,users,588,"hi @atarashansky ,. sorryf or late reply again. . I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right? For the rest, I think the two main points that could be addressed before PR are:. - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking. - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:785,usability,help,help,785,"hi @atarashansky ,. sorryf or late reply again. . I'm not sure about kde issue raised by @gokceneraslan , do you think it's worth to look into it? I think they use the same in original implementation though right? For the rest, I think the two main points that could be addressed before PR are:. - using same nomenclature for inferred params as original implementation (so not like `_1` that is done now) and also returning same set of params. This would make it easier for users who are already familiar, and also for us for benchmarking. - allow the creation of a design matrix so that users could pass any covariate (I'd say batch is the most important and probably used in general). with these two points addressed, I think it's good to start a PR, I'd be happy to review and also help in setting up tests/benchmarks (which like you mentioned, would be probably best to just test against the result of original implementation). Let me know for anything else, exciting for this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:413,deployability,version,version,413,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:374,energy efficiency,model,model,374,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:440,energy efficiency,model,model,440,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:413,integrability,version,version,413,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:413,modifiability,version,version,413,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:635,performance,content,content,635,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:451,reliability,doe,does,451,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:574,reliability,pra,practice,574,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:374,security,model,model,374,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:440,security,model,model,440,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:888,security,control,controls,888,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1049,security,modif,modifications,1049,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:402,testability,simpl,simplified,402,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:888,testability,control,controls,888,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:402,usability,simpl,simplified,402,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1000,usability,learn,learns,1000,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1440,usability,learn,learn,1440,"Hi @atarashansky and everyone following this interesting discussion! I just found this issue after posting quite a related PR yesterday (#1715) that came out of a discussion from the end of last year (berenslab/umi-normalization#1), and wanted to leave a note about that relation here:. In my PR, I implement normalization by analytic Pearson residuals based on a NB offset model, which is an improved/simplified version of the scTransform model that does not need regularization by smoothing anymore... This brings some theoretical advantages and we found it works well in practice (details in this [preprint](https://www.biorxiv.org/content/10.1101/2020.12.01.405886v1) with @dkobak ). One of the differences remaining between the two is how the overdispersion `theta` is treated (scTransform: fitted per gene, analytical residuals: fixed to one `theta` for all genes based on negative controls). I think fixing `theta` like that makes a lot of sense, but also thought about adding a function that learns a global `theta` from the data. With some modifications that could be another fruitful use-case of your `theta.ml` python implementation @atarashansky. Also, I'm curious about the clipping of the Pearson residuals to `[0, sqrt(n/30)]` in your method. We also find that clipping is an important step for obtaining sensible analytical residuals, and I recently though a bit about motivating different cutoffs - so I'd be interested to learn what is behind your choice of `sqrt(n/30)`! Looking forward to you thoughts on this :). Jan.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:402,integrability,wrap,wrapper,402,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector? With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:402,interoperability,wrapper,wrapper,402,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector? With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:368,modifiability,paramet,parameter,368,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector? With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:53,safety,avoid,avoid,53,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector? With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:69,usability,learn,learn,69,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector? With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:130,usability,learn,learning,130,"@jlause Interesting work! It would indeed be nice to avoid having to learn bandwidths altogether. What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector? With regards to the clipping, I turned my brain off and copied the Seurat implementation as much as possible. `sqrt(n/30)` was the default parameter used by the SCTransform wrapper in Seurat. I also removed negative values to preserve sparsity structure of the data. Sorry I couldn't provide any insight about this!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:64,safety,reme,remember,64,"Oh, scTransform uses `sqrt(n/30)` by default? Interesting. If I remember correctly, the paper says they use `sqrt(n)`...",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:102,integrability,wrap,wrapper,102,"If I remember correctly, the SCTransform `vst` method uses `sqrt(n)` by default but the `SCTransform` wrapper in Seurat uses `sqrt(n/30)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:102,interoperability,wrapper,wrapper,102,"If I remember correctly, the SCTransform `vst` method uses `sqrt(n)` by default but the `SCTransform` wrapper in Seurat uses `sqrt(n/30)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:5,safety,reme,remember,5,"If I remember correctly, the SCTransform `vst` method uses `sqrt(n)` by default but the `SCTransform` wrapper in Seurat uses `sqrt(n/30)`.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:389,energy efficiency,estimat,estimating,389,"> What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector? Regarding this -- yes, that's what we thought to do. Do you think it makes sense? However, for computational efficiency, I would threshold the genes by expression and take e.g. 1000 or even 100 genes with the highest average expression (for the purpose of estimating theta). Lowly-expressed genes don't really constrain theta much, it's highly-expressed ones that do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:34,usability,learn,learning,34,"> What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector? Regarding this -- yes, that's what we thought to do. Do you think it makes sense? However, for computational efficiency, I would threshold the genes by expression and take e.g. 1000 or even 100 genes with the highest average expression (for the purpose of estimating theta). Lowly-expressed genes don't really constrain theta much, it's highly-expressed ones that do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:242,usability,efficien,efficiency,242,"> What would be the procedure for learning global theta from the data? Would you just flatten the expression matrix into one vector? Regarding this -- yes, that's what we thought to do. Do you think it makes sense? However, for computational efficiency, I would threshold the genes by expression and take e.g. 1000 or even 100 genes with the highest average expression (for the purpose of estimating theta). Lowly-expressed genes don't really constrain theta much, it's highly-expressed ones that do.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:98,deployability,updat,updates,98,"Hi guys, I've been following this thread and it's been quiet recently :) wondering if there's any updates on incorporating ScTransform on Scanpy. Thanks!! 🙏🏼",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:98,safety,updat,updates,98,"Hi guys, I've been following this thread and it's been quiet recently :) wondering if there's any updates on incorporating ScTransform on Scanpy. Thanks!! 🙏🏼",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:98,security,updat,updates,98,"Hi guys, I've been following this thread and it's been quiet recently :) wondering if there's any updates on incorporating ScTransform on Scanpy. Thanks!! 🙏🏼",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1427,availability,slo,slot,1427,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:128,modifiability,pac,packages,128,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:285,modifiability,layer,layer,285,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:312,modifiability,layer,layer,312,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:332,modifiability,layer,layers,332,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:339,modifiability,layer,layer,339,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:930,modifiability,layer,layer,930,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1046,modifiability,layer,layers,1046,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1159,modifiability,layer,layers,1159,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1230,modifiability,layer,layers,1230,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1294,modifiability,layer,layer,1294,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1487,modifiability,layer,layers,1487,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1427,reliability,slo,slot,1427,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:389,safety,input,input,389,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:389,usability,input,input,389,"Found it after I wrote it.😄 Here is my code for those who want to try 'R' SCT. ```python. import anndata2ri. from rpy2.robjects.packages import importr. from rpy2.robjects import r, pandas2ri. import numpy as np. anndata2ri.activate(). pandas2ri.activate(). def run_sctransform(adata, layer=None, **kwargs):. if layer:. mat = adata.layers[layer]. else:. mat = adata.X. # Set names for the input matrix. cell_names = adata.obs_names. gene_names = adata.var_names. r.assign('mat', mat.T). r.assign('cell_names', cell_names). r.assign('gene_names', gene_names). r('colnames(mat) <- cell_names'). r('rownames(mat) <- gene_names'). seurat = importr('Seurat'). r('seurat_obj <- CreateSeuratObject(mat)'). # Run. for k, v in kwargs.items():. r.assign(k, v). kwargs_str = ', '.join([f'{k}={k}' for k in kwargs.keys()]). r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Extract the SCT data and add it as a new layer in the original anndata object. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@data'))). adata.layers['SCT_data'] = sct_data.T. sct_data = np.asarray(r['as.matrix'](r('seurat_obj@assays$SCT@counts'))). adata.layers['SCT_counts'] = sct_data.T. return adata. ```. ```python. adata.layers[""data""] = adata.X.copy(). adata = run_sctransform(adata, layer=""counts""). R[write to console]: Running SCTransform on assay: RNA. R[write to console]: Place corrected count matrix in counts slot. R[write to console]: Set default assay to SCT. adata. layers: 'counts', 'data', 'SCT_data', 'SCT_counts'. ```. Please use this code and your data with caution.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:244,deployability,contain,contained,244,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:323,integrability,filter,filtered,323,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:487,integrability,sub,subsetting,487,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:443,interoperability,mismatch,mismatch,443,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:275,modifiability,variab,variables,275,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:393,modifiability,layer,layers,393,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:547,modifiability,layer,layer,547,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:739,modifiability,layer,layer,739,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:1129,modifiability,layer,layer,1129,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:375,safety,prevent,prevented,375,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:672,safety,Prevent,Prevent,672,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:375,security,preven,prevented,375,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:672,security,Preven,Prevent,672,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:480,testability,simpl,simple,480,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1643:480,usability,simpl,simple,480,"Thank you everybody, and particularly @Moloch0, for contributing to this discussion. Your `run_sctransform` function is exactly what I needed for my analyses. A little note in case anyone else might run into the same issue: my original AnnData contained a small set of genes/variables present in very few cells, which were filtered out during SCTransform normalisation. This prevented the SCT layers from being added to adata due to dimension mismatch. To address this, I added a simple subsetting script to the function between normalisation and layer addition, as follows:. ```python. #[...]. r(f'seurat_obj <- SCTransform(seurat_obj,vst.flavor=""v2"", {kwargs_str})'). # Prevent partial SCT output because of default min.genes messing up layer addition. r('diffDash <- setdiff(rownames(seurat_obj), rownames(mat))'). r('diffDash <- gsub(""-"", ""_"", diffDash)'). r('diffScore <- setdiff(rownames(mat), rownames(seurat_obj))'). filtout_genes = svconvert(r('setdiff(diffScore, diffDash)')). filtout_indicator = np.in1d(adata.var_names, filtout_genes). adata = adata[:, ~filtout_indicator]. # Extract the SCT data and add it as a new layer in the original anndata object. #[...]. ``` . Hope that comes in handy for anyone else facing this issue!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1643
https://github.com/scverse/scanpy/issues/1644:143,deployability,log,logic,143,@ivirshup Honestly don't know what I was thinking here- the parameters are clearly not passed through. Perhaps I broke things when rearranging logic in the PR. In in any case I'll submit a fix soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:180,integrability,sub,submit,180,@ivirshup Honestly don't know what I was thinking here- the parameters are clearly not passed through. Perhaps I broke things when rearranging logic in the PR. In in any case I'll submit a fix soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:60,modifiability,paramet,parameters,60,@ivirshup Honestly don't know what I was thinking here- the parameters are clearly not passed through. Perhaps I broke things when rearranging logic in the PR. In in any case I'll submit a fix soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:143,safety,log,logic,143,@ivirshup Honestly don't know what I was thinking here- the parameters are clearly not passed through. Perhaps I broke things when rearranging logic in the PR. In in any case I'll submit a fix soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:143,security,log,logic,143,@ivirshup Honestly don't know what I was thinking here- the parameters are clearly not passed through. Perhaps I broke things when rearranging logic in the PR. In in any case I'll submit a fix soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:143,testability,log,logic,143,@ivirshup Honestly don't know what I was thinking here- the parameters are clearly not passed through. Perhaps I broke things when rearranging logic in the PR. In in any case I'll submit a fix soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:75,usability,clear,clearly,75,@ivirshup Honestly don't know what I was thinking here- the parameters are clearly not passed through. Perhaps I broke things when rearranging logic in the PR. In in any case I'll submit a fix soon.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1644:12,integrability,sub,submitted,12,"PR with fix submitted, thanks @fabianrost84 for the report.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1644
https://github.com/scverse/scanpy/issues/1645:9,availability,error,error,9,"From the error message, you may want to try to convert the dense matrix to sparse matrix format as follows:. ```python. from scipy.sparse impor csr_matrix. adata.X = csr_matrix(adata.X). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:15,integrability,messag,message,15,"From the error message, you may want to try to convert the dense matrix to sparse matrix format as follows:. ```python. from scipy.sparse impor csr_matrix. adata.X = csr_matrix(adata.X). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:15,interoperability,messag,message,15,"From the error message, you may want to try to convert the dense matrix to sparse matrix format as follows:. ```python. from scipy.sparse impor csr_matrix. adata.X = csr_matrix(adata.X). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:89,interoperability,format,format,89,"From the error message, you may want to try to convert the dense matrix to sparse matrix format as follows:. ```python. from scipy.sparse impor csr_matrix. adata.X = csr_matrix(adata.X). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:9,performance,error,error,9,"From the error message, you may want to try to convert the dense matrix to sparse matrix format as follows:. ```python. from scipy.sparse impor csr_matrix. adata.X = csr_matrix(adata.X). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:9,safety,error,error,9,"From the error message, you may want to try to convert the dense matrix to sparse matrix format as follows:. ```python. from scipy.sparse impor csr_matrix. adata.X = csr_matrix(adata.X). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:9,usability,error,error,9,"From the error message, you may want to try to convert the dense matrix to sparse matrix format as follows:. ```python. from scipy.sparse impor csr_matrix. adata.X = csr_matrix(adata.X). ```.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:118,availability,error,error,118,"Sure, this works as a workaround. Thing is, if I call doublets like described in the scrublet README, I do not get an error:. This works:. ```python. import scrublet as scr. import scanpy as sc. adata = sc.datasets.paul15(). scrub = scr.Scrublet(X). doublet_scores, predicted_doublets = scrub.scrub_doublets(). ```. So is this upstream?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:118,performance,error,error,118,"Sure, this works as a workaround. Thing is, if I call doublets like described in the scrublet README, I do not get an error:. This works:. ```python. import scrublet as scr. import scanpy as sc. adata = sc.datasets.paul15(). scrub = scr.Scrublet(X). doublet_scores, predicted_doublets = scrub.scrub_doublets(). ```. So is this upstream?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:118,safety,error,error,118,"Sure, this works as a workaround. Thing is, if I call doublets like described in the scrublet README, I do not get an error:. This works:. ```python. import scrublet as scr. import scanpy as sc. adata = sc.datasets.paul15(). scrub = scr.Scrublet(X). doublet_scores, predicted_doublets = scrub.scrub_doublets(). ```. So is this upstream?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:118,usability,error,error,118,"Sure, this works as a workaround. Thing is, if I call doublets like described in the scrublet README, I do not get an error:. This works:. ```python. import scrublet as scr. import scanpy as sc. adata = sc.datasets.paul15(). scrub = scr.Scrublet(X). doublet_scores, predicted_doublets = scrub.scrub_doublets(). ```. So is this upstream?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:59,integrability,translat,translate,59,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:291,integrability,inject,injecting,291,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:59,interoperability,translat,translate,59,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:508,interoperability,convers,conversion,508,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:551,safety,test,test,551,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:595,safety,valid,valid,595,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:291,security,inject,injecting,291,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:551,testability,test,test,551,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:88,usability,workflow,workflow,88,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:158,usability,workflow,workflow,158,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1645:447,usability,effectiv,effectively,447,"Ahh, I think this is just because of the way I've tried to translate into to the Scanpy workflow. There's a sparsing step [at the start of the basic Scrublet workflow](https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100), but I'm [injecting](https://github.com/theislab/scanpy/blob/76814588696d00183e5f6f02e64f145dbcf944a0/scanpy/external/pp/_scrublet.py#L360) the normalised matrix and effectively skipping that step. I'll PR a sparsing check and conversion (and yes @ivirshup , I'll add a test :-) ), but the workaround is perfectly valid for now. .",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1645
https://github.com/scverse/scanpy/issues/1646:58,availability,error,error,58,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:64,integrability,messag,message,64,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:64,interoperability,messag,message,64,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:58,performance,error,error,58,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:58,safety,error,error,58,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:58,usability,error,error,58,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:140,usability,clear,clear,140,"I think this more of an enhancement than a bug, though an error message saying we don't have a way to color by boolean values would be more clear. What would you expect this to look like? Which styling options apply here?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:45,energy efficiency,predict,predicted,45,"I came across this when I wanted to plot the predicted doublets from scrublet. `predicted_doublet` is stored as boolean. So, I would like to have this plotted like a categorical. I realized that plotting actually works when using `pl.scatter`:. ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.scatter(adata, color='boolean', basis='pca'). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:45,safety,predict,predicted,45,"I came across this when I wanted to plot the predicted doublets from scrublet. `predicted_doublet` is stored as boolean. So, I would like to have this plotted like a categorical. I realized that plotting actually works when using `pl.scatter`:. ```python. import scanpy as sc. adata = sc.datasets.blobs(). sc.pp.pca(adata). adata.obs['boolean'] = True. sc.pl.scatter(adata, color='boolean', basis='pca'). ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:268,availability,error,error,268,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:668,availability,sli,sliced,668,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:995,availability,sli,slice,995,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1002,availability,sli,slice,1002,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:442,deployability,modul,module,442,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:90,integrability,sub,subsetting,90,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:442,modifiability,modul,module,442,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:531,modifiability,pac,packages,531,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:851,modifiability,pac,packages,851,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1165,modifiability,pac,packages,1165,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1451,modifiability,pac,packages,1451,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:268,performance,error,error,268,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:668,reliability,sli,sliced,668,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:995,reliability,sli,slice,995,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1002,reliability,sli,slice,1002,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:268,safety,error,error,268,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:415,safety,input,input-,415,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:442,safety,modul,module,442,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1123,safety,compl,complete,1123,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1654,safety,valid,valid,1654,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1857,safety,valid,valid,1857,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:1123,security,compl,complete,1123,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:371,testability,Trace,Traceback,371,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:268,usability,error,error,268,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:415,usability,input,input-,415,"Thanks. I tried this as well. The problem that I had was that going back to a Boolean for subsetting was not easy:. ```python. adata.obs['boolean'] = adata.obs['boolean'].astype(str).astype('category'). adata[adata.obs['boolean'].astype(bool)]. ```. This throws a key error:. ```pytb. ---------------------------------------------------------------------------. KeyError Traceback (most recent call last). <ipython-input-26-3fef793fe5bd> in <module>. ----> 1 adata[adata.obs['boolean'].astype(bool)]. /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in __getitem__(self, index). 1085 def __getitem__(self, index: Index) -> ""AnnData"":. 1086 """"""Returns a sliced view of the object."""""". -> 1087 oidx, vidx = self._normalize_indices(index). 1088 return AnnData(self, oidx=oidx, vidx=vidx, asview=True). 1089 . /opt/conda/lib/python3.7/site-packages/anndata/_core/anndata.py in _normalize_indices(self, index). 1066 . 1067 def _normalize_indices(self, index: Optional[Index]) -> Tuple[slice, slice]:. -> 1068 return _normalize_indices(index, self.obs_names, self.var_names). 1069 . 1070 # TODO: this is not quite complete... /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_indices(index, names0, names1). 32 index = index[0].values, index[1]. 33 ax0, ax1 = unpack_index(index). ---> 34 ax0 = _normalize_index(ax0, names0). 35 ax1 = _normalize_index(ax1, names1). 36 return ax0, ax1. /opt/conda/lib/python3.7/site-packages/anndata/_core/index.py in _normalize_index(indexer, index). 99 not_found = indexer[positions < 0]. 100 raise KeyError(. --> 101 f""Values {list(not_found)}, from {list(indexer)}, "". 102 ""are not valid obs/ var names or indices."". 103 ). KeyError: 'Values [True, True, True, (.... I shorten this part....) True, True, True, True, True, True, True, True, True, True, True, True, True, True], are not valid obs/ var names or indices.'. ```. while this works:. ```python. adata[adata.obs['boolean'].astype(bool) == True]. ```",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:29,safety,avoid,avoid,29,you can make a new column to avoid overwriting the boolean. ```python. adata.obs['boolean_cat'] = adata.obs['boolean'].astype(str).astype('category'). ```,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:110,energy efficiency,current,currently,110,"# Possible solution: just treat boolean as categorical. * This would also handle nullable booleans (though we currently cannot write those through `anndata`). * Color palettes would just be `palette={True: ""#xxxxxx"", False: ""#xxxxxx""},`. * Default color palette (current behavior for 2 group categories) are orange for true, blue for false. * We could change this, but I'm not sure what a good default is if `lightgray` is null. * Ordering would be done as categorical, not numeric (would it make more sense for `True` to show up on top of `False`?). * Maybe ordering would be good: https://github.com/theislab/scanpy/issues/490#issuecomment-768282049",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:263,energy efficiency,current,current,263,"# Possible solution: just treat boolean as categorical. * This would also handle nullable booleans (though we currently cannot write those through `anndata`). * Color palettes would just be `palette={True: ""#xxxxxx"", False: ""#xxxxxx""},`. * Default color palette (current behavior for 2 group categories) are orange for true, blue for false. * We could change this, but I'm not sure what a good default is if `lightgray` is null. * Ordering would be done as categorical, not numeric (would it make more sense for `True` to show up on top of `False`?). * Maybe ordering would be good: https://github.com/theislab/scanpy/issues/490#issuecomment-768282049",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:271,usability,behavi,behavior,271,"# Possible solution: just treat boolean as categorical. * This would also handle nullable booleans (though we currently cannot write those through `anndata`). * Color palettes would just be `palette={True: ""#xxxxxx"", False: ""#xxxxxx""},`. * Default color palette (current behavior for 2 group categories) are orange for true, blue for false. * We could change this, but I'm not sure what a good default is if `lightgray` is null. * Ordering would be done as categorical, not numeric (would it make more sense for `True` to show up on top of `False`?). * Maybe ordering would be good: https://github.com/theislab/scanpy/issues/490#issuecomment-768282049",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:49,availability,sli,slides,49,I remember liking the color palette used in some slides from @yugeji. Think it might have been blue/ red?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:49,reliability,sli,slides,49,I remember liking the color palette used in some slides from @yugeji. Think it might have been blue/ red?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:2,safety,reme,remember,2,I remember liking the color palette used in some slides from @yugeji. Think it might have been blue/ red?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:57,modifiability,variab,variables,57,@ivirshup are you looking for default colors for boolean variables?,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:39,interoperability,standard,standard,39,yellow & blue like viridis is the gold standard I think. The default of orange & blue should work as well though.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:76,usability,close,closer,76,"I don't love using viridis for this since:. * One color is going to be much closer to the background. * Libraries are pretty inconsistent about whether purple or yellow is the high value. Right now, I'm leaning blue for False, red for True. It's a common divergent palette and people will be used to this from DE plots.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:84,usability,close,closer,84,"> I don't love using viridis for this since:. > . > * One color is going to be much closer to the background. > . > * Libraries are pretty inconsistent about whether purple or yellow is the high value. > . > . > Right now, I'm leaning blue for False, red for True. It's a common divergent palette and people will be used to this from DE plots. Interesting, thanks. Gets a +1 from me :) We're running into this issue very regularly with ehrapy where such boolean columns are very common.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:179,availability,operat,operation,179,"Having the same issue and appreciate the posted workarounds. I found that if True and False are categorical, it is easy to accidentally treat them as booleans in e.g. a filtering operation. In that case, pandas will silently produce unexpected behavior. Whatever the change, the code should probably account for this . e.g. `adata.obs['predicted_doublets'] == True` would return False for every value since ""True"" != True",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:169,integrability,filter,filtering,169,"Having the same issue and appreciate the posted workarounds. I found that if True and False are categorical, it is easy to accidentally treat them as booleans in e.g. a filtering operation. In that case, pandas will silently produce unexpected behavior. Whatever the change, the code should probably account for this . e.g. `adata.obs['predicted_doublets'] == True` would return False for every value since ""True"" != True",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:123,safety,accid,accidentally,123,"Having the same issue and appreciate the posted workarounds. I found that if True and False are categorical, it is easy to accidentally treat them as booleans in e.g. a filtering operation. In that case, pandas will silently produce unexpected behavior. Whatever the change, the code should probably account for this . e.g. `adata.obs['predicted_doublets'] == True` would return False for every value since ""True"" != True",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1646:244,usability,behavi,behavior,244,"Having the same issue and appreciate the posted workarounds. I found that if True and False are categorical, it is easy to accidentally treat them as booleans in e.g. a filtering operation. In that case, pandas will silently produce unexpected behavior. Whatever the change, the code should probably account for this . e.g. `adata.obs['predicted_doublets'] == True` would return False for every value since ""True"" != True",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1646
https://github.com/scverse/scanpy/issues/1647:35,energy efficiency,heat,heatmap,35,"Hi, this is not supported. Only in heatmap. The reason colors are not supported is that I am partial color blind (as a significant portion of the population is), which make plots requiring matching colors hard to decode. Hence the use of labels instead. If you need the colors for your plots you can try replacing the groupby labels ax by the colors you want but this is not trivial.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1647
https://github.com/scverse/scanpy/issues/1647:213,modifiability,deco,decode,213,"Hi, this is not supported. Only in heatmap. The reason colors are not supported is that I am partial color blind (as a significant portion of the population is), which make plots requiring matching colors hard to decode. Hence the use of labels instead. If you need the colors for your plots you can try replacing the groupby labels ax by the colors you want but this is not trivial.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1647
https://github.com/scverse/scanpy/issues/1647:119,security,sign,significant,119,"Hi, this is not supported. Only in heatmap. The reason colors are not supported is that I am partial color blind (as a significant portion of the population is), which make plots requiring matching colors hard to decode. Hence the use of labels instead. If you need the colors for your plots you can try replacing the groupby labels ax by the colors you want but this is not trivial.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1647
https://github.com/scverse/scanpy/issues/1647:16,usability,support,supported,16,"Hi, this is not supported. Only in heatmap. The reason colors are not supported is that I am partial color blind (as a significant portion of the population is), which make plots requiring matching colors hard to decode. Hence the use of labels instead. If you need the colors for your plots you can try replacing the groupby labels ax by the colors you want but this is not trivial.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1647
https://github.com/scverse/scanpy/issues/1647:70,usability,support,supported,70,"Hi, this is not supported. Only in heatmap. The reason colors are not supported is that I am partial color blind (as a significant portion of the population is), which make plots requiring matching colors hard to decode. Hence the use of labels instead. If you need the colors for your plots you can try replacing the groupby labels ax by the colors you want but this is not trivial.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1647
https://github.com/scverse/scanpy/issues/1648:0,energy efficiency,cool,cool,0,"cool, thanks!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1648
https://github.com/scverse/scanpy/pull/1649:42,modifiability,paramet,parameters,42,I take note that we should allow all this parameters to be passed from the graphical functions to `sc.get.rank_genes_groups_df` in #1529,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1649
https://github.com/scverse/scanpy/pull/1649:136,testability,plan,plan,136,@Koncopd @fidelram I've added a discussion of this function to the agenda for the next call. I think it would be good to figure out our plan before making changes.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1649
https://github.com/scverse/scanpy/pull/1649:32,availability,avail,available,32,I believe @fidelram isn't super available for scanpy stuff until next month. I figured we could talk about this then.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1649
https://github.com/scverse/scanpy/pull/1649:32,reliability,availab,available,32,I believe @fidelram isn't super available for scanpy stuff until next month. I figured we could talk about this then.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1649
https://github.com/scverse/scanpy/pull/1649:32,safety,avail,available,32,I believe @fidelram isn't super available for scanpy stuff until next month. I figured we could talk about this then.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1649
https://github.com/scverse/scanpy/pull/1649:32,security,availab,available,32,I believe @fidelram isn't super available for scanpy stuff until next month. I figured we could talk about this then.,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/pull/1649
https://github.com/scverse/scanpy/issues/1650:36,deployability,depend,depends,36,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:104,deployability,scale,scaled,104,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:151,deployability,scale,scale,151,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:208,deployability,version,version,208,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:104,energy efficiency,scale,scaled,104,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:151,energy efficiency,scale,scale,151,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:36,integrability,depend,depends,36,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:208,integrability,version,version,208,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:36,modifiability,depend,depends,36,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:104,modifiability,scal,scaled,104,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:151,modifiability,scal,scale,151,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:208,modifiability,version,version,208,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:236,modifiability,scal,scaling,236,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:265,modifiability,layer,layer,265,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:292,modifiability,scal,scaling,292,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:364,modifiability,layer,layers,364,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:455,modifiability,layer,layers,455,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:104,performance,scale,scaled,104,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:151,performance,scale,scale,151,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:36,safety,depend,depends,36,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:130,safety,avoid,avoid,130,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:36,testability,depend,depends,36,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:514,usability,help,helps,514,"Hi @honghh2018,. Whatever is output depends on what you store in `adata.X`. If you don't want to output scaled data, then you can avoid calling `sc.pp.scale()` on your data. An alternative would be to save a version of your data before scaling in a different adata layer. For example, before scaling, you can just store a copy of your data by e.g., calling `adata.layers['normalized_unscaled] = adata.X`. You can export this data matrix by calling `adata.layers['normalized_unscaled'].to_csv(FILENAME)`. Hope this helps!",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:36,usability,help,helpful,36,"Thanks @LuckyMD ,It was tremedously helpful for my case. Thanks again.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:22,modifiability,scal,scaling,22,"> For example, before scaling, you can just store a copy of your data by e.g., calling adata.layers['normalized_unscaled] = adata.X. Why is .copy() not necessary here? ie, adata.layers['normalized_unscaled] = adata.X.copy()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:93,modifiability,layer,layers,93,"> For example, before scaling, you can just store a copy of your data by e.g., calling adata.layers['normalized_unscaled] = adata.X. Why is .copy() not necessary here? ie, adata.layers['normalized_unscaled] = adata.X.copy()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1650:178,modifiability,layer,layers,178,"> For example, before scaling, you can just store a copy of your data by e.g., calling adata.layers['normalized_unscaled] = adata.X. Why is .copy() not necessary here? ie, adata.layers['normalized_unscaled] = adata.X.copy()",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1650
https://github.com/scverse/scanpy/issues/1651:30,availability,error,error,30,"Sort of separate but also, an error writing such data with `adata.write(""results.h5ad"")`. Traceback:. ```. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'uns/rank_genes_groups_filtered/names' of <class 'h5py._hl.files.File'> from /. ```. `del adata.uns[""rank_genes_groups_filtered""]` and the .write() call succeeds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:180,availability,error,error,180,"Sort of separate but also, an error writing such data with `adata.write(""results.h5ad"")`. Traceback:. ```. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'uns/rank_genes_groups_filtered/names' of <class 'h5py._hl.files.File'> from /. ```. `del adata.uns[""rank_genes_groups_filtered""]` and the .write() call succeeds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:30,performance,error,error,30,"Sort of separate but also, an error writing such data with `adata.write(""results.h5ad"")`. Traceback:. ```. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'uns/rank_genes_groups_filtered/names' of <class 'h5py._hl.files.File'> from /. ```. `del adata.uns[""rank_genes_groups_filtered""]` and the .write() call succeeds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:180,performance,error,error,180,"Sort of separate but also, an error writing such data with `adata.write(""results.h5ad"")`. Traceback:. ```. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'uns/rank_genes_groups_filtered/names' of <class 'h5py._hl.files.File'> from /. ```. `del adata.uns[""rank_genes_groups_filtered""]` and the .write() call succeeds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:30,safety,error,error,30,"Sort of separate but also, an error writing such data with `adata.write(""results.h5ad"")`. Traceback:. ```. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'uns/rank_genes_groups_filtered/names' of <class 'h5py._hl.files.File'> from /. ```. `del adata.uns[""rank_genes_groups_filtered""]` and the .write() call succeeds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:180,safety,error,error,180,"Sort of separate but also, an error writing such data with `adata.write(""results.h5ad"")`. Traceback:. ```. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'uns/rank_genes_groups_filtered/names' of <class 'h5py._hl.files.File'> from /. ```. `del adata.uns[""rank_genes_groups_filtered""]` and the .write() call succeeds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:90,testability,Trace,Traceback,90,"Sort of separate but also, an error writing such data with `adata.write(""results.h5ad"")`. Traceback:. ```. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'uns/rank_genes_groups_filtered/names' of <class 'h5py._hl.files.File'> from /. ```. `del adata.uns[""rank_genes_groups_filtered""]` and the .write() call succeeds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:30,usability,error,error,30,"Sort of separate but also, an error writing such data with `adata.write(""results.h5ad"")`. Traceback:. ```. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'uns/rank_genes_groups_filtered/names' of <class 'h5py._hl.files.File'> from /. ```. `del adata.uns[""rank_genes_groups_filtered""]` and the .write() call succeeds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:180,usability,error,error,180,"Sort of separate but also, an error writing such data with `adata.write(""results.h5ad"")`. Traceback:. ```. TypeError: Can't implicitly convert non-string objects to strings. Above error raised while writing key 'uns/rank_genes_groups_filtered/names' of <class 'h5py._hl.files.File'> from /. ```. `del adata.uns[""rank_genes_groups_filtered""]` and the .write() call succeeds",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:85,integrability,filter,filters,85,"This should be solved (albeit a bit differently) by #1529. There, you would pass the filters to the plotting functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:371,usability,tool,tools,371,"Thanks @ivirshup , that makes sense I think. For the 2nd issue (unable to write to h5ad) is actually a more important issue for me at least. I have frequently found that I cannot add dataframes in `adata.uns` and then write to .h5ad file. This looks like a known issue from the code https://github.com/theislab/scanpy/blob/0d25f457e100080e578809d4db625ea147eed121/scanpy/tools/_marker_gene_overlap.py#L160-L163 - is there a github issue/PR that I missed? The only workaround I found is to convert everything within the dataframe to a string, but this is no always desirable",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:80,energy efficiency,current,currently,80,"Yes, that was a known issue when `filter_rank_genes_groups` was implemented. We currently trying to figure out what the appropriate fix is for this. It might be that we remove the `filter_rank_genes_groups` functions and add their functionality to the `rank_genes_groups` plotting functions.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1651:82,energy efficiency,current,currently,82,"> Yes, that was a known issue when `filter_rank_genes_groups` was implemented. We currently trying to figure out what the appropriate fix is for this. It might be that we remove the `filter_rank_genes_groups` functions and add their functionality to the `rank_genes_groups` plotting functions. Hi, @[ivirshup](https://github.com/ivirshup) Have you fixed the problem?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1651
https://github.com/scverse/scanpy/issues/1652:118,availability,avail,available,118,Found the problem. I was running the library using python v3.9 with numba v0.51. Compatibility for python 3.9 is only available in the (currently) latest version of numba v0.53. . This incompatibility generated the crash in the pynndescent library. Why it worked for certain sizes and no other remains a mistery...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:154,deployability,version,version,154,Found the problem. I was running the library using python v3.9 with numba v0.51. Compatibility for python 3.9 is only available in the (currently) latest version of numba v0.53. . This incompatibility generated the crash in the pynndescent library. Why it worked for certain sizes and no other remains a mistery...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:136,energy efficiency,current,currently,136,Found the problem. I was running the library using python v3.9 with numba v0.51. Compatibility for python 3.9 is only available in the (currently) latest version of numba v0.53. . This incompatibility generated the crash in the pynndescent library. Why it worked for certain sizes and no other remains a mistery...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:154,integrability,version,version,154,Found the problem. I was running the library using python v3.9 with numba v0.51. Compatibility for python 3.9 is only available in the (currently) latest version of numba v0.53. . This incompatibility generated the crash in the pynndescent library. Why it worked for certain sizes and no other remains a mistery...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:81,interoperability,Compatib,Compatibility,81,Found the problem. I was running the library using python v3.9 with numba v0.51. Compatibility for python 3.9 is only available in the (currently) latest version of numba v0.53. . This incompatibility generated the crash in the pynndescent library. Why it worked for certain sizes and no other remains a mistery...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:185,interoperability,incompatib,incompatibility,185,Found the problem. I was running the library using python v3.9 with numba v0.51. Compatibility for python 3.9 is only available in the (currently) latest version of numba v0.53. . This incompatibility generated the crash in the pynndescent library. Why it worked for certain sizes and no other remains a mistery...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:154,modifiability,version,version,154,Found the problem. I was running the library using python v3.9 with numba v0.51. Compatibility for python 3.9 is only available in the (currently) latest version of numba v0.53. . This incompatibility generated the crash in the pynndescent library. Why it worked for certain sizes and no other remains a mistery...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:118,reliability,availab,available,118,Found the problem. I was running the library using python v3.9 with numba v0.51. Compatibility for python 3.9 is only available in the (currently) latest version of numba v0.53. . This incompatibility generated the crash in the pynndescent library. Why it worked for certain sizes and no other remains a mistery...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:118,safety,avail,available,118,Found the problem. I was running the library using python v3.9 with numba v0.51. Compatibility for python 3.9 is only available in the (currently) latest version of numba v0.53. . This incompatibility generated the crash in the pynndescent library. Why it worked for certain sizes and no other remains a mistery...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:118,security,availab,available,118,Found the problem. I was running the library using python v3.9 with numba v0.51. Compatibility for python 3.9 is only available in the (currently) latest version of numba v0.53. . This incompatibility generated the crash in the pynndescent library. Why it worked for certain sizes and no other remains a mistery...,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:31,deployability,instal,install,31,"Hi @mr-september , you have to install the prerelease version of Numba. pip install -pre numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:54,deployability,version,version,54,"Hi @mr-september , you have to install the prerelease version of Numba. pip install -pre numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:76,deployability,instal,install,76,"Hi @mr-september , you have to install the prerelease version of Numba. pip install -pre numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:54,integrability,version,version,54,"Hi @mr-september , you have to install the prerelease version of Numba. pip install -pre numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:54,modifiability,version,version,54,"Hi @mr-september , you have to install the prerelease version of Numba. pip install -pre numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:44,deployability,instal,install,44,"@gatocor Thanks, for me it worked with `pip install --pre numba`, . terminal output:. ```. Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2). Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0). Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1). Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0). ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version? Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:645,deployability,version,version,645,"@gatocor Thanks, for me it worked with `pip install --pre numba`, . terminal output:. ```. Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2). Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0). Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1). Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0). ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version? Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:654,deployability,Updat,Update,654,"@gatocor Thanks, for me it worked with `pip install --pre numba`, . terminal output:. ```. Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2). Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0). Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1). Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0). ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version? Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:714,deployability,instal,installed,714,"@gatocor Thanks, for me it worked with `pip install --pre numba`, . terminal output:. ```. Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2). Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0). Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1). Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0). ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version? Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:628,energy efficiency,load,load,628,"@gatocor Thanks, for me it worked with `pip install --pre numba`, . terminal output:. ```. Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2). Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0). Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1). Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0). ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version? Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:645,integrability,version,version,645,"@gatocor Thanks, for me it worked with `pip install --pre numba`, . terminal output:. ```. Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2). Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0). Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1). Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0). ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version? Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:157,modifiability,pac,packages,157,"@gatocor Thanks, for me it worked with `pip install --pre numba`, . terminal output:. ```. Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2). Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0). Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1). Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0). ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version? Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:264,modifiability,pac,packages,264,"@gatocor Thanks, for me it worked with `pip install --pre numba`, . terminal output:. ```. Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2). Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0). Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1). Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0). ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version? Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:368,modifiability,pac,packages,368,"@gatocor Thanks, for me it worked with `pip install --pre numba`, . terminal output:. ```. Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2). Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0). Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1). Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0). ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version? Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:471,modifiability,pac,packages,471,"@gatocor Thanks, for me it worked with `pip install --pre numba`, . terminal output:. ```. Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2). Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0). Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1). Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0). ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version? Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:645,modifiability,version,version,645,"@gatocor Thanks, for me it worked with `pip install --pre numba`, . terminal output:. ```. Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2). Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0). Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1). Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0). ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version? Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:628,performance,load,load,628,"@gatocor Thanks, for me it worked with `pip install --pre numba`, . terminal output:. ```. Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2). Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0). Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1). Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0). ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version? Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:654,safety,Updat,Update,654,"@gatocor Thanks, for me it worked with `pip install --pre numba`, . terminal output:. ```. Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2). Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0). Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1). Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0). ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version? Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:654,security,Updat,Update,654,"@gatocor Thanks, for me it worked with `pip install --pre numba`, . terminal output:. ```. Requirement already satisfied: numba in ./venv/lib/python3.9/site-packages (0.51.2). Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in ./venv/lib/python3.9/site-packages (from numba) (0.34.0). Requirement already satisfied: numpy>=1.15 in ./venv/lib/python3.9/site-packages (from numba) (1.20.1). Requirement already satisfied: setuptools in ./venv/lib/python3.9/site-packages (from numba) (54.0.0). ```. Went back into Python Console, re-do `import numba`, `numba.__version__` still gives `'0.51.2'`. How do you force it to load a different version? Update: Also tried the `force-reinstall` tag: `Successfully installed llvmlite-0.36.0rc2 numba-0.53.0rc2 numpy-1.20.1 setuptools-54.0.0`, but in Python Console it's still stuck at `numba.__version__ '0.51.2'`",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:32,deployability,version,version,32,You have to first uninstall the version you have already installed of numba. pip uninstall numba. pip install --pre numba,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:57,deployability,instal,installed,57,You have to first uninstall the version you have already installed of numba. pip uninstall numba. pip install --pre numba,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:102,deployability,instal,install,102,You have to first uninstall the version you have already installed of numba. pip uninstall numba. pip install --pre numba,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:32,integrability,version,version,32,You have to first uninstall the version you have already installed of numba. pip uninstall numba. pip install --pre numba,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:32,modifiability,version,version,32,You have to first uninstall the version you have already installed of numba. pip uninstall numba. pip install --pre numba,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:99,deployability,instal,installed,99,"@gatocor just tried it, but it says `Successfully uninstalled numba-0.53.0rc2`, then `Successfully installed numba-0.53.0rc2`. So it looks like my system/terminal has the right `numba` version, but I can't get Python Console to also see the new version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:185,deployability,version,version,185,"@gatocor just tried it, but it says `Successfully uninstalled numba-0.53.0rc2`, then `Successfully installed numba-0.53.0rc2`. So it looks like my system/terminal has the right `numba` version, but I can't get Python Console to also see the new version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:245,deployability,version,version,245,"@gatocor just tried it, but it says `Successfully uninstalled numba-0.53.0rc2`, then `Successfully installed numba-0.53.0rc2`. So it looks like my system/terminal has the right `numba` version, but I can't get Python Console to also see the new version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:185,integrability,version,version,185,"@gatocor just tried it, but it says `Successfully uninstalled numba-0.53.0rc2`, then `Successfully installed numba-0.53.0rc2`. So it looks like my system/terminal has the right `numba` version, but I can't get Python Console to also see the new version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:245,integrability,version,version,245,"@gatocor just tried it, but it says `Successfully uninstalled numba-0.53.0rc2`, then `Successfully installed numba-0.53.0rc2`. So it looks like my system/terminal has the right `numba` version, but I can't get Python Console to also see the new version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:185,modifiability,version,version,185,"@gatocor just tried it, but it says `Successfully uninstalled numba-0.53.0rc2`, then `Successfully installed numba-0.53.0rc2`. So it looks like my system/terminal has the right `numba` version, but I can't get Python Console to also see the new version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:245,modifiability,version,version,245,"@gatocor just tried it, but it says `Successfully uninstalled numba-0.53.0rc2`, then `Successfully installed numba-0.53.0rc2`. So it looks like my system/terminal has the right `numba` version, but I can't get Python Console to also see the new version?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:28,deployability,instal,installing,28,"It is possible that you are installing it in a python pathway different than the one you are using to execute your code. I advise you to create a virtual environment (loot it up somewhere), and execute your single cell projects from it to make sure of the packages you have installed in each place.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:274,deployability,instal,installed,274,"It is possible that you are installing it in a python pathway different than the one you are using to execute your code. I advise you to create a virtual environment (loot it up somewhere), and execute your single cell projects from it to make sure of the packages you have installed in each place.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:256,modifiability,pac,packages,256,"It is possible that you are installing it in a python pathway different than the one you are using to execute your code. I advise you to create a virtual environment (loot it up somewhere), and execute your single cell projects from it to make sure of the packages you have installed in each place.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:187,deployability,modul,modules,187,"Yes, I made sure I was in the right venv. I found one workaround - open a new Python Console (and close the old one if you wish), this fresh Console will not have any previously imported modules, but new `import`s will be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:222,deployability,updat,updated,222,"Yes, I made sure I was in the right venv. I found one workaround - open a new Python Console (and close the old one if you wish), this fresh Console will not have any previously imported modules, but new `import`s will be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:187,modifiability,modul,modules,187,"Yes, I made sure I was in the right venv. I found one workaround - open a new Python Console (and close the old one if you wish), this fresh Console will not have any previously imported modules, but new `import`s will be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:187,safety,modul,modules,187,"Yes, I made sure I was in the right venv. I found one workaround - open a new Python Console (and close the old one if you wish), this fresh Console will not have any previously imported modules, but new `import`s will be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:222,safety,updat,updated,222,"Yes, I made sure I was in the right venv. I found one workaround - open a new Python Console (and close the old one if you wish), this fresh Console will not have any previously imported modules, but new `import`s will be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:222,security,updat,updated,222,"Yes, I made sure I was in the right venv. I found one workaround - open a new Python Console (and close the old one if you wish), this fresh Console will not have any previously imported modules, but new `import`s will be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:98,usability,close,close,98,"Yes, I made sure I was in the right venv. I found one workaround - open a new Python Console (and close the old one if you wish), this fresh Console will not have any previously imported modules, but new `import`s will be updated.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:535,availability,error,error,535,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:719,availability,error,errors,719,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2250,availability,state,state,2250,"e ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5641,availability,state,state,5641,"(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). Fil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5966,availability,state,state,5966," 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7537,availability,error,errors,7537,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7625,availability,error,errors,7625,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1750,deployability,build,builder,1750,"context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1933,deployability,build,builder,1933,"RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2964,deployability,modul,module,2964,". File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3688,deployability,modul,module,3688,"tamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppDa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3815,deployability,modul,module,3815,"other exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.comp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3947,deployability,modul,module,3947," line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5000,deployability,pipelin,pipeline,5000,"ocal\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\L",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7647,deployability,Fail,Failed,7647,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7671,deployability,pipelin,pipeline,7671,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:714,energy efficiency,core,core,714,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:854,energy efficiency,core,core,854,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1006,energy efficiency,core,core,1006,"am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Prog",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1169,energy efficiency,core,core,1169," more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1334,energy efficiency,core,core,1334," matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\sit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1499,energy efficiency,core,core,1499,"auss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1681,energy efficiency,core,core,1681,"thon39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1863,energy efficiency,core,core,1863,"ng.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most rece",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2053,energy efficiency,core,core,2053," self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2353,energy efficiency,core,core,2353,"ne 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connect",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2538,energy efficiency,core,core,2538,"l. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivitie",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2684,energy efficiency,core,core,2684,"lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2699,energy efficiency,model,models,2699,"ne 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4058,energy efficiency,core,core,4058,"rams\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\L",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4204,energy efficiency,core,core,4204,"ppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4381,energy efficiency,core,core,4381,"ities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppD",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4566,energy efficiency,core,core,4566,"rt fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4749,energy efficiency,core,core,4749,"Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_mach",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4946,energy efficiency,core,core,4946,"odule>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5114,energy efficiency,core,core,5114,"g). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5278,energy efficiency,core,core,5278,"s, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPas",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5442,energy efficiency,core,core,5442,"f._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5582,energy efficiency,core,core,5582,""", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5733,energy efficiency,core,core,5733,"ackages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lower",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5889,energy efficiency,core,core,5889,"Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\co",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6058,energy efficiency,core,core,6058,"ocal\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6231,energy efficiency,core,core,6231,"rams\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\nu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6415,energy efficiency,core,core,6415,"\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6580,energy efficiency,core,core,6580,"core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6725,energy efficiency,core,core,6725,"numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 t",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6889,energy efficiency,core,core,6889,"re\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\u",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7075,energy efficiency,core,core,7075,"py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_gl",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7237,energy efficiency,core,core,7237,"piler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7532,energy efficiency,core,core,7532,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7620,energy efficiency,core,core,7620,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7743,energy efficiency,core,core,7743,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7758,energy efficiency,model,models,7758,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2082,integrability,wrap,wrapper,2082,"ile ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2250,integrability,state,state,2250,"e ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4092,integrability,wrap,wrapper,4092,"es\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\si",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5000,integrability,pipelin,pipeline,5000,"ocal\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\L",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5641,integrability,state,state,5641,"(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). Fil",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5966,integrability,state,state,5966," 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7671,integrability,pipelin,pipeline,7671,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2082,interoperability,wrapper,wrapper,2082,"ile ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4092,interoperability,wrapper,wrapper,4092,"es\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\si",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:699,modifiability,pac,packages,699,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:839,modifiability,pac,packages,839,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:991,modifiability,pac,packages,991,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1154,modifiability,pac,packages,1154,"oblem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Prog",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1319,modifiability,pac,packages,1319," numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Pyt",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1484,modifiability,pac,packages,1484,"ata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1666,modifiability,pac,packages,1666,"ams\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1848,modifiability,pac,packages,1848,"a\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceba",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2038,modifiability,pac,packages,2038,"r_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2188,modifiability,pac,packages,2188," 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2338,modifiability,pac,packages,2338,"ering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2523,modifiability,pac,packages,2523," in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2964,modifiability,modul,module,2964,". File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3090,modifiability,pac,packages,3090,"urn fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wr",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3253,modifiability,pac,packages,3253,"top = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = sel",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3465,modifiability,pac,packages,3465,"RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, r",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3688,modifiability,modul,module,3688,"tamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppDa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3815,modifiability,modul,module,3815,"other exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.comp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3947,modifiability,modul,module,3947," line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4043,modifiability,pac,packages,4043,"ta\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RU",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4063,modifiability,deco,decorators,4063,"hon\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Pro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4189,modifiability,pac,packages,4189,"Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode().",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4366,modifiability,pac,packages,4366,"ute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4551,modifiability,pac,packages,4551,"ap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4734,modifiability,pac,packages,4734,"UTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4931,modifiability,pac,packages,4931,"ine 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5099,modifiability,pac,packages,5099,"sp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5263,modifiability,pac,packages,5263,"r.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 29",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5427,modifiability,pac,packages,5427," retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_ma",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5567,modifiability,pac,packages,5567,"dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packag",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5718,modifiability,pac,packages,5718,"39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\num",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5874,modifiability,pac,packages,5874,"UTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-pack",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6043,modifiability,pac,packages,6043,"TBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Pytho",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6216,modifiability,pac,packages,6216,"ta\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\sit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6400,modifiability,pac,packages,6400,"thon\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6565,modifiability,pac,packages,6565,"ckages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_err",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6710,modifiability,pac,packages,6710,"ite-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid s",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6874,modifiability,pac,packages,6874,"ages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\P",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7060,modifiability,pac,packages,7060,"ompiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, fu",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7222,modifiability,pac,packages,7222,"umba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7517,modifiability,pac,packages,7517,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:535,performance,error,error,535,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:719,performance,error,errors,719,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7537,performance,error,errors,7537,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7625,performance,error,errors,7625,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7647,reliability,Fail,Failed,7647,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:535,safety,error,error,535,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:719,safety,error,errors,719,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2805,safety,except,exception,2805,"cal\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41,",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2824,safety,except,exception,2824,"\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2964,safety,modul,module,2964,". File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3688,safety,modul,module,3688,"tamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppDa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3815,safety,modul,module,3815,"other exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.comp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3947,safety,modul,module,3947," line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7537,safety,error,errors,7537,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7625,safety,error,errors,7625,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1585,security,sign,signature,1585,"------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise Ty",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2699,security,model,models,2699,"ne 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7758,security,model,models,7758,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2845,testability,Trace,Traceback,2845,"ckages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7369,testability,context,contextlib,7369,"\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7436,testability,trace,traceback,7436,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:215,usability,Minim,Minimal,215,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:535,usability,error,error,535,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:639,usability,User,Users,639,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:719,usability,error,errors,719,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:779,usability,User,Users,779,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:931,usability,User,Users,931,"> Hi, I am working with a big dataset and I run into a problem when computing the neigbours. Find below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Lo",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1094,usability,User,Users,1094," below an small example using a random matrix. . > . > The problem is even more dramatic for method=""umap"". > . > ### Minimal code sample (that we can copy&paste without having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1259,usability,User,Users,1259,"hout having any data). > ```python. > import scanpy. > import numpy. > . > matrix = numpy.random.uniform(size=[10000,1000]). > . > adata = scanpy.AnnData(matrix). > . > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop =",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1424,usability,User,Users,1424,". > scanpy.pp.pca(adata,n_comps=10). > scanpy.pp.neighbors(adata,method=""gauss"",n_pcs=10). > ```. > . > The error I get:. > ---------------------------------------------------------------------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1606,usability,User,Users,1606,"-----------------. > File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 823, in new_error_context. yield. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid st",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1788,usability,User,Users,1788,"AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:1978,usability,User,Users,1978,"\lib\site-packages\numba\core\lowering.py"", line 438, in lower_inst. val = self.lower_assign(ty, inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.ne",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2128,usability,User,Users,2128,"thon\Python39\lib\site-packages\numba\core\lowering.py"", line 624, in lower_assign. return self.lower_expr(ty, value). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", l",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2256,usability,stop,stop,2256,"\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packa",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2263,usability,stop,stop,2263,"RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\sca",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2278,usability,User,Users,2278,"cal\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1159, in lower_expr. res = self.lower_call(resty, expr). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2463,usability,User,Users,2463,"Python39\lib\site-packages\numba\core\lowering.py"", line 889, in lower_call. res = self._lower_call_normal(fnty, expr, signature). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 1130, in _lower_call_normal. res = impl(self.builder, argvals, self.loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1201, in __call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-p",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:2890,usability,User,Users,2890,"_call__. res = self._imp(self._context, builder, self._sig, args, loc=loc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Progra",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3030,usability,User,Users,3030,"site-packages\numba\core\base.py"", line 1231, in wrapper. return fn(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3193,usability,User,Users,3193,"\numba\cpython\rangeobj.py"", line 40, in range1_impl. state.stop = stop. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 164, in __setattr__. self[self._datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-package",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3405,usability,User,Users,3405,"datamodel.get_field_position(field)] = value. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\cgutils.py"", line 188, in __setitem__. raise TypeError(""Invalid store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 1",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3602,usability,User,Users,3602,"d store of {value.type} to "". TypeError: Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _comp",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3731,usability,User,Users,3731,"15592499520> (trying to write member #1). During handling of the above exception, another exception occurred:. Traceback (most recent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3861,usability,User,Users,3861,"ent call last):. File ""C:\Users\RUTBO\PycharmProjects\pvalue\single cell analysis.py"", line 44, in <module>. sc.pp.neighbors(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. Fi",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:3983,usability,User,Users,3983,"rs(tab, n_neighbors=10, n_pcs=40). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4129,usability,User,Users,4129,"ne 139, in neighbors. neighbors.compute_neighbors(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 808, in compute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"",",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4306,usability,User,Users,4306,"pute_neighbors. self._distances, self._connectivities = _compute_connectivities_umap(. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4424,usability,statu,status,4424,"cal\Programs\Python\Python39\lib\site-packages\scanpy\neighbors\__init__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4491,usability,User,Users,4491,"nit__.py"", line 387, in _compute_connectivities_umap. from umap.umap_ import fuzzy_simplicial_set. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\__init__.py"", line 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4674,usability,User,Users,4674,"ine 2, in <module>. from .umap_ import UMAP. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\umap_.py"", line 41, in <module>. from umap.layouts import (. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:4871,usability,User,Users,4871,"ppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 39, in <module>. def rdist(x, y):. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5039,usability,User,Users,5039,"-packages\numba\core\decorators.py"", line 219, in wrapper. disp.compile(sig). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5203,usability,User,Users,5203,"re\dispatcher.py"", line 965, in compile. cres = self._compiler.compile(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5367,usability,User,Users,5367,"ages\numba\core\dispatcher.py"", line 125, in compile. status, retval = self._compile_cached(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Prog",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5507,usability,User,Users,5507,"\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 139, in _compile_cached. retval = self._compile_core(args, return_type). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5658,usability,User,Users,5658,"e). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\dispatcher.py"", line 152, in _compile_core. cres = compiler.compile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5814,usability,User,Users,5814,"pile_extra(self.targetdescr.typing_context,. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 693, in compile_extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Use",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:5983,usability,User,Users,5983,"extra. return pipeline.compile_extra(func). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 429, in compile_extra. return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_bod",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6156,usability,User,Users,6156,". return self._compile_bytecode(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 497, in _compile_bytecode. return self._compile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File """,MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6340,usability,User,Users,6340,"ompile_core(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 476, in _compile_core. raise e. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Pro",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6505,usability,User,Users,6505,"sers\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler.py"", line 463, in _compile_core. pm.run(self.state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6650,usability,User,Users,6650," ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 353, in run. raise patched_exception. File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:6814,usability,User,Users,6814,"rs\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 341, in run. self._runPass(idx, pass_inst, state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to ",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7000,usability,User,Users,7000,"Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_lock.py"", line 35, in _acquire_compile_lock. return func(*args, **kwargs). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During:",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7162,usability,User,Users,7162,"BO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 296, in _runPass. mutated |= check(pss.run_pass, internal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\R",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7314,usability,User,Users,7314,"ernal_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @g",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7457,usability,User,Users,7457,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7537,usability,error,errors,7537,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:7625,usability,error,errors,7625,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1652:8158,usability,User,Users,8158,"te-packages\numba\core\compiler_machinery.py"", line 269, in check. mangled = func(compiler_state). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\typed_passes.py"", line 394, in run_pass. lower.lower(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 196, in lower. self.lower_normal_function(self.fndesc). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 250, in lower_normal_function. entry_block_tail = self.lower_function_body(). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 279, in lower_function_body. self.lower_block(block). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\lowering.py"", line 293, in lower_block. self.lower_inst(inst). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\contextlib.py"", line 135, in __exit__. self.gen.throw(type, value, traceback). File ""C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\site-packages\numba\core\errors.py"", line 837, in new_error_context. raise newerr.with_traceback(tb). numba.core.errors.LoweringError: Failed in nopython mode pipeline (step: native lowering). Invalid store of i64 to i32 in <numba.core.datamodel.models.RangeModel object at 0x0000015592499520> (trying to write member #1). File ""..\..\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py"", line 53:. def rdist(x, y):. <source elided>. dim = x.shape[0]. for i in range(dim):. ^. During: lowering ""$20call_function.7 = call $16load_global.5(dim, func=$16load_global.5, args=[Var(dim, layouts.py:52)], kws=(), vararg=None, target=None)"" at C:\Users\RUTBO\AppData\Local\Programs\Python\Python39\lib\umap\layouts.py (53). I am running scanpy using python v3.9 with numba v0.55. . _Originally posted by @gatocor in https://github.com/theislab/scanpy/issues/1652#issuecomment-779686831_",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1652
https://github.com/scverse/scanpy/issues/1653:807,deployability,integr,integrate,807,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:919,deployability,integr,integrate,919,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:511,energy efficiency,profil,profile,511,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:588,energy efficiency,draw,draw,588,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:807,integrability,integr,integrate,807,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:919,integrability,integr,integrate,919,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:807,interoperability,integr,integrate,807,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:919,interoperability,integr,integrate,919,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:722,modifiability,pac,package,722,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:767,modifiability,pac,package,767,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:807,modifiability,integr,integrate,807,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:919,modifiability,integr,integrate,919,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:511,performance,profil,profile,511,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:807,reliability,integr,integrate,807,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:919,reliability,integr,integrate,919,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:366,safety,test,test,366,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:807,security,integr,integrate,807,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:919,security,integr,integrate,919,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:23,testability,simpl,simple,23,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:366,testability,test,test,366,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:569,testability,plan,planned,569,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:807,testability,integr,integrate,807,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:851,testability,plan,planning,851,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:919,testability,integr,integrate,919,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:1052,testability,plan,plans,1052,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:15,usability,tool,tool,15,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:23,usability,simpl,simple,23,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:39,usability,tool,tool,39,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:86,usability,tool,tools,86,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
https://github.com/scverse/scanpy/issues/1653:130,usability,tool,tools,130,"> New analysis tool: A simple analysis tool you have been using and are missing in sc.tools. What about alternative normalization tools like SCTransform? I read that they are supposed to be better for spatial data. As non-mathematician of course I'm not sure how big the difference will really be in the end but it would be great if there was a easy way to call and test them if it's worth it. > New plotting function: A kind of plot you would like to seein sc.pl? I think a plot that shows the gene expression profile along a spatial axis would be nice if this is not planned yet. So to draw in e.g. a line in napari and get the gene expression of certain genes along this line. > External tools: Do you know an existing package that should go into sc.external.*? A package I found very useful and easy to integrate with scanpy is SpatialDE. Are you planning to provide this in `sc.external.*`? And of course tools to integrate sc-RNA-seq and spatial data (like Stereoscope, cell2location,...) would be great! But I think you mentioned that there are plans for own tools, right?",MatchSource.ISSUE_COMMENT,scverse,scanpy,1.10.2,https://scanpy.readthedocs.io,https://github.com/scverse/scanpy/issues/1653
